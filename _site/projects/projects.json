[
  {
    "path": "projects/2022-07-12-multipleMortality_vultureNetworks/",
    "title": "Modeling Situation-Specific Effects of Multiple Mortality on Social Network Structure in Griffon Vultures (Gyps fulvus)",
    "description": "This post is a companion to the poster that I presented at ABS 2022, San José, Costa Rica. It contains a bit more information about the project.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2022-07-13",
    "categories": [],
    "contents": "\n\nContents\nIntroduction and Links\nResearch Questions\nBackground\nData: A Vulture\nCo-Feeding Network\nPreliminary Data\nFiltering\nSelecting a Focal Time\nPeriod\n\nModeling Approach\nBaseline Network\nDynamics\nNode Removal and\nRewiring\n\n\nPackages needed for this code:\n\n\nlibrary(tidyverse) # for data wrangling\nlibrary(vultureUtils) # https://github.com/kaijagahm/vultureUtils (custom funs for vulture data)\nlibrary(downloadthis) # for the download link to the poster pdf\nlibrary(ggraph) # for making networks\nlibrary(fitdistrplus) # for fitting beta distributions\nlibrary(igraph) # for network calculations\n\n\n\nIntroduction and Links\nThis is an overview of my modeling process so far for the vulture\nmultiple mortality project. If you want to see the poster, download it\nhere:\n\n\n.button_large {\n  font-size: 24px;\n}\n\nimg {\n    max-width: 100%;\n    max-height: 100%;\n}\n\n\n\n Download the poster\n\n\nThe code for the project so far can be found on GitHub.\nPlease feel free to contact me by email, on Twitter, etc. if you have\nany questions or suggestions! Contact links are on the home page of this\nsite.\nResearch Questions\nHow long does it take for a social network to recover its\nstructure after the loss of several connected individuals?\nHow does recovery differ between social\nsituations?\nBackground\nEurasian griffon vultures (Gyps fulvus) are obligate\nscavengers (Ruxton and Houston\n2004) and social foragers (Harel et al. 2017).\nPoisoned carcasses can kill many vultures quickly as they assemble to\neat in groups. Poisoning is a major threat to griffon vultures. It is\nthe leading cause of death in the griffon vulture population in Israel\n(Anglister\n2022), and also threatens vulture species across the world\n(Ives et al. 2022).\nA Eurasian Griffon Vulture (Gyps fulvus)\nand an Egyptian Vulture (Neophron percnopterus) feed at a carcass\nprovided at a feeding site in Israel. Photo by Noa\nPinter-Wollman.Social network analysis of GPS-tagged griffon vultures in Israel lets\nus study their population social structure. Past work by Nitika Sharma\nand Noa Pinter-Wollman shows that social networks differ between\nsituations (co-flight, co-feeding, and co-roosting) (Sharma et al. 2022).\n\nData: A Vulture Co-Feeding\nNetwork\nThe data that I’m basing this model on comes from a GPS-tagged\npopulation of griffon vultures in Israel. The species is locally\ncritically endangered (Efrat et al.\n2020) and is managed by the Israel Nature and Parks Authority\n(INPA), which runs supplemental feeding stations throughout the country\n(Spiegel et al.\n2013). Israel’s griffon vultures live mainly in the Negev and\nJudean deserts in the south. There is a small population in northeastern\nIsrael and a declining population in the northwest. For this analysis, I\nrestricted the data to the southern population, which is relatively\nwell-mixed and somewhat separated from the northern populations.\nAs part of a collaborative NSF-BSF project between Orr Spiegel and\nNoa Pinter-Wollman, nearly 100 vultures have been fitted with GPS\ntransmitters over the past two years. These tags provide location\ninformation at 10-minute intervals, with data automatically transmitted\nto and stored on Movebank. As of 2022, approximately 70% of the griffon\npopulation in Israel is tagged. This high coverage of the population\nmeans that the majority of a tagged individual’s social interactions are\nwith other tagged individuals, allowing us to study social interactions\nwith greater certainty than is possible in many studies of free-ranging\nanimals.\nPreliminary Data Filtering\nIn addition to restricting the data to only the southern population,\nI did the following:\nGeography | I used a rough geographic mask to\ninclude only individuals that spent at least 30% of their GPS-tracked\ndays in Israel, and then to include only locations inside of\nIsrael.\nBehavioral Situation | I\nincluded only diurnal ground interactions. To do this, I restricted the\ndata to GPS points taken when the bird was moving at a speed of\n< 5 m/s (a proxy for walking/hopping/standing,\nversus flying). Because vultures mostly land during the day when they\nare feeding, I refer to the network created from this data, loosely, as\na “co-feeding network.” A more accurate term would be an “network of\ndiurnal ground interactions”.\nSelecting a Focal Time\nPeriod\nI wanted to choose a time period to study during which as many tagged\nindividuals as possible were present for the duration of the period. I\nstarted by exploring my data.\n\n\nShow code\n\n# Load data, previously downloaded from Movebank and filtered as described above.\nload(\"data/southernEdges_20200101_20220430.Rda\")\nload(\"data/southernPoints_20200101_20220430.Rda\")\n\n## Create a time series graph to see which individuals are present at which times.\ntimeseries <- southernPoints_20200101_20220430 %>%\n  sf::st_drop_geometry() %>%\n  dplyr::select(trackId, dateOnly) %>%\n  dplyr::distinct()\n\n# I want to order the individual vultures in the plot by number of data points we have for them.\norder <- timeseries %>%\n  group_by(trackId) %>%\n  summarize(n = n()) %>%\n  arrange(n) %>%\n  pull(trackId)\n\n# Make a plot:\ntimeseries %>%\n  mutate(trackId = factor(trackId, levels = order)) %>% # apply the ordering\n  ggplot(aes(x = dateOnly, y = trackId))+\n  geom_point(size = 0.5)+\n  theme_minimal()+\n  ylab(\"Vulture\")+\n  xlab(\"Date\")+\n  theme(axis.text.y = element_text(size = 4)) # teeny tiny vulture IDs because I just want to get the overall picture for now.\n\n\n\n\nOk, based on this, it looks like we want to restrict the dates to the\nwindow between 2020-10-01 and 2021-09-01.\n\n\nShow code\n\n# See what it looks like if we restrict the dates to 2020-10-01 through 2021-09-01\nstartDate <- \"2020-10-01\"\nendDate <- \"2021-09-01\"\ntimeseries %>%\n  filter(dateOnly > lubridate::ymd(startDate) & dateOnly < lubridate::ymd(endDate)) %>%\n  mutate(trackId = factor(trackId, levels = order)) %>%\n  ggplot(aes(x = dateOnly, y = trackId))+\n  geom_point(size = 0.5)+\n  theme_minimal()+\n  ylab(\"Vulture\")+\n  xlab(\"Date\")+\n  theme(axis.text.y = element_text(size = 7))\n\n\n\n\nFinally, I excluded a few individuals that were observed only at one\nend of this range. I included only individuals observed both before and\nafter 2021-02-01 in the final data set.\n\n\nShow code\n\n# Exclude a few individuals that were observed only at one end of this range, arbitrarily using February 2021 as the cutoff point\ntoKeep <- timeseries %>%\n  group_by(trackId) %>%\n  summarize(min = min(dateOnly),\n            max = max(dateOnly)) %>%\n  filter(min < lubridate::ymd(\"2021-02-01\") & max > lubridate::ymd(\"2021-02-01\")) %>%\n  pull(trackId)\n\n# Get usable edges, removing the individuals that don't fall within those constraints, and removing bad dates.\ntoUse <- southernEdges_20200101_20220430 %>%\n  filter(ID1 %in% toKeep | ID2 %in% toKeep,\n         minTimestamp > lubridate::ymd(startDate),\n         maxTimestamp < lubridate::ymd(endDate))\nsave(toUse, file = \"data/toUse.Rda\")\n\n\n\nAfter doing a sensitivity analysis on this data to explore different\ntime windows, I opted to create networks aggregated over spans of 5\ndays. I wanted to capture as much detail as possible in the networks\nwhile accounting for the fact that vultures don’t feed every day, so\nnetworks created for each day ended up being extremely sparse, sometimes\nincluding only one or two birds per network.\nHere is an animation showing the 5-day networks, during the\nperiod\n\n\nShow code\n\n# Create networks with 5-day increments, using a custom network creation function. The function code is in the vultureUtils package, which you can find at https://github.com/kaijagahm/vultureUtils.\nrealGraphs <- vultureUtils::makeGraphs(edges = toUse, interval = \"5 days\", \n                         dateTimeStart = \"2020-10-01 00:00:00\",\n                         dateTimeEnd = \"2021-09-01 11:59:00\",\n                         weighted = FALSE, allVertices = TRUE)$graphs\n\n# Create some random coordinates to use, which will be kept consistent throughout the animation:\ncoords <- data.frame(name = names(igraph::V(realGraphs[[1]])),\n                     x = rnorm(n = length(igraph::V(realGraphs[[1]]))),\n                     y = rnorm(n = length(igraph::V(realGraphs[[1]]))))\n\n# Create the animation\nfor(i in 1:length(realGraphs)){\n  p <- tidygraph::as_tbl_graph(realGraphs[[i]], nodes = coords$name) %>%\n    ggraph(layout = \"manual\", x = coords$x, y = coords$y)+\n    geom_edge_link()+\n    geom_node_point(size = 4, aes(col = name))+\n    theme_graph()+\n    scale_color_viridis_d()+\n    theme(legend.position = \"none\")+\n    ggtitle(paste(\"5-day co-feeding network starting on\", \n                  names(realGraphs)[i]))\n  print(p)\n}\n\n\n\n\nModeling Approach\nTo investigate this question, I am building an agent-based\nnetwork model. In my model, nodes in the network represent\nvultures, and edges represent their interactions. For now, the network\nis unweighted and undirected. Each\nedge is either present (the two individuals did interact in this time\nstep) or absent (the two individuals did not interact in this time\nstep).\nBaseline Network Dynamics\nBefore I can use the model to ask questions about what will happen to\nthe vultures’ social structure when nodes are removed, I have to build\nsome baseline network dynamics that roughly approximate what happens in\nthe real data.\nI decided to use a discrete-time model. In each time step, edges are\nadded or lost depending on their history in the previous two\ntime steps.\nTo do this, I assign each edge a history: either 00\n(the edge did not exist in either of the previous two time steps),\n11 (the edge existed in both of the previous two time\nsteps), 01 (the edge existed only in the most recent\ntime step), or 10 (the edge existed in the second most\nrecent time step but not in the most recent one).\nA diagram of how the baseline network\ndynamics work in the model. Edges are added and removed based on the two\nprevious time steps. The arrows show two examples: an edge with history\n11 that subsequently disappears, and an edge with\nhistory 01 that remains in the current time\nstep.Next, I used the observed co-feeding network data to compute\nprobability distributions for the four probabilities: add00\n(the probability of adding an edge with history 00);\nadd10 (the probability of adding an edge with history\n10); lose01 (the probability of losing an\nedge with history 01); and lose11 (the\nprobability of losing an edge with history 11). For\neach time step in the observed networks, I determined the history of\neach edge and its fate.\n\n\nShow code\n\nprobs <- vultureUtils::computeProbs(graphList = realGraphs) # again, using a function from vultureUtils to compute these probabilities. See https://github.com/kaijagahm/vultureUtils for the code.\n\n# Remove NA probabilities\nprobs <- probs %>%\n  dplyr::filter(!is.nan(prob),\n                !is.na(prob)) %>%\n  # To be able to fit beta distributions, we have to move the probabilities slightly away from 0 and 1.\n  dplyr::mutate(prob = (prob - min(prob) + 0.001) / (max(prob) - min(prob) + 0.002))\n\n# Fit a beta distribution to each of the four probabilities.\nfit_add00 <- fitdist(probs %>% \n                       filter(type == \"add00\") %>% \n                       pull(prob), \n                     \"beta\")\n\nfit_add10 <- fitdist(probs %>% \n                       filter(type == \"add10\") %>% \n                       pull(prob), \n                     \"beta\")\n\nfit_lose01 <- fitdist(probs %>% \n                        filter(type == \"lose01\") %>% \n                        pull(prob), \n                      \"beta\")\n\nfit_lose11 <- fitdist(probs %>% \n                        filter(type == \"lose11\") %>% \n                        pull(prob), \n                      \"beta\")\n\n# Plot the density curves with the beta distributions over them\nadd00Plot <- probs %>%\n  filter(type == \"add00\") %>%\n  ggplot(aes(x = prob))+\n  geom_histogram(fill = \"lightgrey\", col = \"darkgrey\")+\n  theme_minimal()+\n  ylab(\"\")+\n  xlab(\"\")+\n  stat_function(fun = function(x) dbeta(x, fit_add00$estimate[1], fit_add00$estimate[2]), \n                color = \"blue\",\n                size = 1)+\n  ggtitle(\"P(add | 00)\")\n\nadd10Plot <- probs %>%\n  filter(type == \"add10\") %>%\n  ggplot(aes(x = prob))+\n  geom_histogram(fill = \"lightgrey\", col = \"darkgrey\")+\n  theme_minimal()+\n  ylab(\"\")+\n  xlab(\"\")+\n  stat_function(fun = function(x) dbeta(x, fit_add10$estimate[1], fit_add10$estimate[2]), \n                color = \"blue\",\n                size = 1)+\n ggtitle(\"P(add | 10)\")\n\nlose11Plot <- probs %>%\n  filter(type == \"lose11\") %>%\n  ggplot(aes(x = prob))+\n  geom_histogram(fill = \"lightgrey\", col = \"darkgrey\")+\n  theme_minimal()+\n  ylab(\"\")+\n  xlab(\"\")+\n  stat_function(fun = function(x) dbeta(x, fit_lose11$estimate[1], fit_lose11$estimate[2]), \n                color = \"blue\",\n                size = 1)+\n  ggtitle(\"P(lose | 11)\")\n\nlose01Plot <- probs %>%\n  filter(type == \"lose01\") %>%\n  ggplot(aes(x = prob))+\n  geom_histogram(fill = \"lightgrey\", col = \"darkgrey\")+\n  theme_minimal()+\n  ylab(\"\")+\n  xlab(\"\")+\n  stat_function(fun = function(x) dbeta(x, fit_lose01$estimate[1], fit_lose01$estimate[2]), \n                color = \"blue\",\n                size = 1)+\n  ggtitle(\"P(lose | 01)\")\n\ngrid <- cowplot::plot_grid(add00Plot, add10Plot, lose01Plot, lose11Plot, nrow = 1, ncol = 4)\n\n\n\nI incorporated these distributions into the baseline model dynamics.\nSo, when an edge has history 00, the probability it will be added is\ndrawn from the beta distribution on the far left. I followed the same\nprocess for the other edge histories.\nValidation\nI compared the baseline model dynamics to the observed GPS network\ndata, using the 5-day co-feeding networks between 2020-10-01 and\n2021-09-01. To do this, I used the model to generate some baseline\nnetworks. Because my observed data included 36 individuals and 72 time\nsteps, I created modeled networks with those numbers of nodes and time\nsteps, respectively.\n\n\nShow code\n\n# Load the model function and its supporting functions\nsource(\"supportingFunctions.R\")\nsource(\"modelFunction_rewiring.R\")\n\n# Get parameters from the observed networks\nnIndivs <- length(igraph::V(realGraphs[[1]]))\nnTimesteps <- length(realGraphs)\n\n# Run the model\nmodelGraphs <- runModel(N = nIndivs, \n                        burn.in = nTimesteps, # same number of time steps as in the real/observed networks\n                        doRemoval = FALSE) %>% # just running the baseline dynamics, not doing a removal yet.\n  lapply(., function(x){\n    # Create igraph objects!\n    igraph::graph_from_adjacency_matrix(x, mode = \"undirected\")\n  })\n\n\n\n1) Degree Distributions\nIn a network, the “degree” of a node is the number of other nodes it\nis connected to.\n\n\nShow code\n\n# Obtain degree information\nfn <- function(x){\n  igraph::degree(x) %>%\n    as.data.frame()\n}\n\n# For the observed network:\ndegrees_real <- lapply(realGraphs, fn) %>% \n  setNames(., NULL) %>%\n  data.table::rbindlist(idcol = \"timestep\") %>% \n  mutate(type = \"real\")\n\n# For the model network:\ndegrees_model <- lapply(modelGraphs, fn) %>%\n  setNames(., NULL) %>%\n  data.table::rbindlist(idcol = \"timestep\") %>%\n  mutate(type = \"model\")\n\n# Single data frame for plotting:\ndegrees <- bind_rows(degrees_real, degrees_model) %>%\n  rename(\"degree\" = \".\")\n\n# Visualize degree distributions\ndegrees %>%\n  mutate(timestep = as.factor(timestep)) %>%\n  mutate(type = case_when(type == \"model\" ~ \"Model\",\n                          type == \"real\" ~ \"Observed\")) %>%\n  ggplot(aes(x = degree, col = timestep))+\n  geom_density()+\n  facet_wrap(~type)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n2) Degree Over Time\nHow does the network’s degree distribution change over the course of\nthe baseline network simulation?\n\n\nShow code\n\ndegreePlot <- degrees %>%\n  mutate(type = case_when(type == \"model\" ~ \"Model\",\n                          type == \"real\" ~ \"Observed\")) %>%\n  ggplot(aes(x = timestep, y = degree))+\n  geom_point(alpha = 0.2) +\n  geom_smooth()+\n  facet_wrap(~type)+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  ylab(\"Degree\")+\n  xlab(\"Timestep\")+\n  theme(axis.text = element_text(size = 16),\n        axis.title = element_text(size = 18, face = \"bold\"))\n\ndegreePlot\n\n\n\n\nWe seem to have a lot more disconnected individuals (individuals with\ndegree 0) in the observed networks than in the modeled networks.\nRemoving the individuals with degree 0 gives us a slightly better\nmatch:\n\n\nShow code\n\ndegrees %>%\n  mutate(type = case_when(type == \"model\" ~ \"Model\",\n                          type == \"real\" ~ \"Observed\")) %>%\n  filter(degree > 0) %>%\n  ggplot(aes(x = timestep, y = degree))+\n  geom_point(alpha = 0.2) +\n  geom_smooth()+\n  facet_wrap(~type)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n…but the distributions still don’t look quite the same.\nMy conclusion from this is that my baseline model doesn’t accurately\ncapture some of the fundamental biology that’s going on in the vulture\nsystem. I suspect that I need to tweak the assumptions of the model a\nlittle bit.\nHowever, I decided that the model captures the network dynamics\nwell enough for now–I’m going to move forward with modeling\nnode loss and rewiring. Then I can come back and tweak the baseline\ndynamics later on.\nNode Removal and Rewiring\nLosses of individuals from the population are irregular enough that\nit is not straightforward to figure out how the vulture social structure\nbehaves following a loss. Therefore, for my initial model, I decided to\nimplement a “second-degree rewiring” model (Farine 2021). Under this model, when a\nnode is lost, there is a higher probability of an edge forming between\ntwo individuals that were both connected to the lost individual than\nbetween two individuals that were not both connected to the lost\nindividual.\nTo make it easier to talk about this process, I have decided to refer\nto nodes either as bereaved or\nnon-bereaved. A bereaved node is one\nthat was directly connected to the lost node. A\nnon-bereaved node is one that was not directly\nconnected to the lost node.\nRewiring after the loss of a single\nnode.The idea of second-degree rewiring, by its nature, already takes into\naccount one previous time step. We have to ask whether the nodes were or\nwere not connected to the lost node in the previous time step (bereaved\nvs. not). We also have to ask whether there is the potential for a new\nedge to form—i.e. did that edge already exist in the previous time step?\nWhen forming new edges, we only need to deal with edges that did not\npreviously exist.\nTo match the baseline network dynamics, I wanted to incorporate\nanother time step of history: two steps before the focal node is lost.\nSo, for each potential edge, I multiplied its probability of being\nformed by a constant 1.2 if the edge existed in the previous time step.\nIf it didn’t exist in the previous time step, I did not modify the\nprobability. Then I restricted probabilities to the range [0, 1] and\nused those proabilities to determine whether the edge formed or not.\nValidation\nIt is harder to compare network behavior following the loss of a node\nto the real data, because real instances of loss are often not\nclear-cut. Loss of a vulture from our dataset can also have many\nunderlying causes.\nGPS tag fell off or stopped working. I would perceive a loss, but\nthe bird would continue its behavior and interactions\nunobserved.\nThe individual flew out of the study area, maybe to return later,\nmaybe not. I have so far restricted this analysis to an area that\nroughly encompasses Israel. It is possible for griffons to make longer\nforays, though this is not common (Nathan et al. 2012).\nThe individual died. This is the case that I’m most interested in\nstudying. But even here, the cause of death could affect post-mortality\nnetwork dynamics. For example, other vultures might react differently to\nexpected deaths than to sudden ones (Shizuka and Johnson 2020).\nAs a preliminary investigation, I decided to lump all lost nodes\ntogether regardless of the cause, just to begin exploring the\nreal dynamics of node loss in this system.\nI made a timeline plot of the individuals in the population during\nthe time frame that I was studying, to determine which individuals were\nlost during that time window.\n\n\nShow code\n\n# For each individual, get nEdges and the end date\nindivsSummary <- toUse %>%\n  mutate(pair = paste(ID1, ID2)) %>%\n  pivot_longer(cols = c(\"ID1\", \"ID2\"), values_to = \"trackId\") %>% # pivot to long so we can sort by individual, but still only listing the edges in one direction.\n  group_by(trackId) %>%\n  summarize(nEdges = n(),\n            firstDate = min(lubridate::date(minTimestamp), na.rm = FALSE),\n            lastDate = max(lubridate::date(maxTimestamp), na.rm = FALSE))\n\n# Plot the durations as segments\ntimeline <- indivsSummary %>%\n  ggplot()+\n  geom_segment(aes(x = firstDate, xend = lastDate, y = trackId, yend = trackId))+\n  geom_point(aes(x = firstDate, y = trackId), color = \"skyblue\", size=2 )+\n  geom_point(aes(x = lastDate, y = trackId), color = \"firebrick\", size=2 )+\n  theme_minimal()+\n  xlab(\"Date\")+\n  ylab(\"Vulture\")+\n  theme(panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        axis.title.y = element_text(size = 18, face = \"bold\"),\n        axis.title.x = element_text(size = 18, face = \"bold\"),\n        axis.text.x = element_text(size = 16))\n\ntimeline\n\n\n\n\nThis plot showed me that during the target time period, eight\nindividuals were lost considerably before the end of the time period\n(not counting T25b, which was not lost long enough before the end of the\ntime period to examine the network dynamics following the loss). Of\nthese eight individuals, I excluded J30w, which was lost mere days after\nappearing for the first time. This left me with seven individuals whose\nlosses I could examine: T85w, J36w, A09w, A04w, A76w, A13w, and\nT77w.\nFor each of these lost individuals, I obtained information about its\nfirst and last date present in the dataset. Then I created a subset of\nthe networks, containing only the time steps immediately before and\nafter the loss.\nFor this examination, I used networks aggregated over 9 day windows\ninstead of 5 day windows. Because the observed networks are incredibly\nsparse (as I discovered when I compared them to the modeled networks),\nin most cases the bereaved nodes formed no edges at all in the 5 days\nfollowing the loss of the focal individual—because most nodes formed no\nedges at all. I wasn’t able to get meaningful comparisons between the\nedge-formation rates of bereaved and non-bereaved nodes with a 5 day\nwindow.\nI do recognize that using a 9 day window means that my findings here\nare not directly applicable to the modeled networks constructed with a 5\nday window. But my findings here are no more than suggestive anyway,\nsince I have so few individuals. I am treating this as a purely\nexploratory phase of examining the data. Once I have modified the base\nmodel to more closely approximate the real data, I will look into\nchanging the time windows used.\nSo, first, I took network subsets immediately before and after the\nloss of each individual. Then, I used these subsets to compute, for\nbereaved and non-bereaved individuals, the probability of gaining and\nlosing edges with other bereaved and non-bereaved individuals. I also\ntook into account the history of the edges in the second-to-last time\nstep containing the about-to-be-lost individual, to see whether edge\nhistory affected the likelihood of gaining/losing an edge following the\nloss of a node.\n\n\nShow code\n\nremovedIndivs <- indivsSummary %>%\n  filter(trackId %in% c(\"T85w\", \"J36w\", \"A09w\", \"A04w\", \"A76w\", \"A13w\", \"T77w\"))\n\n# Get network subsets for each individual: immediately before and after the loss.\nninedays <- vultureUtils::makeGraphs(edges = toUse, interval = \"9 days\", \n                         dateTimeStart = \"2020-10-01 00:00:00\",\n                         dateTimeEnd = \"2021-09-01 11:59:00\",\n                         weighted = FALSE, allVertices = TRUE)$graphs\nninedays_reduced <- lapply(ninedays, function(x){\n  delete.vertices(x, degree(x) < 1)\n})\nearlyDates <- lubridate::ymd(names(ninedays_reduced)) # looking at reduced networks to only include nodes involved in edges.\nnetworkSubsets <- vector(mode = \"list\", length = nrow(removedIndivs))\nfor(i in 1:length(networkSubsets)){\n  removed <- removedIndivs$trackId[i]\n  \n  lastSlice <- lapply(ninedays_reduced, function(x){\n    removed %in% names(V(x))\n  }) %>%\n    unlist() %>%\n    which() %>%\n    max()\n  \n  networkSubsets[[i]] <- ninedays[(lastSlice-1):(lastSlice+1)] # subsetting from ninedays, not ninedays_reduced, here, because we want to have all the vertices.\n}\n\n# Define a function to examine the networks before and after the loss and compute the proportions of edges formed/lost between individuals, taking into account their history and bereavement status.\nexamineLoss <- function(dat, removed){\n  whichRemoved <- which(names(V(dat[[1]])) == removed)\n  \n  # Convert networks to adjacency matrices\n  adjMatrices <- lapply(dat, function(x){\n    igraph::get.adjacency(x) %>%\n      as.matrix()\n  })\n  \n  # Get names of bereaved and non-bereaved nodes\n  edges <- adjMatrices[[1]][whichRemoved,]\n  bereaved <- names(which(edges == 1))\n  non.bereaved <- names(which(edges == 0))\n  \n  # Make an edge list\n  el <- expand.grid(names(edges), names(edges)) %>%\n    mutate(across(.cols = c(Var1, Var2), as.character)) %>%\n    filter(Var1 != removed,\n           Var2 != removed) %>%\n    mutate(prev = ifelse(get.edge.ids(graph = dat[[1]], vp = c(Var1, Var2), directed = FALSE) > 0, 1, 0),\n           cur = ifelse(get.edge.ids(graph = dat[[2]], vp = c(Var1, Var2), directed = FALSE) > 0, 1, 0),\n           after = ifelse(get.edge.ids(graph = dat[[3]], vp = c(Var1, Var2), directed = FALSE) > 0, 1, 0),\n           category = case_when(Var1 %in% bereaved & Var2 %in% bereaved ~ \"bb\",\n                                (Var1 %in% bereaved & Var2 %in% non.bereaved) | \n                                  (Var2 %in% bereaved & Var1 %in% non.bereaved) ~ \"bn\",\n                                Var1 %in% non.bereaved & Var2 %in% non.bereaved ~ \"nn\",\n                                TRUE ~ NA_character_))\n  \n  # Are bb edges more likely to form than nb or nn edges?\n  pForm <- el %>%\n    filter(cur == 0) %>%\n    group_by(category) %>%\n    summarize(nPotential = n(),\n              nFormed = sum(after == 1),\n              pForm = nFormed/nPotential)\n  \n  # Are bb edges less likely to disappear than nb or nn edges?\n  pDisappear <- el %>%\n    filter(cur == 1) %>%\n    group_by(category) %>%\n    summarize(n = n(),\n              nDisappeared = sum(after == 0),\n              pDisappear = nDisappeared/n)\n  \n  # Are bb edges with a history more likely to form than bb edges without a history?\n  pFormHistory <- el %>%\n    filter(cur == 0) %>%\n    group_by(category, prev) %>%\n    summarize(nPotential = n(),\n              nFormed = sum(after == 1),\n              pForm = nFormed/nPotential)\n  \n  # Are bb edges with a history less likely to be lost than bb edges without a history?\n  pDisappearHistory <- el %>%\n    filter(cur == 1) %>%\n    group_by(category, prev) %>%\n    summarize(nPotential = n(),\n              nDisappeared = sum(after == 0),\n              pDisappear = nDisappeared/nPotential)\n  \n  return(list(\"pForm\" = pForm, \"pDisappear\" = pDisappear, \"pFormHistory\" = pFormHistory, \"pDisappearHistory\"= pDisappearHistory, \"el\" = el))\n}\n\n# Apply the function to the network subsets:\nlossInfo_ninedays <- map2(.x = networkSubsets, .y = removedIndivs$trackId, .f = function(.x, .y){\n  examineLoss(dat = .x, removed = .y)\n})\n\n\n\nI used the information that I collected to answer some questions\nabout how real networks behave when a node is lost.\nIs an edge between two bereaved individuals (B-B) more likely to\nform than an edge between two non-bereaved individuals (N-N) or between\na bereaved and a non-bereaved individual (B-N / N-B)? (Note: Because\nthese are undirected networks, I am using B-N and N-B interchangeably\nhere).\n\n\nShow code\n\npForms_9 <- lapply(lossInfo_ninedays, function(x) x$pForm) %>%\n  setNames(., nm = removedIndivs$trackId) %>%\n  data.table::rbindlist(idcol = \"lostIndiv\")\n\nformBereaved <- pForms_9 %>%\n  mutate(category = case_when(category == \"bb\" ~ \"B-B\",\n                              category == \"bn\" ~ \"B-N\",\n                              category == \"nn\" ~ \"N-N\")) %>%\n  mutate(category = factor(category, levels = c(\"B-B\", \"B-N\", \"N-N\"))) %>%\n  ggplot(aes(x = category, y = pForm, fill = category))+\n  geom_boxplot()+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"red\", \"purple\", \"blue\"))+\n  ylab(\"P(add)\")+\n  xlab(\"Bereaved\")+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 18),\n        axis.title = element_text(size = 20, face = \"bold\"))\nformBereaved\n\n\n\n\nInteresting! At least in these seven cases,\nsecond-degree rewiring does not seem to be occurring. If anything, there\nmight be a lower probability of forming a B-B edge than\na N-B or N-N edge. Keep in mind, though, that these differences are\nnot statistically significant, and they are probably\nnot biological significant either, given the absolutely tiny sample size\nhere. But I am definitely going to explore further!\nAre B-B edges less likely to disappear than N-B or N-N edges?\n\n\nShow code\n\npDisappears_9 <- lapply(lossInfo_ninedays, function(x) x$pDisappear) %>%\n  setNames(., nm = removedIndivs$trackId) %>%\n  data.table::rbindlist(idcol = \"lostIndiv\")\n\npDisappears_9 %>%\n  mutate(category = case_when(category == \"bb\" ~ \"B-B\",\n                              category == \"bn\" ~ \"B-N\",\n                              category == \"nn\" ~ \"N-N\")) %>%\n  mutate(category = factor(category, levels = c(\"B-B\", \"B-N\", \"N-N\"))) %>%\n  ggplot(aes(x = category, y = pDisappear, fill = category))+\n  geom_boxplot()+\n  theme_minimal()+\n  scale_fill_manual(values = c(\"red\", \"purple\", \"blue\"))+\n  ylab(\"P(lose)\")+\n  xlab(\"Bereaved\")+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 18),\n        axis.title = element_text(size = 20, face = \"bold\"))\n\n\n\n\nThere seems to be absolutely no difference in the likelihood of\nlosing an edge between B-B, B-N, and N-N edges.\nOkay, so second-degree rewiring might not be occurring here! I’ve\nincorporated it into my model, but this preliminary examination of my\ndata tells me that before I get too far into modeling the consequences\nof multiple-individual loss in my model, I need to get\na better handle on how the loss of a single individual\naffects the rest of the network. This probably means I’ll need to\nexamine much more data, over a larger time span, to tease out any\npatterns.\nDoes history (in the previous time step) make a difference to the\nlikelihood of edges to form?\n\n\nShow code\n\npFormHistories_9 <- lapply(lossInfo_ninedays, function(x) x$pFormHistory) %>%\n  setNames(., nm = removedIndivs$trackId) %>%\n  data.table::rbindlist(idcol = \"lostIndiv\")\n\nformHistory <- pFormHistories_9 %>%\n  mutate(category = case_when(category == \"bb\" ~ \"B-B\",\n                              category == \"bn\" ~ \"B-N\",\n                              category == \"nn\" ~ \"N-N\")) %>%\n  mutate(category = factor(category, levels = c(\"B-B\", \"B-N\", \"N-N\"))) %>%\n  ggplot(aes(x = factor(prev), y = pForm))+\n  geom_boxplot(aes(fill = factor(prev)))+\n  scale_fill_manual(values = c(\"grey\", \"white\"))+\n  facet_wrap(~category)+\n  theme_minimal()+\n  ylab(\"P(add)\")+\n  xlab(\"Edge history\")+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 18),\n        axis.title = element_text(size = 20, face = \"bold\"),\n        strip.text.x = element_text(size = 18))\nformHistory\n\n\n\n\nHere, the behavior of the network actually does align with my\nexpectations! Edges seem to be more likely to form following a loss if\nthey previously existed before the loss. (Note: this is only focusing on\nedges that did not exist during the lost individual’s last\npresent time step. I didn’t count edges that were present in all three\ntime steps, since that wouldn’t constitute an “addition”. In other\nwords, I counted only edges with history 010).\nDoes history make a difference to the likelihood of edges to\ndisappear?\n\n\nShow code\n\npDisappearHistories_9 <- lapply(lossInfo_ninedays, function(x) x$pDisappearHistory) %>%\n  setNames(., nm = removedIndivs$trackId) %>%\n  data.table::rbindlist(idcol = \"lostIndiv\")\n\npDisappearHistories_9 %>%\n  mutate(category = case_when(category == \"bb\" ~ \"B-B\",\n                              category == \"bn\" ~ \"B-N\",\n                              category == \"nn\" ~ \"N-N\")) %>%\n  mutate(category = factor(category, levels = c(\"B-B\", \"B-N\", \"N-N\"))) %>%\n  ggplot(aes(x = factor(prev), y = pDisappear))+\n  geom_boxplot(aes(fill = factor(prev)))+\n  scale_fill_manual(values = c(\"grey\", \"white\"))+\n  facet_wrap(~category)+\n  theme_minimal()+\n  ylab(\"P(lose)\")+\n  xlab(\"Edge history\")+\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 18),\n        axis.title = element_text(size = 20, face = \"bold\"),\n        strip.text.x = element_text(size = 18))\n\n\n\n\nThere’s no apparent pattern to whether edge history affects the\nlikelihood of an edge disappearing. More investigation is in order here\nas well, I think.\nOnce again, remember that the graphs above are based on a\nsample size of SEVEN individual losses! They are just suggestions of\npatterns or lack thereof. I might find totally different patterns when I\nadd more data.\n\n\n\nAnglister, Nili. 2022. “Analysis of Individual Risk for Morbidity\nand Mortality in a Population of an Intensively Managed Species in\nIsrael, the Griffon Vulture (Gyps Fulvus).” Tel Aviv University.\n\n\nEfrat, Ron, Ohad Hatzofe, Ygal Miller, and Oded Berger-Tal. 2020.\n“Determinants of Survival in Captive-Bred Griffon Vultures Gyps\nFulvus After Their Release to the Wild.” Conservation Science\nand Practice 2 (12): e308. https://doi.org/10.1111/csp2.308.\n\n\nFarine, Damien R. 2021. “Structural Trade-Offs Can Predict\nRewiring in Shrinking Social Networks.” Journal of Animal\nEcology 90 (1): 120–30. https://doi.org/10.1111/1365-2656.13140.\n\n\nHarel, Roi, Orr Spiegel, Wayne M. Getz, and Ran Nathan. 2017.\n“Social Foraging and Individual Consistency in Following\nBehaviour: Testing the Information Centre Hypothesis in Free-Ranging\nVultures.” Proceedings of the Royal Society B: Biological\nSciences 284 (1852): 20162654. https://doi.org/10.1098/rspb.2016.2654.\n\n\nIves, Angela M., Maris Brenn-White, Jacqueline Y. Buckley, Corinne J.\nKendall, Sara Wilton, and Sharon L. Deem. 2022. “A Global Review\nof Causes of Morbidity and Mortality in Free-Living Vultures.”\nEcoHealth, January. https://doi.org/10.1007/s10393-021-01573-5.\n\n\nNathan, Ran, Orr Spiegel, Scott Fortmann-Roe, Roi Harel, Martin\nWikelski, and Wayne M. Getz. 2012. “Using Tri-Axial Acceleration\nData to Identify Behavioral Modes of Free-Ranging Animals: General\nConcepts and Tools Illustrated for Griffon Vultures.” Journal\nof Experimental Biology 215 (6): 986–96. https://doi.org/10.1242/jeb.058602.\n\n\nRuxton, Graeme D., and David C. Houston. 2004. “Obligate\nVertebrate Scavengers Must Be Large Soaring Fliers.” Journal\nof Theoretical Biology 228 (3): 431–36. https://doi.org/10.1016/j.jtbi.2004.02.005.\n\n\nSharma, Nitika, Nili Anglister, Orr Spiegel, and Noa Pinter-Wollman.\n2022. “Social Situations Differ in Their Contribution to\nPopulation-Level Social Structure in Griffon Vultures.” https://doi.org/10.22541/au.165107196.66953827/v1.\n\n\nShizuka, Daizaburo, and Allison E Johnson. 2020. “How Demographic\nProcesses Shape Animal Social Networks.” Edited by Leigh Simmons.\nBehavioral Ecology 31 (1): 1–11. https://doi.org/10.1093/beheco/arz083.\n\n\nSpiegel, Orr, Roi Harel, Wayne M. Getz, and Ran Nathan. 2013.\n“Mixed Strategies of Griffon Vultures’ (Gyps Fulvus)\nResponse to Food Deprivation Lead to a Hump-Shaped Movement\nPattern.” Movement Ecology 1 (1): 5. https://doi.org/10.1186/2051-3933-1-5.\n\n\n\n\n",
    "preview": "projects/2022-07-12-multipleMortality_vultureNetworks/multipleMortality_vultureNetworks_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-07-19T17:19:43-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "projects/2021-07-13-retroactivelyReproducible/",
    "title": "Retroactively Reproducible: Lessons from a workflow audit",
    "description": "A talk I gave at the 2021 SORTEE conference about my experience implementing data management and reproducibility tools retroactively, for an existing project, even if best practices weren’t used from the beginning. I also discuss organizing and making sense of someone else’s analyses when approaching the project as an outsider.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-10-27",
    "categories": [],
    "contents": "\n\nContents\nSlides and descriptions\nVideo\nResources\n\n\n\n\nAt the 2021 SORTEE conference this past summer, I gave a talk about\nmy experience conducting a reproducibility audit and cleaning up a\ncolleague’s workflow as she prepared to publish her M.S. thesis\nresearch.\nHere, I’m embedding the video of the talk, and also reproducing it as\na blog post (slides and captions) in case you’d rather read than\nlisten/watch.\nThe slide descriptions are a pretty direct transcription of what I\nsaid during the talk, so if the style seems strange for a blog post,\nthat’s why.\nScroll down to watch the video!\nSlides and descriptions\n\nHello everyone! Thank you for\nbeing here. My name is Kaija Gahm, and I’m a data manager at the Cary\nInstitute of Ecosystem Studies. Late last year, my colleague Chelsea had\njust finished her Master’s thesis, and my supervisor asked me to help\nher to organize her code and analysis into a reproducible workflow that\nwe could reference in the manuscript that we were putting together.\n In this talk, I’m going to be speaking about my experience\nimplementing data management and reproducibility tools retroactively,\nfor an existing project, even if best practices weren’t used from the\nbeginning. I’ll also discuss organizing and making sense of someone\nelse’s analyses, approaching the project as an outsider.A little bit of background on the project:\nIt consisted of some pretty typical ecological data––measurements of\nlake dissolved organic carbon, morphometric landmarks of bluegill\nsunfish, and a bunch of statistical analyses and\nfigures.Some of the\nchallenges I knew I was going to face were that the data had largely\nbeen edited manually in Excel, or copied and pasted from other files\nwith uncertain origin.There were many file versions, and it\nwasn’t really clear how the data and code tied together.The goal of my work was to compile all\nof this data and code into a reproducible analysis. I knew that we were\ngoing to be storing the data on Figshare; all of the analyses would be\nconducted in RStudio and stored on GitHub. The data and code could be\nthen referenced in the final manuscript.To begin my work, I asked Chelsea to start\nby uploading all of her files to GitHub.I think GitHub is a very important\ntool, not just technically, for version control, but also\npsychologically. If everything was being version controlled, then I\nwould have the power to edit and delete without being burdened by the\nidea that I might be deleting something very important. I knew that as\nlong as everything was on GitHub, I could always walk the repository\nback to its original version.My first step in organizing this repository\nwas to set it up as an RStudio Project, which means that it would open a\nfresh R session for everyone who downloaded the code, and all file paths\ncould be written relative to the root directory.  I also\nimplemented renv, which is a great package that allows for\nmanaging package versions. This was particularly important in the case\nof this analysis, because some of the code depended on an old version of\nthe geomorph package for analyzing morphometric landmarks.\nI personally wasn’t very familiar with the package, so even if I had\nwanted to update the code to the newest version, I wouldn’t really have\nbeen able to. Using renv allowed me to run the code with\nthe old version of the package, and it ensures that anyone who wants to\nrun our analysis in the future will be able to restore the packages from\nthe lockfile and immediately proceed without affecting packages on their\nown computer.My next\nstep was to organize the data.  The first principle is that raw\ndata is sacred and should not be modified. So that means no copying and\npasting. Derived data should then be fully reproducible and, by\nextension, disposable. I should be able to just delete all of the\nderived data, and then recreate it by re-running the scripts in the\npipeline. Now, that might be a bit of a pain because some of the\nanalyses might take a long time to run, but it would be possible in a\nfully reproducible analysis. And my job approaching this project\nwas to distinguish between data that was raw––like maybe it was created\nmanually or it was exported from a program somewhere else––versus data\nthat could be regenerated from the analysis code.This is a list of all the data files that I\nwas originally presented with in the GitHub repository, and it was\npretty overwhelming. I had to sort through all of those and figure out\nwhich were raw, which were derived, and where they all went in the\npipeline.And in order\nto do this, I asked the key question, ‘Where did this come from?’ For\neach file, I wanted to know, how was it generated? Can I trace the\norigin of all the files and their columns?I started out by making a spreadsheet\nlisting out each file, and this seemed like a good idea at the time, but\nit ended up being pretty overwhelming, and it was hard for Chelsea to go\nthrough and answer my questions, especially because I was attacking all\nthe files at once, without regard for their importance or the order that\nthey might go in. I was just trying to get my head around\nit.Chelsea and I\ndecided that a better approach would be to start with the most recent\nand up-to-date analysis file; in our case, called\nReviewApril2020.Then, I\ncould look at all of the data files that were read in as inputs. It was\nmuch easier, with this reduced set of files, to\ndetermine……which ones\nwere raw data and which ones were derived data. So, my next step was,\nfor each of those derived data files, to figure out how to re-create it\nfrom the data in the database, using R scripts, so that the process\nwould be fully reproducible.To do that, I created one R script for each\nof the derived data files, and I used that R script to read in data from\nthe database stored on Figshare and modify it to generate these ‘FINAL’\ncsv files. In the background, I kept the original data and used it as a\ncomparison, just to make sure I was accurately recreating that original\ndata.My next step was\nto organize the code in the analysis files. I had to start by\nreorganizing a lot of file paths; they had been written as absolute\npaths, and I rewrote them as relative paths using the wonderful\nhere package.I clarified a lot of the code by renaming\nthe variables with descriptive names and adding clear\ncomments.I also\norganized the code to make sure it was easy to read and\nfollow.And to do that, I\nmade heavy use of the outline panel within RStudio, which you can access\nby clicking on this sort of list icon at the top right corner of the\nscript pane. You’ll notice that this ‘Load packages’ header\ncorresponds to a line in this document outline, and that actually\nhappens automatically within RStudio.You can create these header lines by\nclicking Command + Shift + R on a Mac, and they automatically get added\nto the outlines. And then you can also see that I have a lot of\npackages loaded at the top here, a lot of comments, and everything is\njust a little bit cleaner and easier to read.I wrote a README to document this project,\nincluding an abstract to introduce people to the project, instructions\nfor how to download the data from Figshare and how to run the project,\nrestoring all the packages from the renv lock file, and\nalso information about the contributors.Finally, I wanted to emphasize that I\nhad to go over the same scripts multiple times, and sometimes doing a\nfirst cleanup of the code helped me realize that reorganizing the code\nin different way would actually be clearer, and so I ended up doing what\nfelt like the same work twice. I think that that re-tidying––it\nshouldn’t be dismissed as being inefficient or not doing it right the\nfirst time, because in my case, I had to take a lot of time to just get\nmy head around how the code was organized before I could make informed\ndecisions about how to clean it up.So, I’ve talked about a lot of tools that I\nused in creating this reproducible pipeline, from Figshare to RStudio\nand RStudio Projects, GitHub for version control, Google Drawings in my\ncase for creating this flowchart, and renv for organizing\npackages. And this might be overwhelming if you’ve never used these\ntools before and it feels like you have a lot of things to learn if you\nwant to start implementing reproducible workflows. But I want to\nemphasize that there are smaller things that you can do to move in the\nright direction, even if you can’t jump right into using best practices\nfrom the get-go.For\nexample, don’t be afraid to tell stories with your comments. I like to\nbe very conversational; I like to say ‘Okay, now we’re going to do X’,\n‘I noticed this about the output, and that causes me to make this\ndecision about the model’ For example here, I wrote, ‘To make\nthese calculations easier, I’m going to….’ do XYZ, and then I have some\nlines of code. Now, you’ll notice here that I only have about\nfour lines of code, and a LOT of lines of comments, and not all of my\nscripts look like that. But I just want to get the point across that\nit’s really okay to write essays in your code and to have your code\ncomments be informal, conversational and very\ndescriptive.Something that\nChelsea did in her raw code files that I found very helpful when I went\nthrough and organized them, was to keep track, in the comments, of\npackage versions and decisions that were made in the analyses, and file\npaths. So for example, I already discussed how we had to use an older\nversion of the package geomorph, and conveniently, Chelsea\nhad recorded the version number of the older version that she was using,\nso it was much easier for me to then restore that version when I started\nusing renv.You’ll notice that this script is far\nfrom being perfect; it’s still using absolute file paths, it hasn’t yet\nbeen edited to use the here package, but even without using\nall best practices in reproducibility, just having these introductory\ncomments is really helpful.I want to conclude this talk by just\nemphasizing that we really need to teach workflow in graduate programs,\nundergraduate programs, whenever we teach coding. I would argue that,\nfor biologists, workflow, organization, and reproducibility are some of\nthe things that take up the greatest proportion of our research time.\nThey are actually more valuable skills sometimes than the actual\nstatistical analysis. They are also not intuitive. I think it\ncan be easy, as a student, to assume that if you’re just an organized\nperson, you will magically know how to organize your code. And then it’s\neasy to get frustrated when that doesn’t come naturally and when no one\nhas really taught you what the best practices are. I think this\nresource, written by Jenny Bryan and Jim Hester, ‘What they forgot to\nteach you about R’, is a great example of this. This resource focuses on\na lot of the workflow tactics that I’ve been talking about here, like a\nproject-oriented workflow, and it’s titled ‘What they forgot to teach\nyou about R’. And I think for many students, that’s exactly true. They\ndo forget to teach you the workflow stuff, and that’s sometimes the most\nimportant part.Here’s\na list of resources that I put together related to the various tools for\nreproducibility that I’ve talked about in this\npresentationAnd I would be\nhappy to take any questions at the Q&A session right after this\ntalk. Thank you very much.Hello everyone! Thank you for being\nhere. My name is Kaija Gahm, and I’m a data manager at the Cary\nInstitute of Ecosystem Studies. Late last year, my colleague Chelsea had\njust finished her Master’s thesis, and my supervisor asked me to help\nher to organize her code and analysis into a reproducible workflow that\nwe could reference in the manuscript that we were putting together.\n In this talk, I’m going to be speaking about my experience\nimplementing data management and reproducibility tools retroactively,\nfor an existing project, even if best practices weren’t used from the\nbeginning. I’ll also discuss organizing and making sense of someone\nelse’s analyses, approaching the project as an outsider.A little bit of background on the project:\nIt consisted of some pretty typical ecological data––measurements of\nlake dissolved organic carbon, morphometric landmarks of bluegill\nsunfish, and a bunch of statistical analyses and\nfigures.Some of the\nchallenges I knew I was going to face were that the data had largely\nbeen edited manually in Excel, or copied and pasted from other files\nwith uncertain origin.There were many file versions, and it\nwasn’t really clear how the data and code tied together.The goal of my work was to compile all\nof this data and code into a reproducible analysis. I knew that we were\ngoing to be storing the data on Figshare; all of the analyses would be\nconducted in RStudio and stored on GitHub. The data and code could be\nthen referenced in the final manuscript.To begin my work, I asked Chelsea to start\nby uploading all of her files to GitHub.I think GitHub is a very important\ntool, not just technically, for version control, but also\npsychologically. If everything was being version controlled, then I\nwould have the power to edit and delete without being burdened by the\nidea that I might be deleting something very important. I knew that as\nlong as everything was on GitHub, I could always walk the repository\nback to its original version.My first step in organizing this repository\nwas to set it up as an RStudio Project, which means that it would open a\nfresh R session for everyone who downloaded the code, and all file paths\ncould be written relative to the root directory.  I also\nimplemented renv, which is a great package that allows for\nmanaging package versions. This was particularly important in the case\nof this analysis, because some of the code depended on an old version of\nthe geomorph package for analyzing morphometric landmarks.\nI personally wasn’t very familiar with the package, so even if I had\nwanted to update the code to the newest version, I wouldn’t really have\nbeen able to. Using renv allowed me to run the code with\nthe old version of the package, and it ensures that anyone who wants to\nrun our analysis in the future will be able to restore the packages from\nthe lockfile and immediately proceed without affecting packages on their\nown computer.My next\nstep was to organize the data.  The first principle is that raw\ndata is sacred and should not be modified. So that means no copying and\npasting. Derived data should then be fully reproducible and, by\nextension, disposable. I should be able to just delete all of the\nderived data, and then recreate it by re-running the scripts in the\npipeline. Now, that might be a bit of a pain because some of the\nanalyses might take a long time to run, but it would be possible in a\nfully reproducible analysis. And my job approaching this project\nwas to distinguish between data that was raw––like maybe it was created\nmanually or it was exported from a program somewhere else––versus data\nthat could be regenerated from the analysis code.This is a list of all the data files that I\nwas originally presented with in the GitHub repository, and it was\npretty overwhelming. I had to sort through all of those and figure out\nwhich were raw, which were derived, and where they all went in the\npipeline.And in order\nto do this, I asked the key question, ‘Where did this come from?’ For\neach file, I wanted to know, how was it generated? Can I trace the\norigin of all the files and their columns?I started out by making a spreadsheet\nlisting out each file, and this seemed like a good idea at the time, but\nit ended up being pretty overwhelming, and it was hard for Chelsea to go\nthrough and answer my questions, especially because I was attacking all\nthe files at once, without regard for their importance or the order that\nthey might go in. I was just trying to get my head around\nit.Chelsea and I\ndecided that a better approach would be to start with the most recent\nand up-to-date analysis file; in our case, called\nReviewApril2020.Then, I\ncould look at all of the data files that were read in as inputs. It was\nmuch easier, with this reduced set of files, to\ndetermine……which ones\nwere raw data and which ones were derived data. So, my next step was,\nfor each of those derived data files, to figure out how to re-create it\nfrom the data in the database, using R scripts, so that the process\nwould be fully reproducible.To do that, I created one R script for each\nof the derived data files, and I used that R script to read in data from\nthe database stored on Figshare and modify it to generate these ‘FINAL’\ncsv files. In the background, I kept the original data and used it as a\ncomparison, just to make sure I was accurately recreating that original\ndata.My next step was\nto organize the code in the analysis files. I had to start by\nreorganizing a lot of file paths; they had been written as absolute\npaths, and I rewrote them as relative paths using the wonderful\nhere package.I clarified a lot of the code by renaming\nthe variables with descriptive names and adding clear\ncomments.I also\norganized the code to make sure it was easy to read and\nfollow.And to do that, I\nmade heavy use of the outline panel within RStudio, which you can access\nby clicking on this sort of list icon at the top right corner of the\nscript pane. You’ll notice that this ‘Load packages’ header\ncorresponds to a line in this document outline, and that actually\nhappens automatically within RStudio.You can create these header lines by\nclicking Command + Shift + R on a Mac, and they automatically get added\nto the outlines. And then you can also see that I have a lot of\npackages loaded at the top here, a lot of comments, and everything is\njust a little bit cleaner and easier to read.I wrote a README to document this project,\nincluding an abstract to introduce people to the project, instructions\nfor how to download the data from Figshare and how to run the project,\nrestoring all the packages from the renv lock file, and\nalso information about the contributors.Finally, I wanted to emphasize that I\nhad to go over the same scripts multiple times, and sometimes doing a\nfirst cleanup of the code helped me realize that reorganizing the code\nin different way would actually be clearer, and so I ended up doing what\nfelt like the same work twice. I think that that re-tidying––it\nshouldn’t be dismissed as being inefficient or not doing it right the\nfirst time, because in my case, I had to take a lot of time to just get\nmy head around how the code was organized before I could make informed\ndecisions about how to clean it up.So, I’ve talked about a lot of tools that I\nused in creating this reproducible pipeline, from Figshare to RStudio\nand RStudio Projects, GitHub for version control, Google Drawings in my\ncase for creating this flowchart, and renv for organizing\npackages. And this might be overwhelming if you’ve never used these\ntools before and it feels like you have a lot of things to learn if you\nwant to start implementing reproducible workflows. But I want to\nemphasize that there are smaller things that you can do to move in the\nright direction, even if you can’t jump right into using best practices\nfrom the get-go.For\nexample, don’t be afraid to tell stories with your comments. I like to\nbe very conversational; I like to say ‘Okay, now we’re going to do X’,\n‘I noticed this about the output, and that causes me to make this\ndecision about the model’ For example here, I wrote, ‘To make\nthese calculations easier, I’m going to….’ do XYZ, and then I have some\nlines of code. Now, you’ll notice here that I only have about\nfour lines of code, and a LOT of lines of comments, and not all of my\nscripts look like that. But I just want to get the point across that\nit’s really okay to write essays in your code and to have your code\ncomments be informal, conversational and very\ndescriptive.Something that\nChelsea did in her raw code files that I found very helpful when I went\nthrough and organized them, was to keep track, in the comments, of\npackage versions and decisions that were made in the analyses, and file\npaths. So for example, I already discussed how we had to use an older\nversion of the package geomorph, and conveniently, Chelsea\nhad recorded the version number of the older version that she was using,\nso it was much easier for me to then restore that version when I started\nusing renv.You’ll notice that this script is far\nfrom being perfect; it’s still using absolute file paths, it hasn’t yet\nbeen edited to use the here package, but even without using\nall best practices in reproducibility, just having these introductory\ncomments is really helpful.I want to conclude this talk by just\nemphasizing that we really need to teach workflow in graduate programs,\nundergraduate programs, whenever we teach coding. I would argue that,\nfor biologists, workflow, organization, and reproducibility are some of\nthe things that take up the greatest proportion of our research time.\nThey are actually more valuable skills sometimes than the actual\nstatistical analysis. They are also not intuitive. I think it\ncan be easy, as a student, to assume that if you’re just an organized\nperson, you will magically know how to organize your code. And then it’s\neasy to get frustrated when that doesn’t come naturally and when no one\nhas really taught you what the best practices are. I think this\nresource, written by Jenny Bryan and Jim Hester, ‘What they forgot to\nteach you about R’, is a great example of this. This resource focuses on\na lot of the workflow tactics that I’ve been talking about here, like a\nproject-oriented workflow, and it’s titled ‘What they forgot to teach\nyou about R’. And I think for many students, that’s exactly true. They\ndo forget to teach you the workflow stuff, and that’s sometimes the most\nimportant part.Here’s\na list of resources that I put together related to the various tools for\nreproducibility that I’ve talked about in this\npresentationAnd I would be\nhappy to take any questions at the Q&A session right after this\ntalk. Thank you very much.\n\nVideo\nFor the conference, I embedded captions in the video using iMovie,\nwhich was a pretty clunky process but was all I had access to at the\ntime. There are some glitches and at one point I accidentally inserted\nthe caption for a previous slide. On the whole, I hope the captions are\nbetter than nothing.\n\n\nResources\nThe resource links from the second to last slide didn’t translate\nwell to this format, so here they are:\nRStudio\ninteractive tutorials\nHappy Git with R (Jenny\nBryan and Jim Hester)\nProject-oriented\nworkflow with RStudio projects (Jenny Bryan)\nUsing\nRStudio projects\nUsing the here\npackage\nHow to use renv\nKeeping\na Paper Trail: Data Management Skills for Reproducible Science(Kate\nLaskowski)\nMake a README\nGet an embed\nlink for a Google drawing so you can put it into e.g. a\nREADME\nUsing the\n`tree` command in the terminal to make a file tree\nThe document\noutline in RStudio\nHeaders\nand comments in RStudio (Y. Wendy Hyunh)\nWhat they forgot to teach you about\nR (Jenny Bryan and Jim Hester)\nFinally, you can download the video and the annotated slides from OSF\nhere.\n\n\n\n",
    "preview": "projects/2021-07-13-retroactivelyReproducible/slideImages/0001.jpg",
    "last_modified": "2022-07-13T00:18:08-06:00",
    "input_file": {}
  },
  {
    "path": "projects/2021-05-13-greenT/",
    "title": "greenT: Exploring grapheme-color synesthesia",
    "description": "For my whole life, I've associated colors with letters and numbers. This is called grapheme-color synesthesia. For my second major project in Shiny, I built an app to help me explain synesthesia to friends, and to learn more about the experiences of other synesthetes.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: The digits 0-9, according to my brain\n\n\n\nI have what’s known as grapheme-color synesthesia. I experience letters and numbers as having colors associated with them. I don’t literally see colors floating in the air, but I’ve consistently associated a color with each letter and number for most of my life.\nWhen I tell my friends about my synesthesia, they want to know what color their name is. I wanted a way to quickly represent any word or words in the colors I see it in. I did a bit of googling to see if anyone had come up with a synesthesia simulator.\nThe closest thing I found was Bernadette Sheridan’s website synesthesia.me, which did exactly what I wanted to do, but only for Bernadette’s own colors! Cool, but not useful to me or to any other synesthetes (like my brother) who might want to show me their own colors.\nSo, I built this app. The source code for the app is here.\nFor a guided tour of the parts of the app, see this blog post\nTo read an explanation of the code used in the app, see this blog post\nThis was my second major Shiny project, after the YGDP Dashboard.\nHere’s the app, embedded:\n\n\nIf you have questions about this work, please feel free to get in touch. If you want to suggest an improvement or change to the app, or to report a bug or file a pull request, head over to the github page for the project.\nTo read more, make sure to check out my blog posts about this app. Part 1, Part 2\n\n\n\n",
    "preview": "projects/2021-05-13-greenT/numbers.png",
    "last_modified": "2021-10-27T17:50:01-06:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "projects/2021-02-05-ygdp-dashboard/",
    "title": "YGDP Dashboard",
    "description": "From June through December 2020, I taught myself Shiny and ended up creating a pretty complicated interactive dashboard for exploring dialect variation across the US. Here's a bit about my experience using Shiny, as well as the app itself for you to play around with.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-05",
    "categories": [],
    "contents": "\nA map showing interpolated ratings data for the sentence “Here’s you some money.” A rating of 1 is “totally unacceptable sentence, even in casual conversation”, while 5 is “totally acceptable sentence.”Studying language across the US\nThe Yale Grammatical Diversity Project (YGDP) has been collecting survey data about syntax diversity in U.S. English for over five years.\nYou can read about the YGDP’s work here, but in a nutshell:\nWe put out surveys online to learn about people’s dialects across the United States\nWe ask survey participants to judge whether test sentences, such as “Here’s you a piece of pizza”, seem acceptable in casual conversation.\nWe’re not interested in “proper” or “correct” English–we want to know how people actually talk in real life across the country.\n(If you want to learn about the survey and data analysis methods in great detail, check out the comprehensive mapbook that we published earlier this year.)\nIn the past, there has been a lot of research into regional language diversity, but a lot of it has focused on lexical (words, vocabulary) diversity. A great example of that is the New York Times dialect quiz, which asks questions like “What do you call it when rain falls while the sun is shining?”, with answers ranging from “sunshower” to “the wolf is giving birth”.\nThe YGDP focuses more on syntactic (“grammatical”; related to syntax) diversity. So, they want to know how people put sentences together, not necessarily which words they use.\nThe Scots Syntax Atlas has done some great work along these lines in Scotland. The YGDP’s methods are a little different, but they’re asking the same types of questions about grammatical variation.\nI’m actually not a linguist, so I’m not going to explain the research in detail. To learn more about the YGDP’s research, you can check out their website. Head over to the grammatical phenomena pages to learn more about constructions like “All the further”, “Done my homework”, and “Come with”.\nData work\nIn 2019, I started working with the YGDP to pull together and organize their survey data. One of our primary goals was to make our findings easily accessible and explorable for anyone who might be interested.\nI had heard about R Shiny, but I had never created a Shiny app. In the spring of 2020, I talked to Jim Wood, one of the YGDP’s supervisors, about the possibility of developing a Shiny app to display the YGDP’s findings in a colorful, interactive way. Jim was excited about the idea, even though I had been very clear that I’d be starting absolutely from scratch and learning Shiny along the way.\nFrom June through December 2020, I dove in headfirst. I created a couple simple Shiny apps as practice, but then I set about developing a pretty complicated Shiny dashboard. I’ve always found that the best way to learn a new tool is to plunge in and use it, learning as you go. Learning Shiny was the ultimate baptism by fire. I think I started trying to create hierarchical selectInputs by about day 2, when I was still far from understanding the fundamentals of Shiny’s inputs and outputs. Next came the dashboard layout, which I achieved using shinydashboard and eventually shinydashboardPlus. I learned to insert a map using leaflet, another package I had never dabbled in.\n\n\n\nFigure 1: Hierarchical selectInputs: the choices in the ‘Sentence 1’ menu depend on the selection in ‘Survey’.\n\n\n\n\n\n\nFigure 2: A map made with the leaflet package\n\n\n\nThe dashboard is still evolving, but here is the current version. I’m immensely proud of how far I’ve come with Shiny, and I’m really excited about how this dashboard will allow the YGDP, and the general public, to explore the linguistic survey data in great detail.\n\n\nIf you have questions about this work, please feel free to get in touch. If you want to suggest an improvement or change to the dashboard, or to report a bug or file a pull request, head over to the github page for the project. I’ll be leaving the project shortly, so I’m not sure how actively the dashboard will be maintained, but suggestions are certainly welcome.\n\n\n\n",
    "preview": "projects/2021-02-05-ygdp-dashboard/hexMap.png",
    "last_modified": "2021-10-27T17:50:01-06:00",
    "input_file": {},
    "preview_width": 1434,
    "preview_height": 810
  },
  {
    "path": "projects/2021-01-29-tadpoles/",
    "title": "The Tadpole Olympics",
    "description": "For my B.S. thesis project at Yale, I measured how fast wood frog tadpoles swim in response to a simulated predator attack, and how that swimming speed relates to their developmental rate. It turns out that developing fast comes with a performance cost, and that tradeoff might help to explain some of the patterns we've observed in wood frogs' developmental rates in the past.\nA short description of the post.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [],
    "contents": "\nIntroduction\nFor my senior (B.S.) thesis in E&EB at Yale, I studied a tradeoff between growth rate and swimming performance in wood frog tadpoles.\nWhat does that mean? Basically, my collaborator Andis and I raised tadpoles in the lab under warm and cold conditions. Since frogs are cold-blooded, the warm tadpoles grew faster than the cold tadpoles.\nOn the one hand, growing rapidly is good for animals like wood frogs, who need to escape from predators and compete for mating opportunities. But when we stuck our tadpoles into an arena and tested how fast they swam, we found that the tadpoles pay a cost for that fast growth. The fast-growing (warm) tadpoles didn’t perform as well in their swimming trials as the slow-growing (cold) tadpoles.\nThis kind of tradeoff exists in other animals too, but we tested several populations of tadpoles and found that the effect was really consistent, no matter which population the tadpoles came from.\nWhat determines how fast a tadpole grows?\nWhy is this important? Well, our findings might help to explain a pattern that my lab has been wondering about for a while, called “countergradient variation”. Here’s how it works. We know that growing fast is helpful to tadpoles for a lot of reasons: they might have better mating or survival success as adults (though we don’t know this for sure); they might get too big for aquatic predators to eat while they’re still tadpoles; they can be sure to leave their ephemeral ponds before the water dries up in the summer; and they can get a head start on terrestrial life, perhaps outcompeting slower-growing tadpoles.\nTwo factors can help frogs grow fast: 1) environmental factors, like the temperature of the water they live in, and 2) genetic factors: that is, their genes can predispose them to “intrinsically” fast growth, independent of their environment.\nLet’s say we have two ponds in an imaginary forest, called “Fire Pond” and “Ice Pond” because one has really warm water and the other has really cold water.\nEnvironmental factors\nWe know that the tadpoles living in Fire Pond will grow faster than the tadpoles living in Ice Pond, since frogs are cold-blooded and their growth is pretty heavily influenced by the temperature of the water around them. Warmer water -> faster growth! This is the relationship that we made use of to manipulate our frogs’ developmental rates in the lab.\nGenetic factors\nFire Pond and Ice Pond are pretty far apart, and wood frogs tend to return to the same ponds where they grew up to breed. So, the frog populations in Fire and Ice can evolve roughly independently of each other.\nSo, if fast growth is unequivocally beneficial, then we might expect that both the Fire and Ice populations would evolve toward faster intrinsic growth and development. Put another way: natural selection would favor tadpoles that had genes for fast growth over those who grew more slowly, independent of the surrounding water temperature. We would expect to see this evolutionary trend in both ponds. Tadpoles from both Fire and Ice would evolve fast intrinsic growth rates, such that the Ice tadpoles will grow fast despite the cold water, and the Fire tadpoles will grow EXTRA FAST from the combined effects of genetics and warm water.\nBut this pattern is not what researchers, including the Skelly lab, have observed! When other studies have taken tadpoles from Fire and Ice and reared them at the same temperature (so, removing the effects of the different pond temperatures), they find that tadpoles from Fire grow more slowly than tadpoles from Ice. So, for some reason, those Fire frogs aren’t evolving to grow super fast intrinsically, even though doing so would let them have an edge over the chilly little Ice frogs.\nPutting it together\nThe pattern that I’ve described above is called “countergradient variation” because the frogs’ intrinsic (genetic) growth rates run in the opposite direction (counter-gradient) as their environmental circumstances.\nFor a while, we’ve wondered why we see this pattern. Why don’t the Fire tadpoles make the most of what seems like an advantage: their warm water? The countergradient pattern suggests that there must be a tradeoff, something that makes growing EXTRA FAST not quite so advantageous after all.\nThat’s where my study comes in. It turns out, growing fast comes with a cost in burst swimming speed! If you’re a tadpole, you might get bigger faster, but if you also swim more slowly than your slower-growing peers, you’re at risk of getting gobbled up by a predator. If that happens, all that growing would be for nothing.\nPaper\nMy collaborators and I published this work, and if you want to read more, you can download the paper. But I hope my description is a little easier to get through than the paper itself.\nIf you want a more detailed summary of the paper, with reference to each of its figures, Andis did a great job explaining it in his blog post.\nTalk\nIf you want to learn more about my work and would prefer to see a video of a talk I gave, aimed at a more general audience than the published paper, you can watch it below. This talk was given as part of the 2020 end-of-year E&EB Senior Symposium, which I organized as a remote event due to the COVID-19 pandemic.\nAcknowledgements and reflections\nThis work was made possible with the tremendous support of the Skelly Lab in the Yale School of the Environment. Andis Arietta, in particular, was a great mentor and collaborator. You should check out his research on his website and follow him on Twitter.\nThanks to Kirby Broderick and Simon Stump for help with the MATLAB code I wrote to analyze all my tadpole videos. Thanks to Joaquin Bellomio for help ground-truthing videos, and to Adriana Rubinstein and David Skelly for assistance in the lab.\nFinally, I want to recognize the privilege that allowed me to publish my research in 2020. Apart from summer fellowships, Yale doesn’t pay undergrads for research hours beyond course credit. I was able to spend a lot of extra hours in the lab during my senior year, hours that were not an option for a lot of my fellow students because they had to prioritize fulfilling their student income contributions (SIC) or working to support their families. Yale needs to abolish the SIC (SUN at Yale is doing great work toward this end) and pay undergraduate researchers to help even out these opportunities.\nWhen the COVID-19 pandemic hit, I was able to return to a calm and supportive home environment, where I had lots of time to work on preparing this manuscript for publication. Many of my friends had their thesis work cut short, and they had far more pressing things on their minds than writing and publishing papers. Across the scientific community, some academics are publishing more under lockdowns, while others (disproportionately women, people of color, those caring for children, etc.–aka people who were already at a disadvantage before this pandemic made things worse) have less time, not more, to devote to their work (see this and this, although this calls things into question. I don’t have good data for disparities by race and family status, and if/when I find papers that support/refute my intuition above, I’ll change my view accordingly!).\nI hope that in the coming years, we will see academia recognize that traditional metrics of success, like publication numbers and grant awards, are far from equitable, especially in the wake of this troubled year.\nThank you for reading, and if you have any questions about my work, please feel free to reach out!\n\n\n\n",
    "preview": "projects/2021-01-29-tadpoles/tadpole.png",
    "last_modified": "2021-10-27T17:35:25-06:00",
    "input_file": {},
    "preview_width": 1421,
    "preview_height": 575
  }
]
