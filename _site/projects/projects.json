[
  {
    "path": "projects/2022-07-12-multipleMortality_vultureNetworks/",
    "title": "Modeling Situation-Specific Effects of Multiple Mortality on Social Network Structure in Griffon Vultures (Gyps fulvus)",
    "description": "This is a placeholder page for more information about my agent-based modeling project, ABS 2022.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2022-07-13",
    "categories": [],
    "contents": "\nThe code for the project can be found on GitHub.\n\n\n\n",
    "preview": "projects/2022-07-12-multipleMortality_vultureNetworks/vultureNetwork.png",
    "last_modified": "2022-07-12T23:17:59-07:00",
    "input_file": {},
    "preview_width": 1688,
    "preview_height": 891
  },
  {
    "path": "projects/2021-07-13-retroactivelyReproducible/",
    "title": "Retroactively Reproducible: Lessons from a workflow audit",
    "description": "A talk I gave at the 2021 SORTEE conference about my experience implementing data management and reproducibility tools retroactively, for an existing project, even if best practices weren’t used from the beginning. I also discuss organizing and making sense of someone else’s analyses when approaching the project as an outsider.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-10-27",
    "categories": [],
    "contents": "\n\nContents\nSlides and descriptions\nVideo\nResources\n\n\n\n\nAt the 2021 SORTEE conference this past summer, I gave a talk about\nmy experience conducting a reproducibility audit and cleaning up a\ncolleague’s workflow as she prepared to publish her M.S. thesis\nresearch.\nHere, I’m embedding the video of the talk, and also reproducing it as\na blog post (slides and captions) in case you’d rather read than\nlisten/watch.\nThe slide descriptions are a pretty direct transcription of what I\nsaid during the talk, so if the style seems strange for a blog post,\nthat’s why.\nScroll down to watch the video!\nSlides and descriptions\n\nHello everyone! Thank you for\nbeing here. My name is Kaija Gahm, and I’m a data manager at the Cary\nInstitute of Ecosystem Studies. Late last year, my colleague Chelsea had\njust finished her Master’s thesis, and my supervisor asked me to help\nher to organize her code and analysis into a reproducible workflow that\nwe could reference in the manuscript that we were putting together.\n In this talk, I’m going to be speaking about my experience\nimplementing data management and reproducibility tools retroactively,\nfor an existing project, even if best practices weren’t used from the\nbeginning. I’ll also discuss organizing and making sense of someone\nelse’s analyses, approaching the project as an outsider.A little bit of background on the project:\nIt consisted of some pretty typical ecological data––measurements of\nlake dissolved organic carbon, morphometric landmarks of bluegill\nsunfish, and a bunch of statistical analyses and\nfigures.Some of the\nchallenges I knew I was going to face were that the data had largely\nbeen edited manually in Excel, or copied and pasted from other files\nwith uncertain origin.There were many file versions, and it\nwasn’t really clear how the data and code tied together.The goal of my work was to compile all\nof this data and code into a reproducible analysis. I knew that we were\ngoing to be storing the data on Figshare; all of the analyses would be\nconducted in RStudio and stored on GitHub. The data and code could be\nthen referenced in the final manuscript.To begin my work, I asked Chelsea to start\nby uploading all of her files to GitHub.I think GitHub is a very important\ntool, not just technically, for version control, but also\npsychologically. If everything was being version controlled, then I\nwould have the power to edit and delete without being burdened by the\nidea that I might be deleting something very important. I knew that as\nlong as everything was on GitHub, I could always walk the repository\nback to its original version.My first step in organizing this repository\nwas to set it up as an RStudio Project, which means that it would open a\nfresh R session for everyone who downloaded the code, and all file paths\ncould be written relative to the root directory.  I also\nimplemented renv, which is a great package that allows for\nmanaging package versions. This was particularly important in the case\nof this analysis, because some of the code depended on an old version of\nthe geomorph package for analyzing morphometric landmarks.\nI personally wasn’t very familiar with the package, so even if I had\nwanted to update the code to the newest version, I wouldn’t really have\nbeen able to. Using renv allowed me to run the code with\nthe old version of the package, and it ensures that anyone who wants to\nrun our analysis in the future will be able to restore the packages from\nthe lockfile and immediately proceed without affecting packages on their\nown computer.My next\nstep was to organize the data.  The first principle is that raw\ndata is sacred and should not be modified. So that means no copying and\npasting. Derived data should then be fully reproducible and, by\nextension, disposable. I should be able to just delete all of the\nderived data, and then recreate it by re-running the scripts in the\npipeline. Now, that might be a bit of a pain because some of the\nanalyses might take a long time to run, but it would be possible in a\nfully reproducible analysis. And my job approaching this project\nwas to distinguish between data that was raw––like maybe it was created\nmanually or it was exported from a program somewhere else––versus data\nthat could be regenerated from the analysis code.This is a list of all the data files that I\nwas originally presented with in the GitHub repository, and it was\npretty overwhelming. I had to sort through all of those and figure out\nwhich were raw, which were derived, and where they all went in the\npipeline.And in order\nto do this, I asked the key question, ‘Where did this come from?’ For\neach file, I wanted to know, how was it generated? Can I trace the\norigin of all the files and their columns?I started out by making a spreadsheet\nlisting out each file, and this seemed like a good idea at the time, but\nit ended up being pretty overwhelming, and it was hard for Chelsea to go\nthrough and answer my questions, especially because I was attacking all\nthe files at once, without regard for their importance or the order that\nthey might go in. I was just trying to get my head around\nit.Chelsea and I\ndecided that a better approach would be to start with the most recent\nand up-to-date analysis file; in our case, called\nReviewApril2020.Then, I\ncould look at all of the data files that were read in as inputs. It was\nmuch easier, with this reduced set of files, to\ndetermine……which ones\nwere raw data and which ones were derived data. So, my next step was,\nfor each of those derived data files, to figure out how to re-create it\nfrom the data in the database, using R scripts, so that the process\nwould be fully reproducible.To do that, I created one R script for each\nof the derived data files, and I used that R script to read in data from\nthe database stored on Figshare and modify it to generate these ‘FINAL’\ncsv files. In the background, I kept the original data and used it as a\ncomparison, just to make sure I was accurately recreating that original\ndata.My next step was\nto organize the code in the analysis files. I had to start by\nreorganizing a lot of file paths; they had been written as absolute\npaths, and I rewrote them as relative paths using the wonderful\nhere package.I clarified a lot of the code by renaming\nthe variables with descriptive names and adding clear\ncomments.I also\norganized the code to make sure it was easy to read and\nfollow.And to do that, I\nmade heavy use of the outline panel within RStudio, which you can access\nby clicking on this sort of list icon at the top right corner of the\nscript pane. You’ll notice that this ‘Load packages’ header\ncorresponds to a line in this document outline, and that actually\nhappens automatically within RStudio.You can create these header lines by\nclicking Command + Shift + R on a Mac, and they automatically get added\nto the outlines. And then you can also see that I have a lot of\npackages loaded at the top here, a lot of comments, and everything is\njust a little bit cleaner and easier to read.I wrote a README to document this project,\nincluding an abstract to introduce people to the project, instructions\nfor how to download the data from Figshare and how to run the project,\nrestoring all the packages from the renv lock file, and\nalso information about the contributors.Finally, I wanted to emphasize that I\nhad to go over the same scripts multiple times, and sometimes doing a\nfirst cleanup of the code helped me realize that reorganizing the code\nin different way would actually be clearer, and so I ended up doing what\nfelt like the same work twice. I think that that re-tidying––it\nshouldn’t be dismissed as being inefficient or not doing it right the\nfirst time, because in my case, I had to take a lot of time to just get\nmy head around how the code was organized before I could make informed\ndecisions about how to clean it up.So, I’ve talked about a lot of tools that I\nused in creating this reproducible pipeline, from Figshare to RStudio\nand RStudio Projects, GitHub for version control, Google Drawings in my\ncase for creating this flowchart, and renv for organizing\npackages. And this might be overwhelming if you’ve never used these\ntools before and it feels like you have a lot of things to learn if you\nwant to start implementing reproducible workflows. But I want to\nemphasize that there are smaller things that you can do to move in the\nright direction, even if you can’t jump right into using best practices\nfrom the get-go.For\nexample, don’t be afraid to tell stories with your comments. I like to\nbe very conversational; I like to say ‘Okay, now we’re going to do X’,\n‘I noticed this about the output, and that causes me to make this\ndecision about the model’ For example here, I wrote, ‘To make\nthese calculations easier, I’m going to….’ do XYZ, and then I have some\nlines of code. Now, you’ll notice here that I only have about\nfour lines of code, and a LOT of lines of comments, and not all of my\nscripts look like that. But I just want to get the point across that\nit’s really okay to write essays in your code and to have your code\ncomments be informal, conversational and very\ndescriptive.Something that\nChelsea did in her raw code files that I found very helpful when I went\nthrough and organized them, was to keep track, in the comments, of\npackage versions and decisions that were made in the analyses, and file\npaths. So for example, I already discussed how we had to use an older\nversion of the package geomorph, and conveniently, Chelsea\nhad recorded the version number of the older version that she was using,\nso it was much easier for me to then restore that version when I started\nusing renv.You’ll notice that this script is far\nfrom being perfect; it’s still using absolute file paths, it hasn’t yet\nbeen edited to use the here package, but even without using\nall best practices in reproducibility, just having these introductory\ncomments is really helpful.I want to conclude this talk by just\nemphasizing that we really need to teach workflow in graduate programs,\nundergraduate programs, whenever we teach coding. I would argue that,\nfor biologists, workflow, organization, and reproducibility are some of\nthe things that take up the greatest proportion of our research time.\nThey are actually more valuable skills sometimes than the actual\nstatistical analysis. They are also not intuitive. I think it\ncan be easy, as a student, to assume that if you’re just an organized\nperson, you will magically know how to organize your code. And then it’s\neasy to get frustrated when that doesn’t come naturally and when no one\nhas really taught you what the best practices are. I think this\nresource, written by Jenny Bryan and Jim Hester, ‘What they forgot to\nteach you about R’, is a great example of this. This resource focuses on\na lot of the workflow tactics that I’ve been talking about here, like a\nproject-oriented workflow, and it’s titled ‘What they forgot to teach\nyou about R’. And I think for many students, that’s exactly true. They\ndo forget to teach you the workflow stuff, and that’s sometimes the most\nimportant part.Here’s\na list of resources that I put together related to the various tools for\nreproducibility that I’ve talked about in this\npresentationAnd I would be\nhappy to take any questions at the Q&A session right after this\ntalk. Thank you very much.Hello everyone! Thank you for being\nhere. My name is Kaija Gahm, and I’m a data manager at the Cary\nInstitute of Ecosystem Studies. Late last year, my colleague Chelsea had\njust finished her Master’s thesis, and my supervisor asked me to help\nher to organize her code and analysis into a reproducible workflow that\nwe could reference in the manuscript that we were putting together.\n In this talk, I’m going to be speaking about my experience\nimplementing data management and reproducibility tools retroactively,\nfor an existing project, even if best practices weren’t used from the\nbeginning. I’ll also discuss organizing and making sense of someone\nelse’s analyses, approaching the project as an outsider.A little bit of background on the project:\nIt consisted of some pretty typical ecological data––measurements of\nlake dissolved organic carbon, morphometric landmarks of bluegill\nsunfish, and a bunch of statistical analyses and\nfigures.Some of the\nchallenges I knew I was going to face were that the data had largely\nbeen edited manually in Excel, or copied and pasted from other files\nwith uncertain origin.There were many file versions, and it\nwasn’t really clear how the data and code tied together.The goal of my work was to compile all\nof this data and code into a reproducible analysis. I knew that we were\ngoing to be storing the data on Figshare; all of the analyses would be\nconducted in RStudio and stored on GitHub. The data and code could be\nthen referenced in the final manuscript.To begin my work, I asked Chelsea to start\nby uploading all of her files to GitHub.I think GitHub is a very important\ntool, not just technically, for version control, but also\npsychologically. If everything was being version controlled, then I\nwould have the power to edit and delete without being burdened by the\nidea that I might be deleting something very important. I knew that as\nlong as everything was on GitHub, I could always walk the repository\nback to its original version.My first step in organizing this repository\nwas to set it up as an RStudio Project, which means that it would open a\nfresh R session for everyone who downloaded the code, and all file paths\ncould be written relative to the root directory.  I also\nimplemented renv, which is a great package that allows for\nmanaging package versions. This was particularly important in the case\nof this analysis, because some of the code depended on an old version of\nthe geomorph package for analyzing morphometric landmarks.\nI personally wasn’t very familiar with the package, so even if I had\nwanted to update the code to the newest version, I wouldn’t really have\nbeen able to. Using renv allowed me to run the code with\nthe old version of the package, and it ensures that anyone who wants to\nrun our analysis in the future will be able to restore the packages from\nthe lockfile and immediately proceed without affecting packages on their\nown computer.My next\nstep was to organize the data.  The first principle is that raw\ndata is sacred and should not be modified. So that means no copying and\npasting. Derived data should then be fully reproducible and, by\nextension, disposable. I should be able to just delete all of the\nderived data, and then recreate it by re-running the scripts in the\npipeline. Now, that might be a bit of a pain because some of the\nanalyses might take a long time to run, but it would be possible in a\nfully reproducible analysis. And my job approaching this project\nwas to distinguish between data that was raw––like maybe it was created\nmanually or it was exported from a program somewhere else––versus data\nthat could be regenerated from the analysis code.This is a list of all the data files that I\nwas originally presented with in the GitHub repository, and it was\npretty overwhelming. I had to sort through all of those and figure out\nwhich were raw, which were derived, and where they all went in the\npipeline.And in order\nto do this, I asked the key question, ‘Where did this come from?’ For\neach file, I wanted to know, how was it generated? Can I trace the\norigin of all the files and their columns?I started out by making a spreadsheet\nlisting out each file, and this seemed like a good idea at the time, but\nit ended up being pretty overwhelming, and it was hard for Chelsea to go\nthrough and answer my questions, especially because I was attacking all\nthe files at once, without regard for their importance or the order that\nthey might go in. I was just trying to get my head around\nit.Chelsea and I\ndecided that a better approach would be to start with the most recent\nand up-to-date analysis file; in our case, called\nReviewApril2020.Then, I\ncould look at all of the data files that were read in as inputs. It was\nmuch easier, with this reduced set of files, to\ndetermine……which ones\nwere raw data and which ones were derived data. So, my next step was,\nfor each of those derived data files, to figure out how to re-create it\nfrom the data in the database, using R scripts, so that the process\nwould be fully reproducible.To do that, I created one R script for each\nof the derived data files, and I used that R script to read in data from\nthe database stored on Figshare and modify it to generate these ‘FINAL’\ncsv files. In the background, I kept the original data and used it as a\ncomparison, just to make sure I was accurately recreating that original\ndata.My next step was\nto organize the code in the analysis files. I had to start by\nreorganizing a lot of file paths; they had been written as absolute\npaths, and I rewrote them as relative paths using the wonderful\nhere package.I clarified a lot of the code by renaming\nthe variables with descriptive names and adding clear\ncomments.I also\norganized the code to make sure it was easy to read and\nfollow.And to do that, I\nmade heavy use of the outline panel within RStudio, which you can access\nby clicking on this sort of list icon at the top right corner of the\nscript pane. You’ll notice that this ‘Load packages’ header\ncorresponds to a line in this document outline, and that actually\nhappens automatically within RStudio.You can create these header lines by\nclicking Command + Shift + R on a Mac, and they automatically get added\nto the outlines. And then you can also see that I have a lot of\npackages loaded at the top here, a lot of comments, and everything is\njust a little bit cleaner and easier to read.I wrote a README to document this project,\nincluding an abstract to introduce people to the project, instructions\nfor how to download the data from Figshare and how to run the project,\nrestoring all the packages from the renv lock file, and\nalso information about the contributors.Finally, I wanted to emphasize that I\nhad to go over the same scripts multiple times, and sometimes doing a\nfirst cleanup of the code helped me realize that reorganizing the code\nin different way would actually be clearer, and so I ended up doing what\nfelt like the same work twice. I think that that re-tidying––it\nshouldn’t be dismissed as being inefficient or not doing it right the\nfirst time, because in my case, I had to take a lot of time to just get\nmy head around how the code was organized before I could make informed\ndecisions about how to clean it up.So, I’ve talked about a lot of tools that I\nused in creating this reproducible pipeline, from Figshare to RStudio\nand RStudio Projects, GitHub for version control, Google Drawings in my\ncase for creating this flowchart, and renv for organizing\npackages. And this might be overwhelming if you’ve never used these\ntools before and it feels like you have a lot of things to learn if you\nwant to start implementing reproducible workflows. But I want to\nemphasize that there are smaller things that you can do to move in the\nright direction, even if you can’t jump right into using best practices\nfrom the get-go.For\nexample, don’t be afraid to tell stories with your comments. I like to\nbe very conversational; I like to say ‘Okay, now we’re going to do X’,\n‘I noticed this about the output, and that causes me to make this\ndecision about the model’ For example here, I wrote, ‘To make\nthese calculations easier, I’m going to….’ do XYZ, and then I have some\nlines of code. Now, you’ll notice here that I only have about\nfour lines of code, and a LOT of lines of comments, and not all of my\nscripts look like that. But I just want to get the point across that\nit’s really okay to write essays in your code and to have your code\ncomments be informal, conversational and very\ndescriptive.Something that\nChelsea did in her raw code files that I found very helpful when I went\nthrough and organized them, was to keep track, in the comments, of\npackage versions and decisions that were made in the analyses, and file\npaths. So for example, I already discussed how we had to use an older\nversion of the package geomorph, and conveniently, Chelsea\nhad recorded the version number of the older version that she was using,\nso it was much easier for me to then restore that version when I started\nusing renv.You’ll notice that this script is far\nfrom being perfect; it’s still using absolute file paths, it hasn’t yet\nbeen edited to use the here package, but even without using\nall best practices in reproducibility, just having these introductory\ncomments is really helpful.I want to conclude this talk by just\nemphasizing that we really need to teach workflow in graduate programs,\nundergraduate programs, whenever we teach coding. I would argue that,\nfor biologists, workflow, organization, and reproducibility are some of\nthe things that take up the greatest proportion of our research time.\nThey are actually more valuable skills sometimes than the actual\nstatistical analysis. They are also not intuitive. I think it\ncan be easy, as a student, to assume that if you’re just an organized\nperson, you will magically know how to organize your code. And then it’s\neasy to get frustrated when that doesn’t come naturally and when no one\nhas really taught you what the best practices are. I think this\nresource, written by Jenny Bryan and Jim Hester, ‘What they forgot to\nteach you about R’, is a great example of this. This resource focuses on\na lot of the workflow tactics that I’ve been talking about here, like a\nproject-oriented workflow, and it’s titled ‘What they forgot to teach\nyou about R’. And I think for many students, that’s exactly true. They\ndo forget to teach you the workflow stuff, and that’s sometimes the most\nimportant part.Here’s\na list of resources that I put together related to the various tools for\nreproducibility that I’ve talked about in this\npresentationAnd I would be\nhappy to take any questions at the Q&A session right after this\ntalk. Thank you very much.\n\nVideo\nFor the conference, I embedded captions in the video using iMovie,\nwhich was a pretty clunky process but was all I had access to at the\ntime. There are some glitches and at one point I accidentally inserted\nthe caption for a previous slide. On the whole, I hope the captions are\nbetter than nothing.\n\n\nResources\nThe resource links from the second to last slide didn’t translate\nwell to this format, so here they are:\nRStudio\ninteractive tutorials\nHappy Git with R (Jenny\nBryan and Jim Hester)\nProject-oriented\nworkflow with RStudio projects (Jenny Bryan)\nUsing\nRStudio projects\nUsing the here\npackage\nHow to use renv\nKeeping\na Paper Trail: Data Management Skills for Reproducible Science(Kate\nLaskowski)\nMake a README\nGet an embed\nlink for a Google drawing so you can put it into e.g. a\nREADME\nUsing the\n`tree` command in the terminal to make a file tree\nThe document\noutline in RStudio\nHeaders\nand comments in RStudio (Y. Wendy Hyunh)\nWhat they forgot to teach you about\nR (Jenny Bryan and Jim Hester)\nFinally, you can download the video and the annotated slides from OSF\nhere.\n\n\n\n",
    "preview": "projects/2021-07-13-retroactivelyReproducible/slideImages/0001.jpg",
    "last_modified": "2022-07-12T23:18:08-07:00",
    "input_file": {}
  },
  {
    "path": "projects/2021-05-13-greenT/",
    "title": "greenT: Exploring grapheme-color synesthesia",
    "description": "For my whole life, I've associated colors with letters and numbers. This is called grapheme-color synesthesia. For my second major project in Shiny, I built an app to help me explain synesthesia to friends, and to learn more about the experiences of other synesthetes.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: The digits 0-9, according to my brain\n\n\n\nI have what’s known as grapheme-color synesthesia. I experience letters and numbers as having colors associated with them. I don’t literally see colors floating in the air, but I’ve consistently associated a color with each letter and number for most of my life.\nWhen I tell my friends about my synesthesia, they want to know what color their name is. I wanted a way to quickly represent any word or words in the colors I see it in. I did a bit of googling to see if anyone had come up with a synesthesia simulator.\nThe closest thing I found was Bernadette Sheridan’s website synesthesia.me, which did exactly what I wanted to do, but only for Bernadette’s own colors! Cool, but not useful to me or to any other synesthetes (like my brother) who might want to show me their own colors.\nSo, I built this app. The source code for the app is here.\nFor a guided tour of the parts of the app, see this blog post\nTo read an explanation of the code used in the app, see this blog post\nThis was my second major Shiny project, after the YGDP Dashboard.\nHere’s the app, embedded:\n\n\nIf you have questions about this work, please feel free to get in touch. If you want to suggest an improvement or change to the app, or to report a bug or file a pull request, head over to the github page for the project.\nTo read more, make sure to check out my blog posts about this app. Part 1, Part 2\n\n\n\n",
    "preview": "projects/2021-05-13-greenT/numbers.png",
    "last_modified": "2021-10-27T16:50:01-07:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "projects/2021-02-05-ygdp-dashboard/",
    "title": "YGDP Dashboard",
    "description": "From June through December 2020, I taught myself Shiny and ended up creating a pretty complicated interactive dashboard for exploring dialect variation across the US. Here's a bit about my experience using Shiny, as well as the app itself for you to play around with.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-05",
    "categories": [],
    "contents": "\nA map showing interpolated ratings data for the sentence “Here’s you some money.” A rating of 1 is “totally unacceptable sentence, even in casual conversation”, while 5 is “totally acceptable sentence.”Studying language across the US\nThe Yale Grammatical Diversity Project (YGDP) has been collecting survey data about syntax diversity in U.S. English for over five years.\nYou can read about the YGDP’s work here, but in a nutshell:\nWe put out surveys online to learn about people’s dialects across the United States\nWe ask survey participants to judge whether test sentences, such as “Here’s you a piece of pizza”, seem acceptable in casual conversation.\nWe’re not interested in “proper” or “correct” English–we want to know how people actually talk in real life across the country.\n(If you want to learn about the survey and data analysis methods in great detail, check out the comprehensive mapbook that we published earlier this year.)\nIn the past, there has been a lot of research into regional language diversity, but a lot of it has focused on lexical (words, vocabulary) diversity. A great example of that is the New York Times dialect quiz, which asks questions like “What do you call it when rain falls while the sun is shining?”, with answers ranging from “sunshower” to “the wolf is giving birth”.\nThe YGDP focuses more on syntactic (“grammatical”; related to syntax) diversity. So, they want to know how people put sentences together, not necessarily which words they use.\nThe Scots Syntax Atlas has done some great work along these lines in Scotland. The YGDP’s methods are a little different, but they’re asking the same types of questions about grammatical variation.\nI’m actually not a linguist, so I’m not going to explain the research in detail. To learn more about the YGDP’s research, you can check out their website. Head over to the grammatical phenomena pages to learn more about constructions like “All the further”, “Done my homework”, and “Come with”.\nData work\nIn 2019, I started working with the YGDP to pull together and organize their survey data. One of our primary goals was to make our findings easily accessible and explorable for anyone who might be interested.\nI had heard about R Shiny, but I had never created a Shiny app. In the spring of 2020, I talked to Jim Wood, one of the YGDP’s supervisors, about the possibility of developing a Shiny app to display the YGDP’s findings in a colorful, interactive way. Jim was excited about the idea, even though I had been very clear that I’d be starting absolutely from scratch and learning Shiny along the way.\nFrom June through December 2020, I dove in headfirst. I created a couple simple Shiny apps as practice, but then I set about developing a pretty complicated Shiny dashboard. I’ve always found that the best way to learn a new tool is to plunge in and use it, learning as you go. Learning Shiny was the ultimate baptism by fire. I think I started trying to create hierarchical selectInputs by about day 2, when I was still far from understanding the fundamentals of Shiny’s inputs and outputs. Next came the dashboard layout, which I achieved using shinydashboard and eventually shinydashboardPlus. I learned to insert a map using leaflet, another package I had never dabbled in.\n\n\n\nFigure 1: Hierarchical selectInputs: the choices in the ‘Sentence 1’ menu depend on the selection in ‘Survey’.\n\n\n\n\n\n\nFigure 2: A map made with the leaflet package\n\n\n\nThe dashboard is still evolving, but here is the current version. I’m immensely proud of how far I’ve come with Shiny, and I’m really excited about how this dashboard will allow the YGDP, and the general public, to explore the linguistic survey data in great detail.\n\n\nIf you have questions about this work, please feel free to get in touch. If you want to suggest an improvement or change to the dashboard, or to report a bug or file a pull request, head over to the github page for the project. I’ll be leaving the project shortly, so I’m not sure how actively the dashboard will be maintained, but suggestions are certainly welcome.\n\n\n\n",
    "preview": "projects/2021-02-05-ygdp-dashboard/hexMap.png",
    "last_modified": "2021-10-27T16:50:01-07:00",
    "input_file": {},
    "preview_width": 1434,
    "preview_height": 810
  },
  {
    "path": "projects/2021-01-29-tadpoles/",
    "title": "The Tadpole Olympics",
    "description": "For my B.S. thesis project at Yale, I measured how fast wood frog tadpoles swim in response to a simulated predator attack, and how that swimming speed relates to their developmental rate. It turns out that developing fast comes with a performance cost, and that tradeoff might help to explain some of the patterns we've observed in wood frogs' developmental rates in the past.\nA short description of the post.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-01-29",
    "categories": [],
    "contents": "\nIntroduction\nFor my senior (B.S.) thesis in E&EB at Yale, I studied a tradeoff between growth rate and swimming performance in wood frog tadpoles.\nWhat does that mean? Basically, my collaborator Andis and I raised tadpoles in the lab under warm and cold conditions. Since frogs are cold-blooded, the warm tadpoles grew faster than the cold tadpoles.\nOn the one hand, growing rapidly is good for animals like wood frogs, who need to escape from predators and compete for mating opportunities. But when we stuck our tadpoles into an arena and tested how fast they swam, we found that the tadpoles pay a cost for that fast growth. The fast-growing (warm) tadpoles didn’t perform as well in their swimming trials as the slow-growing (cold) tadpoles.\nThis kind of tradeoff exists in other animals too, but we tested several populations of tadpoles and found that the effect was really consistent, no matter which population the tadpoles came from.\nWhat determines how fast a tadpole grows?\nWhy is this important? Well, our findings might help to explain a pattern that my lab has been wondering about for a while, called “countergradient variation”. Here’s how it works. We know that growing fast is helpful to tadpoles for a lot of reasons: they might have better mating or survival success as adults (though we don’t know this for sure); they might get too big for aquatic predators to eat while they’re still tadpoles; they can be sure to leave their ephemeral ponds before the water dries up in the summer; and they can get a head start on terrestrial life, perhaps outcompeting slower-growing tadpoles.\nTwo factors can help frogs grow fast: 1) environmental factors, like the temperature of the water they live in, and 2) genetic factors: that is, their genes can predispose them to “intrinsically” fast growth, independent of their environment.\nLet’s say we have two ponds in an imaginary forest, called “Fire Pond” and “Ice Pond” because one has really warm water and the other has really cold water.\nEnvironmental factors\nWe know that the tadpoles living in Fire Pond will grow faster than the tadpoles living in Ice Pond, since frogs are cold-blooded and their growth is pretty heavily influenced by the temperature of the water around them. Warmer water -> faster growth! This is the relationship that we made use of to manipulate our frogs’ developmental rates in the lab.\nGenetic factors\nFire Pond and Ice Pond are pretty far apart, and wood frogs tend to return to the same ponds where they grew up to breed. So, the frog populations in Fire and Ice can evolve roughly independently of each other.\nSo, if fast growth is unequivocally beneficial, then we might expect that both the Fire and Ice populations would evolve toward faster intrinsic growth and development. Put another way: natural selection would favor tadpoles that had genes for fast growth over those who grew more slowly, independent of the surrounding water temperature. We would expect to see this evolutionary trend in both ponds. Tadpoles from both Fire and Ice would evolve fast intrinsic growth rates, such that the Ice tadpoles will grow fast despite the cold water, and the Fire tadpoles will grow EXTRA FAST from the combined effects of genetics and warm water.\nBut this pattern is not what researchers, including the Skelly lab, have observed! When other studies have taken tadpoles from Fire and Ice and reared them at the same temperature (so, removing the effects of the different pond temperatures), they find that tadpoles from Fire grow more slowly than tadpoles from Ice. So, for some reason, those Fire frogs aren’t evolving to grow super fast intrinsically, even though doing so would let them have an edge over the chilly little Ice frogs.\nPutting it together\nThe pattern that I’ve described above is called “countergradient variation” because the frogs’ intrinsic (genetic) growth rates run in the opposite direction (counter-gradient) as their environmental circumstances.\nFor a while, we’ve wondered why we see this pattern. Why don’t the Fire tadpoles make the most of what seems like an advantage: their warm water? The countergradient pattern suggests that there must be a tradeoff, something that makes growing EXTRA FAST not quite so advantageous after all.\nThat’s where my study comes in. It turns out, growing fast comes with a cost in burst swimming speed! If you’re a tadpole, you might get bigger faster, but if you also swim more slowly than your slower-growing peers, you’re at risk of getting gobbled up by a predator. If that happens, all that growing would be for nothing.\nPaper\nMy collaborators and I published this work, and if you want to read more, you can download the paper. But I hope my description is a little easier to get through than the paper itself.\nIf you want a more detailed summary of the paper, with reference to each of its figures, Andis did a great job explaining it in his blog post.\nTalk\nIf you want to learn more about my work and would prefer to see a video of a talk I gave, aimed at a more general audience than the published paper, you can watch it below. This talk was given as part of the 2020 end-of-year E&EB Senior Symposium, which I organized as a remote event due to the COVID-19 pandemic.\nAcknowledgements and reflections\nThis work was made possible with the tremendous support of the Skelly Lab in the Yale School of the Environment. Andis Arietta, in particular, was a great mentor and collaborator. You should check out his research on his website and follow him on Twitter.\nThanks to Kirby Broderick and Simon Stump for help with the MATLAB code I wrote to analyze all my tadpole videos. Thanks to Joaquin Bellomio for help ground-truthing videos, and to Adriana Rubinstein and David Skelly for assistance in the lab.\nFinally, I want to recognize the privilege that allowed me to publish my research in 2020. Apart from summer fellowships, Yale doesn’t pay undergrads for research hours beyond course credit. I was able to spend a lot of extra hours in the lab during my senior year, hours that were not an option for a lot of my fellow students because they had to prioritize fulfilling their student income contributions (SIC) or working to support their families. Yale needs to abolish the SIC (SUN at Yale is doing great work toward this end) and pay undergraduate researchers to help even out these opportunities.\nWhen the COVID-19 pandemic hit, I was able to return to a calm and supportive home environment, where I had lots of time to work on preparing this manuscript for publication. Many of my friends had their thesis work cut short, and they had far more pressing things on their minds than writing and publishing papers. Across the scientific community, some academics are publishing more under lockdowns, while others (disproportionately women, people of color, those caring for children, etc.–aka people who were already at a disadvantage before this pandemic made things worse) have less time, not more, to devote to their work (see this and this, although this calls things into question. I don’t have good data for disparities by race and family status, and if/when I find papers that support/refute my intuition above, I’ll change my view accordingly!).\nI hope that in the coming years, we will see academia recognize that traditional metrics of success, like publication numbers and grant awards, are far from equitable, especially in the wake of this troubled year.\nThank you for reading, and if you have any questions about my work, please feel free to reach out!\n\n\n\n",
    "preview": "projects/2021-01-29-tadpoles/tadpole.png",
    "last_modified": "2021-10-27T16:35:25-07:00",
    "input_file": {},
    "preview_width": 1421,
    "preview_height": 575
  }
]
