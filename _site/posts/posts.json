[
  {
    "path": "posts/2020-02-10-upping-your-pipe-game/",
    "title": "%$%: upping your pipe game",
    "description": "I love the magrittr/dplyr pipe: %>%. But it's meant to work with tidyverse functions, and it doesn't always work well with base R functions that take a single data frame column as input. Here, I use data about my friends' pets to explain how a different magrittr pipe, %$%, solves that problem.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nWhat do I do when %>% doesn’t work?\nContext\nI love the %>% pipe. Originally from magrittr, it’s now characteristic of most tidyverse code. Using %>% has revolutionized how I write code in R. But sometimes the basic pipe falls short.\ntable() is one of my favorite functions for exploring data in R: it creates a frequency table of values in a vector. I use table() to do sanity checks on my data, make sure that all factor levels are present, and generally get a sense of how my observations are distributed.\nA while back, though, I noticed that table() didn’t play nice with the %>% pipe.\nI’ve collected some data on my friends’ pets. Here it is (using pseudonyms, in case anyone has a secret pet they don’t want the world to know about…).\n\n\n\nFigure 1: This is one of the cats in the data frame below. She would like to hold your hand.\n\n\n\n\n\n# Load magrittr\nlibrary(magrittr)\nlibrary(dplyr)\n\n# Create data\npets <- data.frame(\n  friend = c(\"Mark\", \"Mark\", \"Kyle\", \"Kyle\", \"Miranda\", \"Kayla\", \n             \"Kayla\", \"Kayla\", \"Adriana\", \"Adriana\", \"Alex\", \"Randy\", \"Nancy\"), \n  pet = c(\"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"dog\", \"cat\", \"lizard\", \n          \"cat\", \"cat\", \"dog\", \"dog\", \"woodpecker\"), \n  main_pet_color = c(\"brown\", \"brown\", \"multi\", \"multi\", \"brown\", \n                     \"brown\", \"brown\", \"orange\", \"black\", \"white\", \n                     \"multi\", \"white\", \"multi\")) \n\n# Look at the data\npets\n\n\n    friend        pet main_pet_color\n1     Mark        cat          brown\n2     Mark        cat          brown\n3     Kyle        cat          multi\n4     Kyle        cat          multi\n5  Miranda        cat          brown\n6    Kayla        dog          brown\n7    Kayla        cat          brown\n8    Kayla     lizard         orange\n9  Adriana        cat          black\n10 Adriana        cat          white\n11    Alex        dog          multi\n12   Randy        dog          white\n13   Nancy woodpecker          multi\n\nUnsurprisingly, it looks like there are a lot of cats and dogs! There are also a lot of brown pets and a lot of multicolored ones. Let’s say I want to see a frequency table of the pet colors. I know that I can do this with table(), like so:\n\n\n# Make a frequency table of pet colors\ntable(pets$main_pet_color)\n\n\n\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nBut if I want to use tidy syntax, I might try to do it this way instead:\n\n\npets %>%\n  table(main_pet_color)\n\n\nError in table(., main_pet_color): object 'main_pet_color' not found\n\nWhat’s up with this? The syntax should work. pet is definitely a valid variable name in the data frame pets, and if I had used a different function, like arrange(), I would have had no problems:\n\n\n# Arrange the data frame by pet color\npets %>% arrange(main_pet_color) # works fine!\n\n\n    friend        pet main_pet_color\n1  Adriana        cat          black\n2     Mark        cat          brown\n3     Mark        cat          brown\n4  Miranda        cat          brown\n5    Kayla        dog          brown\n6    Kayla        cat          brown\n7     Kyle        cat          multi\n8     Kyle        cat          multi\n9     Alex        dog          multi\n10   Nancy woodpecker          multi\n11   Kayla     lizard         orange\n12 Adriana        cat          white\n13   Randy        dog          white\n\nSo why doesn’t this work with table()?? This problem has driven me crazy on several occasions. I always ended up reverting back to the table(pets$main_pet_color) syntax, but I was not happy about it.\nTurns out, there’s a simple fix.\nSolution\nIntroducing… a new pipe! %$% is called the “exposition pipe,” according to the magrittr package documentation, and it’s basically the tidy version of the with() function, which I wrote about previously.\nIf we simply swap out %>% for %$% in our failed code above, it works!\n\n\n# Make a frequency table of pet colors\npets %$% table(main_pet_color)\n\n\nmain_pet_color\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nImportant note: Make sure you have magrittr loaded if you want to use this pipe. dplyr includes the basic %>%, but not the other magrittr pipes.\nWhy it works\nThe traditional pipe, %>%, works by passing a data frame or tibble into the next function. But that only works if the function you’re piping to is set up to take a data frame/tibble as an argument!\nFunctions in the tidyverse, like arrange(), are set up to take this kind of argument, so that piping works seamlessly. But many base R functions take vectors as inputs instead.\nThat’s the case with table(). When we write table(pets$main_pet_color), the argument pets$main_pet_color is a vector:\n\n\n# This returns a vector\npets$main_pet_color\n\n\n [1] \"brown\"  \"brown\"  \"multi\"  \"multi\"  \"brown\"  \"brown\"  \"brown\" \n [8] \"orange\" \"black\"  \"white\"  \"multi\"  \"white\"  \"multi\" \n\nWhen we try to pass pets into table() with the pipe, table() expects a vector but gets a data frame instead, and it throws an error.\nThe %$% pipe “exposes” the column names of the data frame to the function you’re piping to, allowing that function to make sense of the data frame that is passed to it.\nOutcome\nThe exposition pipe is great for integrating non-tidyverse functions into a tidy workflow. The outcome for me is that I can finally make frequency tables to my heart’s content, without “code switching” back from tidy to base R syntax. Of course, the downside is that you do have to install magrittr, which is sometimes an extra dependency that I don’t want to deal with. But it’s nice to have the option!\n\n\n\nFigure 2: Congrats, you made it to the end! Here are some more cats for you.\n\n\n\nResources\nmagrittr has a couple other pipes, too: %T% and %<>%. The package also has some nice aliases for basic arithmetic functions that allow them to be incorporated into a chain of pipes. To read more about these magrittr options, scroll to the bottom of the magrittr vignette.\nNote: The image at the top of this post was modified from the magrittr documentation.\n\n\n\n",
    "preview": "posts/2020-02-10-upping-your-pipe-game/magrittr.jpg",
    "last_modified": "2021-02-11T19:28:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/",
    "title": "Some lessons from rstudio::conf 2020",
    "description": "Some of my thoughts about R and the R community after attending the 2020 rstudio::conf in San Francisco.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2020-02-03",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nWhen I started in R a few years ago, I never thought I would have any place at a coding conference for computer people. But thanks to some help from my lab and my university, last week I flew out to sunny San Francisco for the 2020 rstudio::conf, and I had a blast. Here are some things I learned and some avenues I’m excited to explore in the future.\n1. The R community is awesome.\n\n\n\nFigure 1: This art is by RStudio artist-in-residence Allison Horst. She does amazing illustrations! You can find her work on her GitHub page.\n\n\n\nI was pretty nervous to attend this conference. I’ve never really considered myself a computer person, and I learned R pretty much accidentally through my work in biology. In my experience, people who do a lot of computer programming tend to use a lot of intimidating jargon, and I was scared I’d flounder.\nI was surprised at how genuinely welcome I felt. At meals and talks, I sat down next to random people and asked what they did or how they knew R. I met people with so many diverse interests! Among others:\n• A linguist now working for a nonprofit • A neuroscience PhD now working for Facebook • Several young professors teaching data science at their schools • A woman who flew from Brazil (!!) to attend the conference • Someone just getting to know R • Two young people who interned at RStudio, despite not being experts in R • An RStudio bigwig (swoon!) • A professor working on genomic data • A researcher at the Smithsonian • An aquatic ecologist who uses R for work • So many people! You get the gist. •\nAnd they were all happy to talk to me!\n2. So much happens on Twitter.\nI joined Twitter on a whim this fall, and it has been awesome. I learned about this conference through Twitter. I’ve found some internships through Twitter. And by following the #rstats hashtag and some key people in the R community, I’ve learned all sorts of tips and tricks about the kinds of things you can do with Twitter.\nApart from that, lots of people were live-tweeting the rstudio::conf, and I tried my hand at that, too! A highlight was when the one and only Hadley Wickham liked one of my tweets.\n3. I need to start making Shiny apps.\nAs I said in my tweet (the one that Hadley liked!), this conference convinced me that I really should start building apps with R Shiny as soon as possible.\nWhy haven’t I done this already?\nThe main reason is that the word “app” strikes fear into my heart. Surely I can’t develop an app??\nShiny apps are web apps, which is a little less intimidating, somehow. The basic gist of Shiny apps is that they are ways to explore your data interactively. That’s it. Some Shiny apps can get pretty complicated. For example, here’s a cool dashboard for visualizing tweets about the 2019 rstudio::conf. As you can see, it’s pretty fancy.\n\n\n\nBut Shiny apps can also be pretty simple, like this one, which shows a simple histogram of the duration of eruptions of the Old Faithful geyser:\n\n\n\nThis app just lets the user change the number of histogram bins, include a density curve, and show the individual observations on the x axis. Pretty simple, but still so much better than a static visualization! I really have no excuse not to make something like this.\nIn particular, I can’t wait to build an app for the Yale Grammatical Diversity Project (YGDP). I’m currently working with the YGDP to help organize their database of linguistic survey data. I’ve already created some reports in RMarkdown to help visualize the data (you can see an example here). But wouldn’t this be so much better if it were interactive??\n\n\n\n4. Bonus: fun with hex stickers\nI finally got to experience the hype about hexagon stickers for R packages. They are so pretty and fun, and they fit together so nicely! I picked up a whole bunch:\n\n\n\nAnd, funnily enough, I’ve been playing with hexagons as wall decorations for a while now, before I even knew about hex stickers…\n\n\n\n…so obviously, the possibilities now are truly endless. Hexagons on hexagons? A hex collage on the wall and one on my computer? Wow.\n\n\n\n",
    "preview": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/rstudioconf2020.png",
    "last_modified": "2021-02-11T18:42:23-05:00",
    "input_file": {},
    "preview_width": 1796,
    "preview_height": 766
  },
  {
    "path": "posts/2019-10-12-loading-packages-efficiently/",
    "title": "Loading packages efficiently",
    "description": "A trick I sometimes use to load a whole bunch of packages for every script in a project, while saving myself some typing. (Preview image from https://towardsdatascience.com/a-comprehensive-list-of-handy-r-packages-e85dad294b3d)",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-12",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from towards data science\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nEspecially in a project with many different scripts, it can be challenging to keep track of all the packages you need to load. It’s also easy to lose track of whether or not you’ve incorporated package loading into the script itself until you switch to a new computer or restart R and all of a sudden, your packages need to be re-loaded.\nContext\nWhen I was first starting out in R, I learned quickly to load packages all together at the top of a script, not along the way as I needed them. But it took a while, until I started using R Projects, before I decided to centralize package loading above the script level. I was sick of having to deal with loading the right packages at the right times, so I decided to just streamline the whole thing.\nSolution\nMake a separate R script, called “libraries.R” or “packages.R” or something. Keep it consistent. Mine is always called “libraries,” and I keep it in my project folder.\n\n\n\nThe script looks something like this (individual packages may vary, of course):\n\n\n\nThen, at the top of each analysis script, I can simply source the libraries script, and all the libraries I need load automatically.\n\n\n\nOutcome\nI can easily load libraries in the context of a single R Project, keep track of which ones are loaded, and not have to worry about making my scripts look messy with a whole chunk of library() commands at the top of each one. It’s also straightforward to pop open the “libraries” script whenever I want to add a new package or delete one.\n\n\n\n",
    "preview": "posts/2019-10-12-loading-packages-efficiently/pkgs.png",
    "last_modified": "2021-02-11T18:43:29-05:00",
    "input_file": {},
    "preview_width": 1097,
    "preview_height": 233
  },
  {
    "path": "posts/2019-09-13-initializing-an-empty-list/",
    "title": "Initializing an empty list",
    "description": "When you're using a for loop to fill up a list, sometimes it's a good idea to initialize the list ahead of time, with the right number of elements but with no data. But actually doing this is a little harder than I had anticipated.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-09-13",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I initialize an empty list for use in a for-loop or function?\nContext\nSometimes I’m writing a for-loop (I know, I know, I shouldn’t use for-loops in R, but sometimes it’s just easier. I’m a little less comfortable with apply functions than I’d like to be) and I know I’ll need to store the output in a list. Once in a while, the new list will be similar in form to an existing one, but more often, I just need to start from scratch, knowing only the number of elements I want to include.\nThis isn’t a totally alien thing to need to do––it’s pretty familiar if you’re used to initializing empty vectors before for-loops. There’s a whole other debate to be had about whether or not it’s acceptable to start with a truly empty vector and append to it on every iteration of the loop or whether you should always know the length beforehand, but I’ll just focus on the latter case for now.\nAnyway, initializing a vector of a given length is easy enough; I usually do it like this:\n\n\ndesired_length <- 10 # or whatever length you want\nempty_vec <- rep(NA, desired_length)\n\n\n\nI couldn’t immediately figure out how to replicate this for a list, though. The solution turns out to be relatively simple, but it’s just different enough that I can never seem to remember the syntax. This post is more for my records than anything, then.\nSolution\nInitializing an empty list turns out to have an added benefit over my rep(NA) method for vectors; namely, the list ends up actually empty, not filled with NA’s. Confusingly, the function to use is vector, not list.\n\n\ndesired_length <- 10 # or whatever length you want\nempty_list <- vector(mode = \"list\", \n                     length = desired_length)\n\nstr(empty_list)\n\n\nList of 10\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n\nOutcome\nVoilà, an empty list. No restrictions on the data type or structure of the individual list elements. Specify the length easily. Useful for loops, primarily, but may have other applications I haven’t come across yet.\nI learned something interesting recently, during the R working group I take part in through the Yale School of Forestry. See how, above, I initialized a list using the vector() function? Why is that?\nMaybe this should have been obvious, but a list is actually a type of vector. The thing that I would typically call a vector (a series of elements, all of the same type), is more specifically an ‘atomic vector’. A list, on the other hand, is just a vector that can contain more than one type of data.\n\n\n\n",
    "preview": "posts/2019-09-13-initializing-an-empty-list/emptyList.jpg",
    "last_modified": "2021-02-23T18:38:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-automatically-show-output/",
    "title": "(Automatically show output)",
    "description": "A neat trick with parentheses to print the contents of an object you just created, without running another line of code.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nIt’s annoying to have to type the name of an object I just created in order to print its output.\nContext\nA certain lightsaber-wielding stats professor of mine liked to point out that R doesn’t go out of its way to be helpful. If you write a line of code that creates an object and then run that line of code, there’s no message to tell you that the object has been successfully created. R doesn’t say “Task complete! What’s next?” or otherwise give you any indication that anything has happened. To actually view the object you just created, you have to type its name or run some other command on it.\nOnce in a while, this lack of transparency can be frustrating. What if I want to save objects and also view them in real time as they are created? Say I’ve used the handy prop.table function to transform a frequency table into a proportion table. I’d like to be able to view prop, prop.1 and prop.2 without typing their names and adding extra lines of code.\nSolution\nThe same lightsaber-wielding stats professor who wished R would be a little more communicative taught me a trick to do just this: encase a command in parentheses to automatically print its output when it runs. Hence,\n\n\n# Load data from GitHub\nlibrary(dplyr)\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n(prop <- with(polygon, table(revetment, pool)) \n %>% prop.table())\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…returns the same thing as leaving out the parentheses and typing the name of the object, prop, on a new line:\n\n\nprop <- with(polygon, table(revetment, pool)) %>%\n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nAlso note that this is different (better) than just running the command without the assignment arrow, like this:\n\n\nwith(polygon, table(revetment, pool)) %>% \n  prop.table()\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…because the above doesn’t save the table you created, it just shows it to you once.\nOutcome\nCreate objects and view them at the same time, while saving some typing. This is also great for use in RMarkdown, because it will print the output below the code chunk without your having to add another line of code.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-23T18:42:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-proptable/",
    "title": "prop.table()",
    "description": "Making a frequency table with proportions instead of counts. Preview image from https://twitter.com/lyric_rep/status/1010594530435846144.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from https://twitter.com/lyric_rep/status/1010594530435846144\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I convert a frequency table into proportions?\nContext\nThis is a continuation of the data manipulation discussed in the with() post. I had just finished making a table\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nWhat if I want to see this table broken down by proportion of polygons, not counts?\nSolution\nThe prop.table() function will do this nicely.\n\n\nlibrary(dplyr)\n\nprop <- with(polygon, table(revetment, pool)) %>% \n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nBy default, the proportions are calculated over the entire table. So each cell represents the proportion of all polygons that are in that pool with that value of revetment. The whole table sums to 1.\nIf you want proportions across rows or down columns, all you need to do is add the margin = argument.\nmargin = 1 sums across rows. Each row sums to 1. This would answer the question, “What proportion of the polygons [with, or without] revetment are located in each of the three pools?”\n\n\nprop.1 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 1)\n\nprop.1\n\n\n         pool\nrevetment         4         8        13\n        0 0.1903485 0.5335121 0.2761394\n        1 0.3701299 0.3441558 0.2857143\n\nmargin = 2 sums down columns. Each column sums to 1. This would answer the question, \"What proportion of the polygons in [pool] have revetment? (or, what proportion don’t have revetment?)\n\n\nprop.2 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 2)\n\nprop.2\n\n\n         pool\nrevetment         4         8        13\n        0 0.5546875 0.7896825 0.7006803\n        1 0.4453125 0.2103175 0.2993197\n\nOutcome\nHandy function for creating proportion tables.\n\n\n\n",
    "preview": "posts/2018-07-22-proptable/propTable.jpg",
    "last_modified": "2021-02-23T18:44:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-20-with/",
    "title": "with()",
    "description": "A brief introduction to the with() function",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-20",
    "categories": [],
    "contents": "\nProblem\nMaking graphics with base R is annoying for many reasons, but a big one is having to type the name of the data frame over and over again to reference different columns.\nContext\nBack to our Mississippi River fish data. I’ve aggregated my sampling points into polygons, and now I want to explore some of their characteristics. To do that, I’d like to make some tables and plots, and because these are just quick, exploratory plots, I don’t feel like dealing with ggplot.\nLoad in the data (accessible on GitHub).\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\") \n\n# Look at what we're dealing with\ndim(polygon) # How big is the data set? \n\n\n[1] 527  21\n\nhead(polygon, 3) # Look at the first few rows\n\n\n     poly_id propsnag n_points habitat_code pool      Area Perimeter\n1 P04_CFL_13   0.8000        5          CFL    4 105288.80  2067.890\n2 P04_CFL_14   0.2000        5          CFL    4  42668.28  1770.465\n3 P04_CFL_15   0.4375       16          CFL    4 678390.21  5226.963\n  max_depth avg_depth tot_vol shoreline_density_index pct_aqveg\n1      1.30 0.3625869   33955                1.797759 19.133960\n2      0.74 0.3291391    5953                2.417852 41.252704\n3      2.68 0.6159651  356757                1.790213  9.256465\n  pct_terr pct_prm_wetf med_dist_to_land med_dist_to_forest\n1 93.87983     79.67522         34.13379           34.13379\n2 94.76871     42.44244         18.90166           32.64112\n3 86.17161     41.27248         21.40210           39.27576\n  med_current wingdam revetment tributary pct_shallow_area\n1        0.02       0         1         0        0.9278354\n2        0.02       0         0         0        1.0000000\n3        0.01       1         1         1        0.9194788\n\nFirst, I’d like to see how total volume tot_vol of the aquatic area scales with its Area.\nIn base R:\n\n\n# Formula notation\nplot(polygon$tot_vol ~ polygon$Area)\n# OR: \n\n# Comma notation\nplot(polygon$Area, polygon$tot_vol)\n\n\n\nEither way, we get this:\n\n\n\nOr a more informative plot, with both variables on a log scale:\n\n\nplot(log(polygon$tot_vol) ~ log(polygon$Area))\n\n\n\n\nThis isn’t too too clunky, but if the data frame name or column names are long, it can get a little annoying.\nSolution\nThe with() function allows you to specify the data frame your variables are coming from and then reference the variables with respect to the data frame, similar to the ggplot argument data =. Handy.\n\n\n# Plot using the with() function \nwith(polygon, plot(tot_vol ~ Area))\n\n\n\n\nYou can add any other arguments inside of the function, as normal, it’s just now wrapped in with().\n\n\n# Log-transform variables and make the points blue dots, because why not?\nwith(polygon, plot(log(tot_vol) ~ log(Area), # log-transform \n                   pch = 20, # dots instead of circles \n                   col = \"blue\", # make the dots blue \n                   main = \"Polygon volume by area, log-transformed\") # title \n     )\n\n\n\n\nIt’s worth noting that this works for other functions besides plot(), too. Here’s an example with table(): let’s look at how many sampling polygons include revetment, broken down by navigation pool (area of the river). The data set contains three navigation pools: 4, 8, and 13.\n\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nOutcome\nQuick plots and data manipulation made even quicker!\nResources\nDiscussion of when to use with(): https://stackoverflow.com/questions/42283479/when-to-use-with-function-and-why-is-it-good\n\n\n\n",
    "preview": "posts/2018-07-20-with/noPreview.png",
    "last_modified": "2021-02-23T18:48:03-05:00",
    "input_file": {},
    "preview_width": 353,
    "preview_height": 50
  },
  {
    "path": "posts/2018-07-11-changing-individual-column-names/",
    "title": "Changing individual column names",
    "description": "How to rename individual columns in a data frame, based on the previous names and without using previous names.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-11",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I change the name of just one column in a data frame?\nContext\nThis is a simple one that keeps coming up. Sometimes, whoever put together my data decided to capitalize the first letter of some column names and not others. Sometimes I’ve merged several data frames together and I need to distinguish the columns from each other.\nSay my data frame is p8_0 and I’d like to change the column Area to area.\nIn the past, I’ve done this in one of two ways. Either I change all of the column names at once (if all of them need to be changed), or I use numerical column indexing. The latter makes a lot more sense if I have a lot of columns to deal with, but it means I have to know the number of the column whose name I have to change.\nTo find this out, I first have to look at all of the column names. Okay, no problem.\n\n\n\n\n\n# See column names and numerical indices\nnames(p8_0)\n\n\n  [1] \"FID\"                     \"Join_Count\"             \n  [3] \"TARGET_FID\"              \"Field1\"                 \n  [5] \"barcode\"                 \"stratum\"                \n  [7] \"lcode\"                   \"sdate\"                  \n  [9] \"utm_e\"                   \"utm_n\"                  \n [11] \"snag\"                    \"OBJECTID\"               \n [13] \"uniq_id\"                 \"aa_num\"                 \n [15] \"AQUA_CODE\"               \"AQUA_DESC\"              \n [17] \"pool\"                    \"Area\"                   \n [19] \"Perimeter\"               \"bath_pct\"               \n [21] \"max_depth\"               \"avg_depth\"              \n [23] \"sd_depth\"                \"tot_vol\"                \n [25] \"area_gt50\"               \"area_gt100\"             \n [27] \"area_gt200\"              \"area_gt300\"             \n [29] \"avg_fetch\"               \"shoreline_density_index\"\n [31] \"econ\"                    \"sill\"                   \n [33] \"min_rm\"                  \"max_rm\"                 \n [35] \"len_met\"                 \"len_prm_lotic\"          \n [37] \"pct_prm_lotic\"           \"num_lotic_outl\"         \n [39] \"len_prm_lentic\"          \"pct_prm_lentic\"         \n [41] \"num_lentic_outl\"         \"pct_aqveg\"              \n [43] \"pct_opwat\"               \"len_terr\"               \n [45] \"pct_terr\"                \"pct_aq\"                 \n [47] \"len_wetf\"                \"pct_prm_wetf\"           \n [49] \"pct_terr_shore_wetf\"     \"len_wd\"                 \n [51] \"wdl_p_m2\"                \"num_wd\"                 \n [53] \"scour_wd\"                \"psco_wd\"                \n [55] \"len_revln\"               \"rev_p_m2\"               \n [57] \"num_rev\"                 \"pct_terr_shore_rev\"     \n [59] \"pct_prm_rev\"             \"area_tpi1\"              \n [61] \"pct_tpi1\"                \"area_tpi2\"              \n [63] \"pct_tpi2\"                \"area_tpi3\"              \n [65] \"pct_tpi3\"                \"area_tpi4\"              \n [67] \"pct_tpi4\"                \"sinuosity\"              \n [69] \"year_phot\"               \"NEAR_TERR_FID\"          \n [71] \"NEAR_TERR_DIST\"          \"NEAR_TERR_CLASS_31\"     \n [73] \"NEAR_TERR_CLASS_15\"      \"NEAR_TERR_CLASS_7\"      \n [75] \"NEAR_TERR_CLASS_31_N\"    \"NEAR_TERR_CLASS_15_N\"   \n [77] \"NEAR_TERR_CLASS_7_N\"     \"NEAR_TERR_HEIGHT_N\"     \n [79] \"NEAR_FOREST_FID\"         \"NEAR_FOREST_DIST\"       \n [81] \"NEAR_FOREST_CLASS_31\"    \"NEAR_FOREST_CLASS_15\"   \n [83] \"NEAR_FOREST_CLASS_7\"     \"NEAR_FOREST_CLASS_31_N\" \n [85] \"NEAR_FOREST_CLASS_15_N\"  \"NEAR_FOREST_CLASS_7_N\"  \n [87] \"NEAR_FOREST_HEIGHT_N\"    \"year.p\"                 \n [89] \"depth.p\"                 \"current.p\"              \n [91] \"gear.p\"                  \"stageht.p\"              \n [93] \"substrt.p\"               \"wingdike.p\"             \n [95] \"riprap.p\"                \"trib.p\"                 \n [97] \"snagyn\"                  \"area_le50\"              \n [99] \"area_le100\"              \"area_le200\"             \n[101] \"area_le300\"              \"pct_area_le100\"         \n[103] \"pct_area_le50\"           \"pct_area_le200\"         \n[105] \"pct_area_le300\"          \"stratum_name\"           \n\nOkay, yes problem.\nIt’s not that hard to see that Area is the 18th column. But there are a bunch of columns that start with NEAR_TERR_ and NEAR_FOREST_ that would be easy to confuse. And what if I later modify my data cleaning script, insert new columns, and mess up the numerical indexing?\nSolution\nThe first solution I came up with is simple but pretty clunky. At least it solves the problem of numerical indices getting misaligned. And if you mistype the column name or try to change the name of a column that doesn’t exist, it doesn’t throw an error.\n\n\n# Change \"Area\" column name to \"area\"\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\n\n\n\nThis works well, but it gets annoying if you have more than one column name to change. Every column requires typing names(p8_0) twice, and that adds up to a lot of lines of code.\nTo no one’s surprise, dplyr has a more elegant solution, using the rename function.\n\n\n\n\n\n# Load dplyr \nlibrary(dplyr) \n\n# Rename variable (new name first) \np8_0 <- p8_0 %>% \n  rename(area = Area)\n\n\n\nA quick note on rename: somewhat counterintuitively, the new name comes before the old name. General example:\n\n\n# General syntax for rename \n#df %>% \n#  rename(newname = oldname)\n\n\n\nrename saves a whole bunch of keystrokes and also scales very well to multiple columns.\nLet’s say I wanted to change Area and Perimeter to area and perimeter, respectively, and I also wanted to change the rather clunky shoreline_density_index to sdi. And while we’re at it, snagyn, a factor variable that indicates whether a large piece of wood was present at the site (“yes” or “no”), might be clearer as snag_yn, and sinuosity could be shortened to sinu\nWithout dplyr:\n\n\n\n\n\n# Change each column name individually\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\nnames(p8_0)[names(p8_0) == \"Perimeter\"] <- \"perimeter\"\nnames(p8_0)[names(p8_0) == \"shoreline_density_index\"] <- \"sdi\"\nnames(p8_0)[names(p8_0) == \"snagyn\"] <- \"snag_yn\"\nnames(p8_0)[names(p8_0) == \"sinuosity\"] <- \"sinu\"\n\n\n\nWith dplyr:\n\n\n\n\n\n# Change any column names you want to, all at once\np8_0 <- p8_0 %>% rename(area = Area, \n                perimeter = Perimeter,\n                sdi = shoreline_density_index, \n                snag_yn = snagyn,\n                sinu = sinuosity)\n\n\n\nSo pretty. As an added bonus, you’re saved from both quotation marks and the dreaded double equals sign (!!!).\nIn case anyone was counting, that’s 102 characters vs. 238 (spaces not included). 116 if you include loading dplyr, but you already had it loaded because you’re using it throughout your code, of course.\nOutcome\nNow I can rename only the columns I want, by name instead of numerical index, without fear of having to change everything if I insert or delete some columns later on.\nResources\nMore thoughts on changing individual variable names, including a couple other packages if you feel like trying them: https://stackoverflow.com/questions/7531868/how-to-rename-a-single-column-in-a-data-frame\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T19:20:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-08-the-notin-operator/",
    "title": "The %notin% operator",
    "description": "Why is negating `%in%` such a pain?",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-08",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nI keep forgetting how to select all elements of an object except a few, by name. I get the ! operator confused with the - operator, and I find both of them less than intuitive to use. How can I negate the %in% operator?\nContext\nI have a data frame called electrofishing that contains observations from a fish sampling survey. One column, stratum, gives the aquatic habitat type of the sampling site. I’d like to exclude observations sampled in the “Tailwater Zone” or “Impounded-Offshore” aquatic habitats.\n\n\nelectrofishing <- data.frame(stratum = c(\"Tailwater Zone\", \"Tailwater Zone\", \"Impounded\", \"Main Channel Border\", \"Side Channel\", \"Impounded-Offshore\", \"Side Channel\"), \n                             idx = 1:7)\n\nelectrofishing\n\n\n              stratum idx\n1      Tailwater Zone   1\n2      Tailwater Zone   2\n3           Impounded   3\n4 Main Channel Border   4\n5        Side Channel   5\n6  Impounded-Offshore   6\n7        Side Channel   7\n\nMy instinct would be to do this:\n\nelectrofishing <- electrofishing[electrofishing$stratum !%in% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\nError: <text>:1:57: unexpected '!'\n1: electrofishing <- electrofishing[electrofishing$stratum !\n                                                            ^\n\nBut that doesn’t work. You can’t negate the %in% operator directly. Instead, you have to wrap the %in% statement in parentheses and negate the entire statement, returning the opposite of the original boolean vector:\n\n\nelectrofishing <- electrofishing[!(electrofishing$stratum %in% \n                                     c(\"Tailwater Zone\", \"Impounded-Offshore\")),]\n\n\n\nI’m not saying this doesn’t make sense, but I can never remember it. My English-speaking brain would much rather say “rows whose stratum is not included in c(”Tailwater Zone“,”Impounded-Offshore“)” than “not rows whose stratum is included in c(”Tailwater Zone“,”Impounded-Offshore“)”.\nSolution\nLuckily, it’s pretty easy to negate %in% and create a %notin% operator. I credit this answer to user “catastrophic-failure” on this Stack Overflow question.\n\n\n`%notin%` <- Negate(`%in%`)\n\n\n\nI didn’t even know that the Negate function existed. The more you know.\nOutcome\nI know there are lots of ways to negate selections in R. dplyr has select() and filter() functions that are easier to use with -c(). Or I could just learn to throw a ! in front of my %in% statements. But %notin% seems a little more intuitive.\nNow it’s straightforward to select these rows from my data frame.\n\n\nelectrofishing <- electrofishing[electrofishing$stratum %notin% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\n\n\n\nResources\nhttps://stackoverflow.com/questions/38351820/negation-of-in-in-r\nThis one does a good job of explaining why !%in% doesn’t work.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T18:41:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-06-where-are-my-nas/",
    "title": "Where are my NA's?",
    "description": "I wrote a function to summarize how many `NA`'s are in each column of a data frame. Preview image by Allison Horst, https://github.com/allisonhorst.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-06",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Artwork by Allison Horst, https://github.com/allisonhorst/stats-illustrations.\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I (quickly and intuitively) figure out how many NA’s are in my dataset and which columns they’re in?\nContext\nWhen I tried to run PCA (Principal Components Analysis) on some USGS fish sampling data, I noticed that I had a bunch of missing values. PCA needs complete observations, so this was a problem.\nOne option would have been to remove any observations with missing values from my data set:\n\n\n# Select only \"complete\" rows from the data frame `df`  \n# noNAs <- df[complete.cases(df),]\n\n\n\nThe problem was, I had over 30 variables and who knows how many missing values. The data frame had only ~2000 observations. By using only complete cases, I might lose a lot of observations and reduce my sample size by a huge amount.\nIn fact, I pretty often find myself in this situation. It would be really nice to have a quick way to see where those NA values are located so I can get a better sense of my dataset and figure out how to move forward.\nSolution\nWrite a loop that tells us how many NA’s are in each column.\nFirst, let’s create a sample data frame and call it sample.df:\n\n\n# Create the data frame\nsample.df <- data.frame(\n     site = 1:4, \n     temp = c(10, 15, 13, NA), \n     depth = c(1.1, NA, 2.0, NA)\n)\n\n# Show the data frame\nsample.df\n\n\n  site temp depth\n1    1   10   1.1\n2    2   15    NA\n3    3   13   2.0\n4    4   NA    NA\n\nLoop through the columns and print out the number of NA’s:\n\n\n# Create a vector full of NA's, the same length as the number of columns in sample.df\nna.vec <- rep(NA, ncol(sample.df))\n\n# Loop through the columns and fill na.vec\nfor(i in 1:ncol(sample.df)){\n     na.vec[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.vec\nna.vec\n\n\n[1] 0 1 2\n\nNow we can see that there are 0 NA’s in the first column, 1 NA in the second column, and 2 NA’s in the third column.\nBut if you have 30 columns, it’s a pain to map those numbers to the column names. So let’s do better. Instead of just printing the numbers of NA’s in a vector, we’ll put them in a data frame along with the names of the columns.\n\n\n# Create a data frame\nna.df <- data.frame(\n     Column = names(sample.df),\n     num.nas = NA\n)\n\n# Loop through the columns of sample.df and fill na.df\nfor(i in 1:ncol(sample.df)){\n     na.df$num.nas[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.df\nna.df\n\n\n  Column num.nas\n1   site       0\n2   temp       1\n3  depth       2\n\nSo much better!\nOnce you get used to it, this is a quick loop to write. But I got sick of re-creating this process every time, so I wrote a function called locate.nas. Feel free to use it:\n\n\n#Locate NA's: produces a data frame with column names and number of na's\nlocate.nas <- function(df){\n  na.df <- data.frame(\n    colname = names(df),\n    nas = NA\n  )\n  \n  for(i in 1:ncol(df)){\n    na.df$nas[i] <- sum(is.na(df[,i]))\n  }\n  return(na.df)\n}\n\n\n\nOutcome\nA quick look at the distribution of missing values (NA’s) in my data frame turned up an obvious pattern. I checked the sampling protocol and saw that certain variables had only been measured for lotic areas (moving water), while others had only been measured for lentic areas (still water). Since every observation point was in either a lotic or a lentic area, filtering out incomplete observations would have left me with no data at all.\nBy adding an indicator variable for lotic/lentic area, I could sort out my data and run PCA separately. Or I could remove the variables measured for only one area. Problem solved.\nResources\nlocate_nas function\nThere’s also a whole package that makes dealing with NA’s easier. I didn’t know about it when I originally wrote this post, but I’ve since discovered it, and you should check it out! It’s called naniar, and you can find it here\n\n\n\n",
    "preview": "posts/2018-07-06-where-are-my-nas/naniar.jpg",
    "last_modified": "2021-02-11T18:41:42-05:00",
    "input_file": {}
  }
]
