[
  {
    "path": "posts/2020-02-10-upping-your-pipe-game/",
    "title": "%$%: upping your pipe game",
    "description": "I love the `magrittr`/`dplyr` pipe: `%>%`. But it's meant to work with `tidyverse` functions, and it doesn't always work well with base R functions that take a single data frame column as input. Here, I use data about my friends' pets to explain how a different `magrittr` pipe, `%$%`, solves that problem.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nWhat do I do when %>% doesn’t work?\nContext\nI love the %>% pipe. Originally from magrittr, it’s now characteristic of most tidyverse code. Using %>% has revolutionized how I write code in R. But sometimes the basic pipe falls short.\ntable() is one of my favorite functions for exploring data in R: it creates a frequency table of values in a vector. I use table() to do sanity checks on my data, make sure that all factor levels are present, and generally get a sense of how my observations are distributed.\nA while back, though, I noticed that table() didn’t play nice with the %>% pipe.\nI’ve collected some data on my friends’ pets. Here it is (using pseudonyms, in case anyone has a secret pet they don’t want the world to know about…).\n\n\n\nFigure 1: This is one of the cats in the data frame below. She would like to hold your hand.\n\n\n\n\n\n# Load magrittr\nlibrary(magrittr)\nlibrary(dplyr)\n\n# Create data\npets <- data.frame(\n  friend = c(\"Mark\", \"Mark\", \"Kyle\", \"Kyle\", \"Miranda\", \"Kayla\", \n             \"Kayla\", \"Kayla\", \"Adriana\", \"Adriana\", \"Alex\", \"Randy\", \"Nancy\"), \n  pet = c(\"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"dog\", \"cat\", \"lizard\", \n          \"cat\", \"cat\", \"dog\", \"dog\", \"woodpecker\"), \n  main_pet_color = c(\"brown\", \"brown\", \"multi\", \"multi\", \"brown\", \n                     \"brown\", \"brown\", \"orange\", \"black\", \"white\", \n                     \"multi\", \"white\", \"multi\")) \n\n# Look at the data\npets\n\n\n    friend        pet main_pet_color\n1     Mark        cat          brown\n2     Mark        cat          brown\n3     Kyle        cat          multi\n4     Kyle        cat          multi\n5  Miranda        cat          brown\n6    Kayla        dog          brown\n7    Kayla        cat          brown\n8    Kayla     lizard         orange\n9  Adriana        cat          black\n10 Adriana        cat          white\n11    Alex        dog          multi\n12   Randy        dog          white\n13   Nancy woodpecker          multi\n\nUnsurprisingly, it looks like there are a lot of cats and dogs! There are also a lot of brown pets and a lot of multicolored ones. Let’s say I want to see a frequency table of the pet colors. I know that I can do this with table(), like so:\n\n\n# Make a frequency table of pet colors\ntable(pets$main_pet_color)\n\n\n\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nBut if I want to use tidy syntax, I might try to do it this way instead:\n\n\npets %>%\n  table(main_pet_color)\n\n\nError in table(., main_pet_color): object 'main_pet_color' not found\n\nWhat’s up with this? The syntax should work. pet is definitely a valid variable name in the data frame pets, and if I had used a different function, like arrange(), I would have had no problems:\n\n\n# Arrange the data frame by pet color\npets %>% arrange(main_pet_color) # works fine!\n\n\n    friend        pet main_pet_color\n1  Adriana        cat          black\n2     Mark        cat          brown\n3     Mark        cat          brown\n4  Miranda        cat          brown\n5    Kayla        dog          brown\n6    Kayla        cat          brown\n7     Kyle        cat          multi\n8     Kyle        cat          multi\n9     Alex        dog          multi\n10   Nancy woodpecker          multi\n11   Kayla     lizard         orange\n12 Adriana        cat          white\n13   Randy        dog          white\n\nSo why doesn’t this work with table()?? This problem has driven me crazy on several occasions. I always ended up reverting back to the table(pets$main_pet_color) syntax, but I was not happy about it.\nTurns out, there’s a simple fix.\nSolution\nIntroducing… a new pipe! %$% is called the “exposition pipe,” according to the magrittr package documentation, and it’s basically the tidy version of the with() function, which I wrote about previously.\nIf we simply swap out %>% for %$% in our failed code above, it works!\n\n\n# Make a frequency table of pet colors\npets %$% table(main_pet_color)\n\n\nmain_pet_color\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nImportant note: Make sure you have magrittr loaded if you want to use this pipe. dplyr includes the basic %>%, but not the other magrittr pipes.\nWhy it works\nThe traditional pipe, %>%, works by passing a data frame or tibble into the next function. But that only works if the function you’re piping to is set up to take a data frame/tibble as an argument!\nFunctions in the tidyverse, like arrange(), are set up to take this kind of argument, so that piping works seamlessly. But many base R functions take vectors as inputs instead.\nThat’s the case with table(). When we write table(pets$main_pet_color), the argument pets$main_pet_color is a vector:\n\n\n# This returns a vector\npets$main_pet_color\n\n\n [1] \"brown\"  \"brown\"  \"multi\"  \"multi\"  \"brown\"  \"brown\"  \"brown\" \n [8] \"orange\" \"black\"  \"white\"  \"multi\"  \"white\"  \"multi\" \n\nWhen we try to pass pets into table() with the pipe, table() expects a vector but gets a data frame instead, and it throws an error.\nThe %$% pipe “exposes” the column names of the data frame to the function you’re piping to, allowing that function to make sense of the data frame that is passed to it.\nOutcome\nThe exposition pipe is great for integrating non-tidyverse functions into a tidy workflow. The outcome for me is that I can finally make frequency tables to my heart’s content, without “code switching” back from tidy to base R syntax. Of course, the downside is that you do have to install magrittr, which is sometimes an extra dependency that I don’t want to deal with. But it’s nice to have the option!\n\n\n\nFigure 2: Congrats, you made it to the end! Here are some more cats for you.\n\n\n\nResources\nmagrittr has a couple other pipes, too: %T% and %<>%. The package also has some nice aliases for basic arithmetic functions that allow them to be incorporated into a chain of pipes. To read more about these magrittr options, scroll to the bottom of the magrittr vignette.\nNote: The image at the top of this post was modified from the magrittr documentation.\n\n\n\n",
    "preview": "posts/2020-02-10-upping-your-pipe-game/magrittr.jpg",
    "last_modified": "2021-02-08T20:39:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/",
    "title": "Some lessons from rstudio::conf 2020",
    "description": "Some of my thoughts about R and the R community after attending the 2020 rstudio::conf in San Francisco.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2020-02-03",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nWhen I started in R a few years ago, I never thought I would have any place at a coding conference for computer people. But thanks to some help from my lab and my university, last week I flew out to sunny San Francisco for the 2020 rstudio::conf, and I had a blast. Here are some things I learned and some avenues I’m excited to explore in the future.\n1. The R community is awesome.\n\n\n\nFigure 1: This art is by RStudio artist-in-residence Allison Horst. She does amazing illustrations! You can find her work on her GitHub page.\n\n\n\nI was pretty nervous to attend this conference. I’ve never really considered myself a computer person, and I learned R pretty much accidentally through my work in biology. In my experience, people who do a lot of computer programming tend to use a lot of intimidating jargon, and I was scared I’d flounder.\nI was surprised at how genuinely welcome I felt. At meals and talks, I sat down next to random people and asked what they did or how they knew R. I met people with so many diverse interests! Among others:\n• A linguist now working for a nonprofit • A neuroscience PhD now working for Facebook • Several young professors teaching data science at their schools • A woman who flew from Brazil (!!) to attend the conference • Someone just getting to know R • Two young people who interned at RStudio, despite not being experts in R • An RStudio bigwig (swoon!) • A professor working on genomic data • A researcher at the Smithsonian • An aquatic ecologist who uses R for work • So many people! You get the gist. •\nAnd they were all happy to talk to me!\n2. So much happens on Twitter.\nI joined Twitter on a whim this fall, and it has been awesome. I learned about this conference through Twitter. I’ve found some internships through Twitter. And by following the #rstats hashtag and some key people in the R community, I’ve learned all sorts of tips and tricks about the kinds of things you can do with Twitter.\nApart from that, lots of people were live-tweeting the rstudio::conf, and I tried my hand at that, too! A highlight was when the one and only Hadley Wickham liked one of my tweets.\n3. I need to start making Shiny apps.\nAs I said in my tweet (the one that Hadley liked!), this conference convinced me that I really should start building apps with R Shiny as soon as possible.\nWhy haven’t I done this already?\nThe main reason is that the word “app” strikes fear into my heart. Surely I can’t develop an app??\nShiny apps are web apps, which is a little less intimidating, somehow. The basic gist of Shiny apps is that they are ways to explore your data interactively. That’s it. Some Shiny apps can get pretty complicated. For example, here’s a cool dashboard for visualizing tweets about the 2019 rstudio::conf. As you can see, it’s pretty fancy.\n\n\n\nBut Shiny apps can also be pretty simple, like this one, which shows a simple histogram of the duration of eruptions of the Old Faithful geyser:\n\n\n\nThis app just lets the user change the number of histogram bins, include a density curve, and show the individual observations on the x axis. Pretty simple, but still so much better than a static visualization! I really have no excuse not to make something like this.\nIn particular, I can’t wait to build an app for the Yale Grammatical Diversity Project (YGDP). I’m currently working with the YGDP to help organize their database of linguistic survey data. I’ve already created some reports in RMarkdown to help visualize the data (you can see an example here). But wouldn’t this be so much better if it were interactive??\n\n\n\n4. Bonus: fun with hex stickers\nI finally got to experience the hype about hexagon stickers for R packages. They are so pretty and fun, and they fit together so nicely! I picked up a whole bunch:\n\n\n\nAnd, funnily enough, I’ve been playing with hexagons as wall decorations for a while now, before I even knew about hex stickers…\n\n\n\n…so obviously, the possibilities now are truly endless. Hexagons on hexagons? A hex collage on the wall and one on my computer? Wow.\n\n\n\n",
    "preview": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/rstudioconf2020.png",
    "last_modified": "2021-02-11T18:42:23-05:00",
    "input_file": {},
    "preview_width": 1796,
    "preview_height": 766
  },
  {
    "path": "posts/2019-10-12-loading-packages-efficiently/",
    "title": "Loading packages efficiently",
    "description": "A trick I sometimes use to load a whole bunch of packages for every script in a project, while saving myself some typing. (Preview image from https://towardsdatascience.com/a-comprehensive-list-of-handy-r-packages-e85dad294b3d)",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-12",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from towards data science\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nEspecially in a project with many different scripts, it can be challenging to keep track of all the packages you need to load. It’s also easy to lose track of whether or not you’ve incorporated package loading into the script itself until you switch to a new computer or restart R and all of a sudden, your packages need to be re-loaded.\nContext\nWhen I was first starting out in R, I learned quickly to load packages all together at the top of a script, not along the way as I needed them. But it took a while, until I started using R Projects, before I decided to centralize package loading above the script level. I was sick of having to deal with loading the right packages at the right times, so I decided to just streamline the whole thing.\nSolution\nMake a separate R script, called “libraries.R” or “packages.R” or something. Keep it consistent. Mine is always called “libraries,” and I keep it in my project folder.\n\n\n\nThe script looks something like this (individual packages may vary, of course):\n\n\n\nThen, at the top of each analysis script, I can simply source the libraries script, and all the libraries I need load automatically.\n\n\n\nOutcome\nI can easily load libraries in the context of a single R Project, keep track of which ones are loaded, and not have to worry about making my scripts look messy with a whole chunk of library() commands at the top of each one. It’s also straightforward to pop open the “libraries” script whenever I want to add a new package or delete one.\n\n\n\n",
    "preview": "posts/2019-10-12-loading-packages-efficiently/pkgs.png",
    "last_modified": "2021-02-11T18:43:28-05:00",
    "input_file": "loading-packages-efficiently.utf8.md",
    "preview_width": 1097,
    "preview_height": 233
  },
  {
    "path": "posts/2018-07-08-the-notin-operator/",
    "title": "The %notin% operator",
    "description": "Why is negating `%in%` such a pain?",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-08",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nI keep forgetting how to select all elements of an object except a few, by name. I get the ! operator confused with the - operator, and I find both of them less than intuitive to use. How can I negate the %in% operator?\nContext\nI have a data frame called electrofishing that contains observations from a fish sampling survey. One column, stratum, gives the aquatic habitat type of the sampling site. I’d like to exclude observations sampled in the “Tailwater Zone” or “Impounded-Offshore” aquatic habitats.\n\n\nelectrofishing <- data.frame(stratum = c(\"Tailwater Zone\", \"Tailwater Zone\", \"Impounded\", \"Main Channel Border\", \"Side Channel\", \"Impounded-Offshore\", \"Side Channel\"), \n                             idx = 1:7)\n\nelectrofishing\n\n\n              stratum idx\n1      Tailwater Zone   1\n2      Tailwater Zone   2\n3           Impounded   3\n4 Main Channel Border   4\n5        Side Channel   5\n6  Impounded-Offshore   6\n7        Side Channel   7\n\nMy instinct would be to do this:\n\nelectrofishing <- electrofishing[electrofishing$stratum !%in% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\nError: <text>:1:57: unexpected '!'\n1: electrofishing <- electrofishing[electrofishing$stratum !\n                                                            ^\n\nBut that doesn’t work. You can’t negate the %in% operator directly. Instead, you have to wrap the %in% statement in parentheses and negate the entire statement, returning the opposite of the original boolean vector:\n\n\nelectrofishing <- electrofishing[!(electrofishing$stratum %in% \n                                     c(\"Tailwater Zone\", \"Impounded-Offshore\")),]\n\n\n\nI’m not saying this doesn’t make sense, but I can never remember it. My English-speaking brain would much rather say “rows whose stratum is not included in c(”Tailwater Zone“,”Impounded-Offshore“)” than “not rows whose stratum is included in c(”Tailwater Zone“,”Impounded-Offshore“)”.\nSolution\nLuckily, it’s pretty easy to negate %in% and create a %notin% operator. I credit this answer to user “catastrophic-failure” on this Stack Overflow question.\n\n\n`%notin%` <- Negate(`%in%`)\n\n\n\nI didn’t even know that the Negate function existed. The more you know.\nOutcome\nI know there are lots of ways to negate selections in R. dplyr has select() and filter() functions that are easier to use with -c(). Or I could just learn to throw a ! in front of my %in% statements. But %notin% seems a little more intuitive.\nNow it’s straightforward to select these rows from my data frame.\n\n\nelectrofishing <- electrofishing[electrofishing$stratum %notin% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\n\n\n\nResources\nhttps://stackoverflow.com/questions/38351820/negation-of-in-in-r\nThis one does a good job of explaining why !%in% doesn’t work.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T18:41:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-06-where-are-my-nas/",
    "title": "Where are my NA's?",
    "description": "I wrote a function to summarize how many `NA`'s are in each column of a data frame. Preview image by Allison Horst, https://github.com/allisonhorst.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-06",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Artwork by Allison Horst, https://github.com/allisonhorst/stats-illustrations.\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I (quickly and intuitively) figure out how many NA’s are in my dataset and which columns they’re in?\nContext\nWhen I tried to run PCA (Principal Components Analysis) on some USGS fish sampling data, I noticed that I had a bunch of missing values. PCA needs complete observations, so this was a problem.\nOne option would have been to remove any observations with missing values from my data set:\n\n\n# Select only \"complete\" rows from the data frame `df`  \n# noNAs <- df[complete.cases(df),]\n\n\n\nThe problem was, I had over 30 variables and who knows how many missing values. The data frame had only ~2000 observations. By using only complete cases, I might lose a lot of observations and reduce my sample size by a huge amount.\nIn fact, I pretty often find myself in this situation. It would be really nice to have a quick way to see where those NA values are located so I can get a better sense of my dataset and figure out how to move forward.\nSolution\nWrite a loop that tells us how many NA’s are in each column.\nFirst, let’s create a sample data frame and call it sample.df:\n\n\n# Create the data frame\nsample.df <- data.frame(\n     site = 1:4, \n     temp = c(10, 15, 13, NA), \n     depth = c(1.1, NA, 2.0, NA)\n)\n\n# Show the data frame\nsample.df\n\n\n  site temp depth\n1    1   10   1.1\n2    2   15    NA\n3    3   13   2.0\n4    4   NA    NA\n\nLoop through the columns and print out the number of NA’s:\n\n\n# Create a vector full of NA's, the same length as the number of columns in sample.df\nna.vec <- rep(NA, ncol(sample.df))\n\n# Loop through the columns and fill na.vec\nfor(i in 1:ncol(sample.df)){\n     na.vec[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.vec\nna.vec\n\n\n[1] 0 1 2\n\nNow we can see that there are 0 NA’s in the first column, 1 NA in the second column, and 2 NA’s in the third column.\nBut if you have 30 columns, it’s a pain to map those numbers to the column names. So let’s do better. Instead of just printing the numbers of NA’s in a vector, we’ll put them in a data frame along with the names of the columns.\n\n\n# Create a data frame\nna.df <- data.frame(\n     Column = names(sample.df),\n     num.nas = NA\n)\n\n# Loop through the columns of sample.df and fill na.df\nfor(i in 1:ncol(sample.df)){\n     na.df$num.nas[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.df\nna.df\n\n\n  Column num.nas\n1   site       0\n2   temp       1\n3  depth       2\n\nSo much better!\nOnce you get used to it, this is a quick loop to write. But I got sick of re-creating this process every time, so I wrote a function called locate.nas. Feel free to use it:\n\n\n#Locate NA's: produces a data frame with column names and number of na's\nlocate.nas <- function(df){\n  na.df <- data.frame(\n    colname = names(df),\n    nas = NA\n  )\n  \n  for(i in 1:ncol(df)){\n    na.df$nas[i] <- sum(is.na(df[,i]))\n  }\n  return(na.df)\n}\n\n\n\nOutcome\nA quick look at the distribution of missing values (NA’s) in my data frame turned up an obvious pattern. I checked the sampling protocol and saw that certain variables had only been measured for lotic areas (moving water), while others had only been measured for lentic areas (still water). Since every observation point was in either a lotic or a lentic area, filtering out incomplete observations would have left me with no data at all.\nBy adding an indicator variable for lotic/lentic area, I could sort out my data and run PCA separately. Or I could remove the variables measured for only one area. Problem solved.\nResources\nlocate_nas function\nThere’s also a whole package that makes dealing with NA’s easier. I didn’t know about it when I originally wrote this post, but I’ve since discovered it, and you should check it out! It’s called naniar, and you can find it here\n\n\n\n",
    "preview": "posts/2018-07-06-where-are-my-nas/naniar.jpg",
    "last_modified": "2021-02-11T18:41:42-05:00",
    "input_file": {}
  }
]
