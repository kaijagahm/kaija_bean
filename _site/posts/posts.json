[
  {
    "path": "posts/2021-05-09-greent/",
    "title": "greenT: Exploring grapheme-color synesthesia",
    "description": "An introduction to a Shiny app I built to show friends the inner workings of my brain--and some musings on my experience learning Shiny this year.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-05-09",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nA guided tour\nPage: ‘Explore’\nPage: ‘About’\nPage: ‘Contribute’\n\nFuture enhancements\nGet involved\nThoughts on the process of building the app\nBehind the app\n\n\n\n\nFigure 1: The digits 0-9, according to my brain\n\n\n\nThis article has been adapted from my entry into the 2021 RStudio Shiny contest, posted on RStudio Community.\n\n\n\nFigure 2: My alphabet\n\n\n\nIntroduction\nMy name is Kaija, and I have what’s known as grapheme-color synesthesia. It’s a (completely benign!) phenomenon where I experience letters and numbers as having colors associated with them. I don’t literally see colors floating in the air, but I’ve consistently associated a color with each letter and number for most of my life.\nHaving synesthesia doesn’t affect my life much. It occasionally comes in handy for remembering credit card PINs and phone numbers, and sometimes I draw associations between completely unrelated words because they happen to have similar colors. But it’s mostly just a fun little party trick.\nStill, when I tell my friends about my synesthesia, they want to know what color their name is. And while I can certainly type out “magenta, red, brown, pale yellow” (Mary) or “magenta, red, blue, blue, whitish, gray” (Maddie), that can be hard to picture. And it’s frustrating to have to consciously translate my mental impressions of color into words.\n\n\n\nFigure 3: Some example names: Mary and Maddie\n\n\n\nI wanted a way to quickly represent any word or words in the colors I see it in. I did a bit of googling to see if anyone had come up with a synesthesia simulator. The closest thing I found was Bernadette Sheridan’s website synesthesia.me, which did exactly what I wanted to do, but only for Bernadette’s own colors! Cool, but not useful to me or to any other synesthetes (like my brother) who might want to show me their own colors.\nSo, I built this app. I’m excited to add more features in the future, but for now, I’ll explain the three main parts.\nThe source code for the app is here.\nA guided tour\nPage: ‘Explore’\nAt the top of the app’s main page, you’ll see 36 color selectors (developed using the colourpicker package), one for each letter and digit. When you load the app, the colors are chosen randomly, but you can change any or all of the colors by clicking on each selector yourself. If you find it easier to start from white selectors, click “Set all to white” and then choose colors one at a time.\n\n\n\nFigure 4: The ‘Explore’ page\n\n\n\nIf you have synesthesia, you can set your own colors. If you don’t, feel free to use the app to choose whatever colors look good to you. Or you can click the “Kaija’s colors” button to see what color your name is in my head, similar to Bernadette’s project. Once you’re happy with the colors you’ve set, feel free to hide the selectors using the “Show/hide selectors” link at the top.\nNext, you can enter text you’d like to translate into colors. The text will plot as rectangles, with the letters optionally superimposed in either white or black, whichever provides the best contrast to the color you’ve chosen. Use the toggle switch to remove the letters. Note that for simplicity, all text gets converted to lowercase, and all non-alphanumeric characters show up as white.\n\n\n\nFigure 5: Entering text to display\n\n\n\nIf you want to show your friends what colors their names are, download the plot you’ve created using the “Download rectangles as .png” button. If you’re interested in analyzing your colors further, download the color values themselves, as both hex codes and RGB values, using the “Download colors as .csv” button.\n\n\n\nFigure 6: Here’s an example of the downloaded color data\n\n\n\nPage: ‘About’\nThe About page gives a bit of backstory for the app, similar to what I’ve written here. It also links to a few more resources you might be interested in if you want to learn more about synesthesia. And most importantly, I use this page to thank Jonathan Trattner, Dean Attali, and Bernadette Sheridan for their contributions to this app, in code or inspiration.\nPage: ‘Contribute’\nI’m always on the hunt for projects, and I’ve thought for a while that I would love to analyze data on my own colors and those of other people. For example, past research has found that there are some patterns in which letters synesthetes associate with which colors. I’m also interested in intra-individual variation. For example, while my K is a very strong magenta color and always has been (it has never varied for as long as I can remember, and it doesn’t change based on the letters around it), some of my letter-color associations are much weaker. E, which I’ve encoded in this app as gray, is really almost colorless or translucent. It tends to take on the colors of the letters around it. So I’d love to repeatedly collect my own color data and analyze it.\n\n\n\nFigure 7: My own name in my colors\n\n\n\nIf you want to contribute your data to my (independent, informal, unofficial!) research into this phenomenon, I encourage you to fill out the form on the Contribute page. Make sure to set your colors using the selectors on the Explore tab before filling out the rest of the form–when you click “Submit”, the currently-selected colors will be recorded along with the demographic info you’ve entered.\n\n\n\nFigure 8: The contribution form\n\n\n\nAlso: feel free to submit colors more than once, if yours are a bit variable like mine are! Just use the same name and (optionally) email address, so I can relate your responses to each other.\nFuture enhancements\nHere’s what I’d like to add to this app in the future:\nBookmarking, so you can save the colors you’ve entered and come back to them\nAbility to upload your previously-downloaded data (more robust version of bookmarking)\nA continuously-updating analysis page that shows statistics based on previously-submitted data (anonymized!)\nOpacity toggles or other more nuanced formatting tweaks for the colors\nA “colored text” mode that displays the letters themselves in color\nGet involved\nIf you’re interested in contributing to the app, submit an issue or PR on GitHub, or get in touch! I’d love to collaborate with you.\nThoughts on the process of building the app\nBuilding this app was a joyful process because it made me acutely aware of how far I’ve come with Shiny in less than a year.\nI had never built a Shiny app until July 28th, 2020, when I watched Garrett Grolemund’s Shiny tutorial videos and started playing around with a simple app. By chance, the Palmer penguins dataset was up for Tidy Tuesday that week, so I made a simple data explorer app for that dataset. I was surprised at how easy it was.\n\n\nLooks like my first Shiny app using the #palmer was also accidentally my first #TidyTuesday contribution! The final-ish version is up: https://t.co/T1Q7IdBD6d https://t.co/dwu9ToXwRZ\n\n— Kaija Gahm (@kaija_bean) July 28, 2020\n\nI had started learning Shiny in order to build a much more complicated data explorer app for the Yale Grammatical Diversity Project. I really didn’t know what I was getting myself into–I had pitched the idea to my boss at the YGDP with the clear caveat that I had never programmed in Shiny before, ever. But I was confident that I could figure it out. I had yet to encounter a skill in R that I couldn’t crack if I put my mind and time to it.\n\n\nToday in learning #rstats #shiny: got my reactivity to work! pic.twitter.com/bAWdrFhggz\n\n— Kaija Gahm (@kaija_bean) August 3, 2020\n\n“Time” turned into around nine months of programming, debugging, and fiddling. It was a frustrating and at times lonely process, since I had no programmer colleagues to turn to for help and brainstorming. But I eventually produced an app I was proud of; you can read more about it here and see the finished app embedded at the bottom of that page or here.\nThe YGDP dashboard reinforced for me that I could do anything in Shiny that I set my mind to, and greenT was a chance to put that into practice. With many fewer moving parts, greenT was a bit of a softball to myself. That’s not to say that it was easy. Rather, when I did run up against a problem, I knew it could not possibly be as challenging as the problems I had already debugged for the YGDP app. I confidently laid out the app and linked up the components and had a working prototype in an afternoon. Then came many more afternoons of tidying and adding features, but that was the fun part.\n\n\n\nFigure 9: A particularly satisfying commit–I got the contribution form to write user data to Google drive\n\n\n\nI hope it doesn’t sound like I’m bragging. I had, at this point, almost a year of 10+ hours a week of Shiny under my belt. But I also want to point out that I didn’t really do this alone at all. I have been, as Steph Locke so eloquently put it, “community-taught”.\n\n\nInstead of referring to myself as self-taught, I'm gonna start referring to myself as community-taught. The sites, the blogs, the books, the user groups, the confs, the forums … all community efforts that I used to learn and advance my programming and data science knowledge.\n\n— Steph Locke (@TheStephLocke) January 7, 2020\n\nPart of that is being able to jump into Shiny by watching beautifully thought-out videos like the Garrett Grolemund tutorial I referred to above. Part of that is benefiting from the packages that others before me have developed. For example, Dean Attali does great work in Shiny, and his code literally made this app possible: the color selectors on the main page come from his colourpicker package, and the contribution form is taken almost verbatim from his tutorial on mimicking a Google form with Shiny.\nI had another advantage in developing greenT: Jonathan Trattner collaborated with me during most of the app’s development. Even though his time was limited by classes and exams, it was invaluable to be able to ping him on Slack or hop on a video call to discuss something I was stuck on.\nBehind the app\nPretty soon, I’ll be making another post that goes into more detail on how I accomplished certain things, code-wise, in the app. It was going to be part of this post, but I kept writing and writing and eventually I decided it would be weird to have a combination introducing-my-app and here’s-five-different-detailed-tutorials post. So, stay tuned!\n\n\n\n",
    "preview": "posts/2021-05-09-greent/numbers.png",
    "last_modified": "2021-05-12T16:49:59-04:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "posts/2020-02-10-upping-your-pipe-game/",
    "title": "%$%: upping your pipe game",
    "description": "I love the magrittr/dplyr pipe: %>%. But it's meant to work with tidyverse functions, and it doesn't always work well with base R functions that take a single data frame column as input. Here, I use data about my friends' pets to explain how a different magrittr pipe, %$%, solves that problem.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nWhat do I do when %>% doesn’t work?\nContext\nI love the %>% pipe. Originally from magrittr, it’s now characteristic of most tidyverse code. Using %>% has revolutionized how I write code in R. But sometimes the basic pipe falls short.\ntable() is one of my favorite functions for exploring data in R: it creates a frequency table of values in a vector. I use table() to do sanity checks on my data, make sure that all factor levels are present, and generally get a sense of how my observations are distributed.\nA while back, though, I noticed that table() didn’t play nice with the %>% pipe.\nI’ve collected some data on my friends’ pets. Here it is (using pseudonyms, in case anyone has a secret pet they don’t want the world to know about…).\n\n\n\nFigure 1: This is one of the cats in the data frame below. She would like to hold your hand.\n\n\n\n\n\n# Load magrittr\nlibrary(magrittr)\nlibrary(dplyr)\n\n# Create data\npets <- data.frame(\n  friend = c(\"Mark\", \"Mark\", \"Kyle\", \"Kyle\", \"Miranda\", \"Kayla\", \n             \"Kayla\", \"Kayla\", \"Adriana\", \"Adriana\", \"Alex\", \"Randy\", \"Nancy\"), \n  pet = c(\"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"dog\", \"cat\", \"lizard\", \n          \"cat\", \"cat\", \"dog\", \"dog\", \"woodpecker\"), \n  main_pet_color = c(\"brown\", \"brown\", \"multi\", \"multi\", \"brown\", \n                     \"brown\", \"brown\", \"orange\", \"black\", \"white\", \n                     \"multi\", \"white\", \"multi\")) \n\n# Look at the data\npets\n\n\n    friend        pet main_pet_color\n1     Mark        cat          brown\n2     Mark        cat          brown\n3     Kyle        cat          multi\n4     Kyle        cat          multi\n5  Miranda        cat          brown\n6    Kayla        dog          brown\n7    Kayla        cat          brown\n8    Kayla     lizard         orange\n9  Adriana        cat          black\n10 Adriana        cat          white\n11    Alex        dog          multi\n12   Randy        dog          white\n13   Nancy woodpecker          multi\n\nUnsurprisingly, it looks like there are a lot of cats and dogs! There are also a lot of brown pets and a lot of multicolored ones. Let’s say I want to see a frequency table of the pet colors. I know that I can do this with table(), like so:\n\n\n# Make a frequency table of pet colors\ntable(pets$main_pet_color)\n\n\n\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nBut if I want to use tidy syntax, I might try to do it this way instead:\n\n\npets %>%\n  table(main_pet_color)\n\n\nError in table(., main_pet_color): object 'main_pet_color' not found\n\nWhat’s up with this? The syntax should work. pet is definitely a valid variable name in the data frame pets, and if I had used a different function, like arrange(), I would have had no problems:\n\n\n# Arrange the data frame by pet color\npets %>% arrange(main_pet_color) # works fine!\n\n\n    friend        pet main_pet_color\n1  Adriana        cat          black\n2     Mark        cat          brown\n3     Mark        cat          brown\n4  Miranda        cat          brown\n5    Kayla        dog          brown\n6    Kayla        cat          brown\n7     Kyle        cat          multi\n8     Kyle        cat          multi\n9     Alex        dog          multi\n10   Nancy woodpecker          multi\n11   Kayla     lizard         orange\n12 Adriana        cat          white\n13   Randy        dog          white\n\nSo why doesn’t this work with table()?? This problem has driven me crazy on several occasions. I always ended up reverting back to the table(pets$main_pet_color) syntax, but I was not happy about it.\nTurns out, there’s a simple fix.\nSolution\nIntroducing… a new pipe! %$% is called the “exposition pipe,” according to the magrittr package documentation, and it’s basically the tidy version of the with() function, which I wrote about previously.\nIf we simply swap out %>% for %$% in our failed code above, it works!\n\n\n# Make a frequency table of pet colors\npets %$% table(main_pet_color)\n\n\nmain_pet_color\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nImportant note: Make sure you have magrittr loaded if you want to use this pipe. dplyr includes the basic %>%, but not the other magrittr pipes.\nWhy it works\nThe traditional pipe, %>%, works by passing a data frame or tibble into the next function. But that only works if the function you’re piping to is set up to take a data frame/tibble as an argument!\nFunctions in the tidyverse, like arrange(), are set up to take this kind of argument, so that piping works seamlessly. But many base R functions take vectors as inputs instead.\nThat’s the case with table(). When we write table(pets$main_pet_color), the argument pets$main_pet_color is a vector:\n\n\n# This returns a vector\npets$main_pet_color\n\n\n [1] \"brown\"  \"brown\"  \"multi\"  \"multi\"  \"brown\"  \"brown\"  \"brown\" \n [8] \"orange\" \"black\"  \"white\"  \"multi\"  \"white\"  \"multi\" \n\nWhen we try to pass pets into table() with the pipe, table() expects a vector but gets a data frame instead, and it throws an error.\nThe %$% pipe “exposes” the column names of the data frame to the function you’re piping to, allowing that function to make sense of the data frame that is passed to it.\nOutcome\nThe exposition pipe is great for integrating non-tidyverse functions into a tidy workflow. The outcome for me is that I can finally make frequency tables to my heart’s content, without “code switching” back from tidy to base R syntax. Of course, the downside is that you do have to install magrittr, which is sometimes an extra dependency that I don’t want to deal with. But it’s nice to have the option!\n\n\n\nFigure 2: Congrats, you made it to the end! Here are some more cats for you.\n\n\n\nResources\nmagrittr has a couple other pipes, too: %T% and %<>%. The package also has some nice aliases for basic arithmetic functions that allow them to be incorporated into a chain of pipes. To read more about these magrittr options, scroll to the bottom of the magrittr vignette.\nNote: The image at the top of this post was modified from the magrittr documentation.\n\n\n\n",
    "preview": "posts/2020-02-10-upping-your-pipe-game/magrittr.jpg",
    "last_modified": "2021-02-11T19:28:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/",
    "title": "Some lessons from rstudio::conf 2020",
    "description": "Some of my thoughts about R and the R community after attending the 2020 rstudio::conf in San Francisco.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2020-02-03",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nWhen I started in R a few years ago, I never thought I would have any place at a coding conference for computer people. But thanks to some help from my lab and my university, last week I flew out to sunny San Francisco for the 2020 rstudio::conf, and I had a blast. Here are some things I learned and some avenues I’m excited to explore in the future.\n1. The R community is awesome.\n\n\n\nFigure 1: This art is by RStudio artist-in-residence Allison Horst. She does amazing illustrations! You can find her work on her GitHub page.\n\n\n\nI was pretty nervous to attend this conference. I’ve never really considered myself a computer person, and I learned R pretty much accidentally through my work in biology. In my experience, people who do a lot of computer programming tend to use a lot of intimidating jargon, and I was scared I’d flounder.\nI was surprised at how genuinely welcome I felt. At meals and talks, I sat down next to random people and asked what they did or how they knew R. I met people with so many diverse interests! Among others:\n• A linguist now working for a nonprofit • A neuroscience PhD now working for Facebook • Several young professors teaching data science at their schools • A woman who flew from Brazil (!!) to attend the conference • Someone just getting to know R • Two young people who interned at RStudio, despite not being experts in R • An RStudio bigwig (swoon!) • A professor working on genomic data • A researcher at the Smithsonian • An aquatic ecologist who uses R for work • So many people! You get the gist. •\nAnd they were all happy to talk to me!\n2. So much happens on Twitter.\nI joined Twitter on a whim this fall, and it has been awesome. I learned about this conference through Twitter. I’ve found some internships through Twitter. And by following the #rstats hashtag and some key people in the R community, I’ve learned all sorts of tips and tricks about the kinds of things you can do with Twitter.\nApart from that, lots of people were live-tweeting the rstudio::conf, and I tried my hand at that, too! A highlight was when the one and only Hadley Wickham liked one of my tweets.\n3. I need to start making Shiny apps.\nAs I said in my tweet (the one that Hadley liked!), this conference convinced me that I really should start building apps with R Shiny as soon as possible.\nWhy haven’t I done this already?\nThe main reason is that the word “app” strikes fear into my heart. Surely I can’t develop an app??\nShiny apps are web apps, which is a little less intimidating, somehow. The basic gist of Shiny apps is that they are ways to explore your data interactively. That’s it. Some Shiny apps can get pretty complicated. For example, here’s a cool dashboard for visualizing tweets about the 2019 rstudio::conf. As you can see, it’s pretty fancy.\n\n\n\nBut Shiny apps can also be pretty simple, like this one, which shows a simple histogram of the duration of eruptions of the Old Faithful geyser:\n\n\n\nThis app just lets the user change the number of histogram bins, include a density curve, and show the individual observations on the x axis. Pretty simple, but still so much better than a static visualization! I really have no excuse not to make something like this.\nIn particular, I can’t wait to build an app for the Yale Grammatical Diversity Project (YGDP). I’m currently working with the YGDP to help organize their database of linguistic survey data. I’ve already created some reports in RMarkdown to help visualize the data (you can see an example here). But wouldn’t this be so much better if it were interactive??\n\n\n\n4. Bonus: fun with hex stickers\nI finally got to experience the hype about hexagon stickers for R packages. They are so pretty and fun, and they fit together so nicely! I picked up a whole bunch:\n\n\n\nAnd, funnily enough, I’ve been playing with hexagons as wall decorations for a while now, before I even knew about hex stickers…\n\n\n\n…so obviously, the possibilities now are truly endless. Hexagons on hexagons? A hex collage on the wall and one on my computer? Wow.\n\n\n\n",
    "preview": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/rstudioconf2020.png",
    "last_modified": "2021-02-11T18:42:23-05:00",
    "input_file": {},
    "preview_width": 1796,
    "preview_height": 766
  },
  {
    "path": "posts/2019-11-22-if-ifelse-had-more-ifs-and-an-else/",
    "title": "If ifelse() had more if's (case_when(), part 2)",
    "description": "In which I learn how to make case_when() behave like a true if/else, including the else part. What happens when you only want to assign particular outcomes to a few cases, without touching the rest? Or if you want to bulk-assign all the unspecified cases to one outcome? The syntax is weird, but it works.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-11-22",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Source: Interesting Engineering (https://interestingengineering.com/video/make-your-very-own-arduino-based-color-candy-sorting-machine)\n\n\n\nThis post has been modified from its original form on woodpeckR.\nProblem\nLast month, I was super excited to discover the case_when() function in dplyr. But when I showed my blog post to a friend, he pointed out a problem: there seemed to be no way to specify a “background” case, like the “else” in ifelse().\n\n\n\nFigure 2: Michael isn’t satisfied with my description of case_when().\n\n\n\nIn the previous post, I gave an example with three outcomes based on test results. The implication was that there would be roughly equal numbers of people in each group. But what if the vast majority of people failed both tests, and we really just wanted to filter out the ones who didn’t?\nContext\nLet’s say I’m analyzing morphometric data for the penguins in the Palmer Penguins dataset. I conduct a principal components analysis.\n\n\n# Load the penguins data\nlibrary(palmerpenguins)\nlibrary(dplyr) # for pipe and case_when and mutate etc.\nlibrary(tibble) # for rownames_to_column\nlibrary(ggplot2) # for plotting\n\n# Conduct Principal Components Analysis (PCA)\npca <- prcomp(~bill_length_mm + bill_depth_mm + \n                flipper_length_mm + body_mass_g, data = penguins)\n\n# Label pca score rows to prepare for join with penguins.\ncomponents <- pca$x %>% \n  as.data.frame() %>%\n  rownames_to_column(\"id\")\n\n# Join pca scores to penguins\npenguins <- penguins %>%\n  rownames_to_column(\"id\") %>% \n  left_join(components, by = \"id\")\n\n\n\n\n\n# Take a peek at the data:\npenguins %>%\n  as.data.frame() %>%\n  head() # now the PC scores are joined onto the penguins data, so I can use it in plotting.\n\n\n  id species    island bill_length_mm bill_depth_mm flipper_length_mm\n1  1  Adelie Torgersen           39.1          18.7               181\n2  2  Adelie Torgersen           39.5          17.4               186\n3  3  Adelie Torgersen           40.3          18.0               195\n4  4  Adelie Torgersen             NA            NA                NA\n5  5  Adelie Torgersen           36.7          19.3               193\n6  6  Adelie Torgersen           39.3          20.6               190\n  body_mass_g    sex year       PC1       PC2         PC3        PC4\n1        3750   male 2007 -452.0232 13.336636 -1.14798019 -0.3534919\n2        3800 female 2007 -401.9500  9.152694  0.09037342 -1.0483310\n3        3250 female 2007 -951.7409 -8.261476  2.35184450  0.8417657\n4          NA   <NA> 2007        NA        NA          NA         NA\n5        3450 female 2007 -751.8127 -1.975922  4.81117040  2.1800839\n6        3650   male 2007 -551.8746  3.343783  1.11849344  2.7060578\n\nAfter running the PCA, I make a plot of the 2nd and 3rd principal components that looks like this:\n\n\n\nBefore continuing my analysis, I wanted to take a closer look at a few points that look like they might be outliers. Specifically, I’m interested in the Adelie point that has a PC2 value greater than 20, and the Chinstrap point that has a PC3 value less than -15. I’m also slightly worried about the two points that have a PC2 value below -15, but they aren’t quite as far out, so I want to classify them separately.\nTo figure out which penguins to look at in the data, I will have to pull out rows based on their scores on the PC2 and PC3 axes.\nSolution\nI decide to add a column called investigate to my data, set to either ‘investigate’, ‘maybe’ or ‘no’ depending on whether the observation in question needs to be checked.\nThis is a great use for my new friend case_when()! I’ll approach it like this:\n\n\npenguins <- penguins %>%\n  mutate(investigate = case_when(PC2 > 20 | PC3 < -15 ~ \"investigate\",\n                                 PC2 < -15 ~ \"maybe\",\n                                 TRUE ~ \"no\"))\n\n\n\nWhat’s up with that weird TRUE ~ \"no\" line at the end of the case_when() statement? Basically, the TRUE is the equivalent of an else. It translates, roughly, to “assign anything that’s left to ‘no.’”\nWhy ‘TRUE’, not ‘else’?\nI don’t love the choice of TRUE here–I think the syntax is pretty confusing, and it’s something I had to memorize long before I understood the logic behind it.\nBasically, case_when() works by checking each of your ‘cases’ (conditions, if-statements) in order. For each row of the data frame, it checks the first case, and applies the resulting assignment if the row meets that case. If the row does not meet the first case, the function moves on to the second case, then the third, and on and on until it finds a case that evaluates to TRUE for that row.\nSo, when you want to write an “else” condition, you write TRUE as a catchall. TRUE will always evaluate to TRUE, so all rows that are left over after failing the first however many conditions will all “pass” that last condition and will be assigned to your desired “else” value.\nBecause of this, order matters! If I had started off with the TRUE ~ “ok” statement and then specified the other conditions, my code wouldn’t have worked: everything would just get assigned to “no”.\nThe dark side of the TRUE condition\nYou might be wondering what would happen if we omitted the TRUE condition. In part 1 of my case_when() explanation\n\n\n\n",
    "preview": "posts/2019-11-22-if-ifelse-had-more-ifs-and-an-else/colorSorter.jpg",
    "last_modified": "2021-03-12T18:36:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-12-loading-packages-efficiently/",
    "title": "Loading packages efficiently",
    "description": "A trick I sometimes use to load a whole bunch of packages for every script in a project, while saving myself some typing. (Preview image from https://towardsdatascience.com/a-comprehensive-list-of-handy-r-packages-e85dad294b3d)",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-12",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from towards data science\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nEspecially in a project with many different scripts, it can be challenging to keep track of all the packages you need to load. It’s also easy to lose track of whether or not you’ve incorporated package loading into the script itself until you switch to a new computer or restart R and all of a sudden, your packages need to be re-loaded.\nContext\nWhen I was first starting out in R, I learned quickly to load packages all together at the top of a script, not along the way as I needed them. But it took a while, until I started using R Projects, before I decided to centralize package loading above the script level. I was sick of having to deal with loading the right packages at the right times, so I decided to just streamline the whole thing.\nSolution\nMake a separate R script, called “libraries.R” or “packages.R” or something. Keep it consistent. Mine is always called “libraries,” and I keep it in my project folder.\n\n\n\nThe script looks something like this (individual packages may vary, of course):\n\n\n\nThen, at the top of each analysis script, I can simply source the libraries script, and all the libraries I need load automatically.\n\n\n\nOutcome\nI can easily load libraries in the context of a single R Project, keep track of which ones are loaded, and not have to worry about making my scripts look messy with a whole chunk of library() commands at the top of each one. It’s also straightforward to pop open the “libraries” script whenever I want to add a new package or delete one.\n\n\n\n",
    "preview": "posts/2019-10-12-loading-packages-efficiently/pkgs.png",
    "last_modified": "2021-02-11T18:43:29-05:00",
    "input_file": {},
    "preview_width": 1097,
    "preview_height": 233
  },
  {
    "path": "posts/2019-10-11-if-ifelse-had-more-ifs/",
    "title": "If ifelse() had more if's",
    "description": "In which I discover dplyr's case_when() function, a vectorized version of ifelse().",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-11",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Source: Interesting Engineering (https://interestingengineering.com/video/make-your-very-own-arduino-based-color-candy-sorting-machine)\n\n\n\nThis post has been modified from its original form on woodpeckR.\nProblem\nThe ifelse() function only allows for one “if” statement, and therefore two cases. You could add nested “if” statements, but that’s just a pain, especially if the 3+ conditions you want to use are all equivalent, conceptually. Is there a way to specify multiple conditions at the same time?\nContext\nI was recently given some survey data to clean up. It looked something like this (but obviously much larger):\n\n\ndat <- data.frame(name = c(\"Kaija\", \"Ella\", \"Andis\"),\n                  test1 = c(FALSE, TRUE, TRUE),\n                  test2 = c(FALSE, FALSE, TRUE))\ndat\n\n\n   name test1 test2\n1 Kaija FALSE FALSE\n2  Ella  TRUE FALSE\n3 Andis  TRUE  TRUE\n\nI needed to classify people in this data set based on whether they had passed or failed certain tests.\nI wanted to separate the people into three groups:\nPeople who passed both tests: Group A\nPeople who passed one test: Group B\nPeople who passed neither test: Group C\nI thought about using a nested ifelse() statement, and I certainly could have done that. But that approach didn’t make sense to me. The tests are equivalent and not given in any order; I simply want to sort the people into three equal groups. Any nesting of “if” statements would seem to imply a hierarchy that doesn’t really exist in the data. Not to mention that I don’t usually like nesting functions–I find it confusing and hard to read.\nSolution\nOnce again, dplyr to the rescue! I’m becoming more and more of a tidyverse fan with each passing day.\nTurns out, dplyr has a function for exactly this purpose: case_when(). It’s also known as “a general vectorised if,” but I like to think of it as “if ifelse() had more if’s.”\nHere’s the syntax:\n\n\nlibrary(dplyr) # load dplyr\n\ndat <- dat %>% # using the dplyr/magrittr pipe\n  mutate(grp = case_when(test1 & test2 ~ \"A\", # both tests: group A\n                         xor(test1, test2) ~ \"B\", # one test: group B\n                         !test1 & !test2 ~ \"C\" # neither test: group C\n  ))\n\n# Show the result:\ndat\n\n\n   name test1 test2 grp\n1 Kaija FALSE FALSE   C\n2  Ella  TRUE FALSE   B\n3 Andis  TRUE  TRUE   A\n\nLet me translate the above into English:\nAfter loading the package, I reassign dat, my data frame, to a modified version of the old dat. Then (%>%), I use the dplyr::mutate function to add a new column called grp. The contents of the column will be defined by the case_when() function.\ncase_when(), in this example, took three conditions. The condition is on the left side of the ~, and the resulting group assignment (in my case, A, B, or C) is on the right.\nI used logical operators for my conditions. The newest one to me was the xor() function, which is an exclusive or: only one of the conditions in the parentheses can be TRUE, not both.\nOutcome\nEasily make conditional assignments within a data frame. This function is a little less succinct than ifelse(), so I’m probably not going to use it for applications with only two cases, where ifelse() would work fine. But for three or more cases, it can’t be beat. Notice that I could have added any number of conditions to my case_when() statement, with no other caveats.\nI love this function, and I think we should all be using it.\nNOTE: case_when() has some tricky behavior when it comes to cases you don’t explicitly assign to an outcome. That is to say, the else part of this vectorized ifelse function is a little confusing. I discuss the else more in the next installment of this post.\n\n\n\n",
    "preview": "posts/2019-10-11-if-ifelse-had-more-ifs/colorSorter.jpg",
    "last_modified": "2021-03-08T16:53:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-13-initializing-an-empty-list/",
    "title": "Initializing an empty list",
    "description": "When you're using a for loop to fill up a list, sometimes it's a good idea to initialize the list ahead of time, with the right number of elements but with no data. But actually doing this is a little harder than I had anticipated.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-09-13",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I initialize an empty list for use in a for-loop or function?\nContext\nSometimes I’m writing a for-loop (I know, I know, I shouldn’t use for-loops in R, but sometimes it’s just easier. I’m a little less comfortable with apply functions than I’d like to be) and I know I’ll need to store the output in a list. Once in a while, the new list will be similar in form to an existing one, but more often, I just need to start from scratch, knowing only the number of elements I want to include.\nThis isn’t a totally alien thing to need to do––it’s pretty familiar if you’re used to initializing empty vectors before for-loops. There’s a whole other debate to be had about whether or not it’s acceptable to start with a truly empty vector and append to it on every iteration of the loop or whether you should always know the length beforehand, but I’ll just focus on the latter case for now.\nAnyway, initializing a vector of a given length is easy enough; I usually do it like this:\n\n\ndesired_length <- 10 # or whatever length you want\nempty_vec <- rep(NA, desired_length)\n\n\n\nI couldn’t immediately figure out how to replicate this for a list, though. The solution turns out to be relatively simple, but it’s just different enough that I can never seem to remember the syntax. This post is more for my records than anything, then.\nSolution\nInitializing an empty list turns out to have an added benefit over my rep(NA) method for vectors; namely, the list ends up actually empty, not filled with NA’s. Confusingly, the function to use is vector, not list.\n\n\ndesired_length <- 10 # or whatever length you want\nempty_list <- vector(mode = \"list\", \n                     length = desired_length)\n\nstr(empty_list)\n\n\nList of 10\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n\nOutcome\nVoilà, an empty list. No restrictions on the data type or structure of the individual list elements. Specify the length easily. Useful for loops, primarily, but may have other applications I haven’t come across yet.\nI learned something interesting recently, during the R working group I take part in through the Yale School of Forestry. See how, above, I initialized a list using the vector() function? Why is that?\nMaybe this should have been obvious, but a list is actually a type of vector. The thing that I would typically call a vector (a series of elements, all of the same type), is more specifically an ‘atomic vector’. A list, on the other hand, is just a vector that can contain more than one type of data.\n\n\n\n",
    "preview": "posts/2019-09-13-initializing-an-empty-list/emptyList.jpg",
    "last_modified": "2021-02-23T18:38:22-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-automatically-show-output/",
    "title": "(Automatically show output)",
    "description": "A neat trick with parentheses to print the contents of an object you just created, without running another line of code.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nIt’s annoying to have to type the name of an object I just created in order to print its output.\nContext\nA certain lightsaber-wielding stats professor of mine liked to point out that R doesn’t go out of its way to be helpful. If you write a line of code that creates an object and then run that line of code, there’s no message to tell you that the object has been successfully created. R doesn’t say “Task complete! What’s next?” or otherwise give you any indication that anything has happened. To actually view the object you just created, you have to type its name or run some other command on it.\nOnce in a while, this lack of transparency can be frustrating. What if I want to save objects and also view them in real time as they are created? Say I’ve used the handy prop.table function to transform a frequency table into a proportion table. I’d like to be able to view prop, prop.1 and prop.2 without typing their names and adding extra lines of code.\nSolution\nThe same lightsaber-wielding stats professor who wished R would be a little more communicative taught me a trick to do just this: encase a command in parentheses to automatically print its output when it runs. Hence,\n\n\n# Load data from GitHub\nlibrary(dplyr)\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n(prop <- with(polygon, table(revetment, pool)) \n %>% prop.table())\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…returns the same thing as leaving out the parentheses and typing the name of the object, prop, on a new line:\n\n\nprop <- with(polygon, table(revetment, pool)) %>%\n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nAlso note that this is different (better) than just running the command without the assignment arrow, like this:\n\n\nwith(polygon, table(revetment, pool)) %>% \n  prop.table()\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…because the above doesn’t save the table you created, it just shows it to you once.\nOutcome\nCreate objects and view them at the same time, while saving some typing. This is also great for use in RMarkdown, because it will print the output below the code chunk without your having to add another line of code.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-23T18:42:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-proptable/",
    "title": "prop.table()",
    "description": "Making a frequency table with proportions instead of counts. Preview image from https://twitter.com/lyric_rep/status/1010594530435846144.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from https://twitter.com/lyric_rep/status/1010594530435846144\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I convert a frequency table into proportions?\nContext\nThis is a continuation of the data manipulation discussed in the with() post. I had just finished making a table\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nWhat if I want to see this table broken down by proportion of polygons, not counts?\nSolution\nThe prop.table() function will do this nicely.\n\n\nlibrary(dplyr)\n\nprop <- with(polygon, table(revetment, pool)) %>% \n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nBy default, the proportions are calculated over the entire table. So each cell represents the proportion of all polygons that are in that pool with that value of revetment. The whole table sums to 1.\nIf you want proportions across rows or down columns, all you need to do is add the margin = argument.\nmargin = 1 sums across rows. Each row sums to 1. This would answer the question, “What proportion of the polygons [with, or without] revetment are located in each of the three pools?”\n\n\nprop.1 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 1)\n\nprop.1\n\n\n         pool\nrevetment         4         8        13\n        0 0.1903485 0.5335121 0.2761394\n        1 0.3701299 0.3441558 0.2857143\n\nmargin = 2 sums down columns. Each column sums to 1. This would answer the question, \"What proportion of the polygons in [pool] have revetment? (or, what proportion don’t have revetment?)\n\n\nprop.2 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 2)\n\nprop.2\n\n\n         pool\nrevetment         4         8        13\n        0 0.5546875 0.7896825 0.7006803\n        1 0.4453125 0.2103175 0.2993197\n\nOutcome\nHandy function for creating proportion tables.\n\n\n\n",
    "preview": "posts/2018-07-22-proptable/propTable.jpg",
    "last_modified": "2021-02-23T18:44:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-20-with/",
    "title": "with()",
    "description": "A brief introduction to the with() function",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-20",
    "categories": [],
    "contents": "\nProblem\nMaking graphics with base R is annoying for many reasons, but a big one is having to type the name of the data frame over and over again to reference different columns.\nContext\nBack to our Mississippi River fish data. I’ve aggregated my sampling points into polygons, and now I want to explore some of their characteristics. To do that, I’d like to make some tables and plots, and because these are just quick, exploratory plots, I don’t feel like dealing with ggplot.\nLoad in the data (accessible on GitHub).\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\") \n\n# Look at what we're dealing with\ndim(polygon) # How big is the data set? \n\n\n[1] 527  21\n\nhead(polygon, 3) # Look at the first few rows\n\n\n     poly_id propsnag n_points habitat_code pool      Area Perimeter\n1 P04_CFL_13   0.8000        5          CFL    4 105288.80  2067.890\n2 P04_CFL_14   0.2000        5          CFL    4  42668.28  1770.465\n3 P04_CFL_15   0.4375       16          CFL    4 678390.21  5226.963\n  max_depth avg_depth tot_vol shoreline_density_index pct_aqveg\n1      1.30 0.3625869   33955                1.797759 19.133960\n2      0.74 0.3291391    5953                2.417852 41.252704\n3      2.68 0.6159651  356757                1.790213  9.256465\n  pct_terr pct_prm_wetf med_dist_to_land med_dist_to_forest\n1 93.87983     79.67522         34.13379           34.13379\n2 94.76871     42.44244         18.90166           32.64112\n3 86.17161     41.27248         21.40210           39.27576\n  med_current wingdam revetment tributary pct_shallow_area\n1        0.02       0         1         0        0.9278354\n2        0.02       0         0         0        1.0000000\n3        0.01       1         1         1        0.9194788\n\nFirst, I’d like to see how total volume tot_vol of the aquatic area scales with its Area.\nIn base R:\n\n\n# Formula notation\nplot(polygon$tot_vol ~ polygon$Area)\n# OR: \n\n# Comma notation\nplot(polygon$Area, polygon$tot_vol)\n\n\n\nEither way, we get this:\n\n\n\nOr a more informative plot, with both variables on a log scale:\n\n\nplot(log(polygon$tot_vol) ~ log(polygon$Area))\n\n\n\n\nThis isn’t too too clunky, but if the data frame name or column names are long, it can get a little annoying.\nSolution\nThe with() function allows you to specify the data frame your variables are coming from and then reference the variables with respect to the data frame, similar to the ggplot argument data =. Handy.\n\n\n# Plot using the with() function \nwith(polygon, plot(tot_vol ~ Area))\n\n\n\n\nYou can add any other arguments inside of the function, as normal, it’s just now wrapped in with().\n\n\n# Log-transform variables and make the points blue dots, because why not?\nwith(polygon, plot(log(tot_vol) ~ log(Area), # log-transform \n                   pch = 20, # dots instead of circles \n                   col = \"blue\", # make the dots blue \n                   main = \"Polygon volume by area, log-transformed\") # title \n     )\n\n\n\n\nIt’s worth noting that this works for other functions besides plot(), too. Here’s an example with table(): let’s look at how many sampling polygons include revetment, broken down by navigation pool (area of the river). The data set contains three navigation pools: 4, 8, and 13.\n\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nOutcome\nQuick plots and data manipulation made even quicker!\nResources\nDiscussion of when to use with(): https://stackoverflow.com/questions/42283479/when-to-use-with-function-and-why-is-it-good\n\n\n\n",
    "preview": "posts/2018-07-20-with/noPreview.png",
    "last_modified": "2021-02-23T18:48:03-05:00",
    "input_file": {},
    "preview_width": 353,
    "preview_height": 50
  },
  {
    "path": "posts/2018-07-11-changing-individual-column-names/",
    "title": "Changing individual column names",
    "description": "How to rename individual columns in a data frame, based on the previous names and without using previous names.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-11",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I change the name of just one column in a data frame?\nContext\nThis is a simple one that keeps coming up. Sometimes, whoever put together my data decided to capitalize the first letter of some column names and not others. Sometimes I’ve merged several data frames together and I need to distinguish the columns from each other.\nSay my data frame is p8_0 and I’d like to change the column Area to area.\nIn the past, I’ve done this in one of two ways. Either I change all of the column names at once (if all of them need to be changed), or I use numerical column indexing. The latter makes a lot more sense if I have a lot of columns to deal with, but it means I have to know the number of the column whose name I have to change.\nTo find this out, I first have to look at all of the column names. Okay, no problem.\n\n\n\n\n\n# See column names and numerical indices\nnames(p8_0)\n\n\n  [1] \"FID\"                     \"Join_Count\"             \n  [3] \"TARGET_FID\"              \"Field1\"                 \n  [5] \"barcode\"                 \"stratum\"                \n  [7] \"lcode\"                   \"sdate\"                  \n  [9] \"utm_e\"                   \"utm_n\"                  \n [11] \"snag\"                    \"OBJECTID\"               \n [13] \"uniq_id\"                 \"aa_num\"                 \n [15] \"AQUA_CODE\"               \"AQUA_DESC\"              \n [17] \"pool\"                    \"Area\"                   \n [19] \"Perimeter\"               \"bath_pct\"               \n [21] \"max_depth\"               \"avg_depth\"              \n [23] \"sd_depth\"                \"tot_vol\"                \n [25] \"area_gt50\"               \"area_gt100\"             \n [27] \"area_gt200\"              \"area_gt300\"             \n [29] \"avg_fetch\"               \"shoreline_density_index\"\n [31] \"econ\"                    \"sill\"                   \n [33] \"min_rm\"                  \"max_rm\"                 \n [35] \"len_met\"                 \"len_prm_lotic\"          \n [37] \"pct_prm_lotic\"           \"num_lotic_outl\"         \n [39] \"len_prm_lentic\"          \"pct_prm_lentic\"         \n [41] \"num_lentic_outl\"         \"pct_aqveg\"              \n [43] \"pct_opwat\"               \"len_terr\"               \n [45] \"pct_terr\"                \"pct_aq\"                 \n [47] \"len_wetf\"                \"pct_prm_wetf\"           \n [49] \"pct_terr_shore_wetf\"     \"len_wd\"                 \n [51] \"wdl_p_m2\"                \"num_wd\"                 \n [53] \"scour_wd\"                \"psco_wd\"                \n [55] \"len_revln\"               \"rev_p_m2\"               \n [57] \"num_rev\"                 \"pct_terr_shore_rev\"     \n [59] \"pct_prm_rev\"             \"area_tpi1\"              \n [61] \"pct_tpi1\"                \"area_tpi2\"              \n [63] \"pct_tpi2\"                \"area_tpi3\"              \n [65] \"pct_tpi3\"                \"area_tpi4\"              \n [67] \"pct_tpi4\"                \"sinuosity\"              \n [69] \"year_phot\"               \"NEAR_TERR_FID\"          \n [71] \"NEAR_TERR_DIST\"          \"NEAR_TERR_CLASS_31\"     \n [73] \"NEAR_TERR_CLASS_15\"      \"NEAR_TERR_CLASS_7\"      \n [75] \"NEAR_TERR_CLASS_31_N\"    \"NEAR_TERR_CLASS_15_N\"   \n [77] \"NEAR_TERR_CLASS_7_N\"     \"NEAR_TERR_HEIGHT_N\"     \n [79] \"NEAR_FOREST_FID\"         \"NEAR_FOREST_DIST\"       \n [81] \"NEAR_FOREST_CLASS_31\"    \"NEAR_FOREST_CLASS_15\"   \n [83] \"NEAR_FOREST_CLASS_7\"     \"NEAR_FOREST_CLASS_31_N\" \n [85] \"NEAR_FOREST_CLASS_15_N\"  \"NEAR_FOREST_CLASS_7_N\"  \n [87] \"NEAR_FOREST_HEIGHT_N\"    \"year.p\"                 \n [89] \"depth.p\"                 \"current.p\"              \n [91] \"gear.p\"                  \"stageht.p\"              \n [93] \"substrt.p\"               \"wingdike.p\"             \n [95] \"riprap.p\"                \"trib.p\"                 \n [97] \"snagyn\"                  \"area_le50\"              \n [99] \"area_le100\"              \"area_le200\"             \n[101] \"area_le300\"              \"pct_area_le100\"         \n[103] \"pct_area_le50\"           \"pct_area_le200\"         \n[105] \"pct_area_le300\"          \"stratum_name\"           \n\nOkay, yes problem.\nIt’s not that hard to see that Area is the 18th column. But there are a bunch of columns that start with NEAR_TERR_ and NEAR_FOREST_ that would be easy to confuse. And what if I later modify my data cleaning script, insert new columns, and mess up the numerical indexing?\nSolution\nThe first solution I came up with is simple but pretty clunky. At least it solves the problem of numerical indices getting misaligned. And if you mistype the column name or try to change the name of a column that doesn’t exist, it doesn’t throw an error.\n\n\n# Change \"Area\" column name to \"area\"\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\n\n\n\nThis works well, but it gets annoying if you have more than one column name to change. Every column requires typing names(p8_0) twice, and that adds up to a lot of lines of code.\nTo no one’s surprise, dplyr has a more elegant solution, using the rename function.\n\n\n\n\n\n# Load dplyr \nlibrary(dplyr) \n\n# Rename variable (new name first) \np8_0 <- p8_0 %>% \n  rename(area = Area)\n\n\n\nA quick note on rename: somewhat counterintuitively, the new name comes before the old name. General example:\n\n\n# General syntax for rename \n#df %>% \n#  rename(newname = oldname)\n\n\n\nrename saves a whole bunch of keystrokes and also scales very well to multiple columns.\nLet’s say I wanted to change Area and Perimeter to area and perimeter, respectively, and I also wanted to change the rather clunky shoreline_density_index to sdi. And while we’re at it, snagyn, a factor variable that indicates whether a large piece of wood was present at the site (“yes” or “no”), might be clearer as snag_yn, and sinuosity could be shortened to sinu\nWithout dplyr:\n\n\n\n\n\n# Change each column name individually\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\nnames(p8_0)[names(p8_0) == \"Perimeter\"] <- \"perimeter\"\nnames(p8_0)[names(p8_0) == \"shoreline_density_index\"] <- \"sdi\"\nnames(p8_0)[names(p8_0) == \"snagyn\"] <- \"snag_yn\"\nnames(p8_0)[names(p8_0) == \"sinuosity\"] <- \"sinu\"\n\n\n\nWith dplyr:\n\n\n\n\n\n# Change any column names you want to, all at once\np8_0 <- p8_0 %>% rename(area = Area, \n                perimeter = Perimeter,\n                sdi = shoreline_density_index, \n                snag_yn = snagyn,\n                sinu = sinuosity)\n\n\n\nSo pretty. As an added bonus, you’re saved from both quotation marks and the dreaded double equals sign (!!!).\nIn case anyone was counting, that’s 102 characters vs. 238 (spaces not included). 116 if you include loading dplyr, but you already had it loaded because you’re using it throughout your code, of course.\nOutcome\nNow I can rename only the columns I want, by name instead of numerical index, without fear of having to change everything if I insert or delete some columns later on.\nResources\nMore thoughts on changing individual variable names, including a couple other packages if you feel like trying them: https://stackoverflow.com/questions/7531868/how-to-rename-a-single-column-in-a-data-frame\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T19:20:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-08-the-notin-operator/",
    "title": "The %notin% operator",
    "description": "Why is negating `%in%` such a pain?",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-08",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nI keep forgetting how to select all elements of an object except a few, by name. I get the ! operator confused with the - operator, and I find both of them less than intuitive to use. How can I negate the %in% operator?\nContext\nI have a data frame called electrofishing that contains observations from a fish sampling survey. One column, stratum, gives the aquatic habitat type of the sampling site. I’d like to exclude observations sampled in the “Tailwater Zone” or “Impounded-Offshore” aquatic habitats.\n\n\nelectrofishing <- data.frame(stratum = c(\"Tailwater Zone\", \"Tailwater Zone\", \"Impounded\", \"Main Channel Border\", \"Side Channel\", \"Impounded-Offshore\", \"Side Channel\"), \n                             idx = 1:7)\n\nelectrofishing\n\n\n              stratum idx\n1      Tailwater Zone   1\n2      Tailwater Zone   2\n3           Impounded   3\n4 Main Channel Border   4\n5        Side Channel   5\n6  Impounded-Offshore   6\n7        Side Channel   7\n\nMy instinct would be to do this:\n\nelectrofishing <- electrofishing[electrofishing$stratum !%in% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\nError: <text>:1:57: unexpected '!'\n1: electrofishing <- electrofishing[electrofishing$stratum !\n                                                            ^\n\nBut that doesn’t work. You can’t negate the %in% operator directly. Instead, you have to wrap the %in% statement in parentheses and negate the entire statement, returning the opposite of the original boolean vector:\n\n\nelectrofishing <- electrofishing[!(electrofishing$stratum %in% \n                                     c(\"Tailwater Zone\", \"Impounded-Offshore\")),]\n\n\n\nI’m not saying this doesn’t make sense, but I can never remember it. My English-speaking brain would much rather say “rows whose stratum is not included in c(”Tailwater Zone“,”Impounded-Offshore“)” than “not rows whose stratum is included in c(”Tailwater Zone“,”Impounded-Offshore“)”.\nSolution\nLuckily, it’s pretty easy to negate %in% and create a %notin% operator. I credit this answer to user “catastrophic-failure” on this Stack Overflow question.\n\n\n`%notin%` <- Negate(`%in%`)\n\n\n\nI didn’t even know that the Negate function existed. The more you know.\nOutcome\nI know there are lots of ways to negate selections in R. dplyr has select() and filter() functions that are easier to use with -c(). Or I could just learn to throw a ! in front of my %in% statements. But %notin% seems a little more intuitive.\nNow it’s straightforward to select these rows from my data frame.\n\n\nelectrofishing <- electrofishing[electrofishing$stratum %notin% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\n\n\n\nResources\nhttps://stackoverflow.com/questions/38351820/negation-of-in-in-r\nThis one does a good job of explaining why !%in% doesn’t work.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T18:41:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-06-where-are-my-nas/",
    "title": "Where are my NA's?",
    "description": "I wrote a function to summarize how many `NA`'s are in each column of a data frame. Preview image by Allison Horst, https://github.com/allisonhorst.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-06",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Artwork by Allison Horst, https://github.com/allisonhorst/stats-illustrations.\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I (quickly and intuitively) figure out how many NA’s are in my dataset and which columns they’re in?\nContext\nWhen I tried to run PCA (Principal Components Analysis) on some USGS fish sampling data, I noticed that I had a bunch of missing values. PCA needs complete observations, so this was a problem.\nOne option would have been to remove any observations with missing values from my data set:\n\n\n# Select only \"complete\" rows from the data frame `df`  \n# noNAs <- df[complete.cases(df),]\n\n\n\nThe problem was, I had over 30 variables and who knows how many missing values. The data frame had only ~2000 observations. By using only complete cases, I might lose a lot of observations and reduce my sample size by a huge amount.\nIn fact, I pretty often find myself in this situation. It would be really nice to have a quick way to see where those NA values are located so I can get a better sense of my dataset and figure out how to move forward.\nSolution\nWrite a loop that tells us how many NA’s are in each column.\nFirst, let’s create a sample data frame and call it sample.df:\n\n\n# Create the data frame\nsample.df <- data.frame(\n     site = 1:4, \n     temp = c(10, 15, 13, NA), \n     depth = c(1.1, NA, 2.0, NA)\n)\n\n# Show the data frame\nsample.df\n\n\n  site temp depth\n1    1   10   1.1\n2    2   15    NA\n3    3   13   2.0\n4    4   NA    NA\n\nLoop through the columns and print out the number of NA’s:\n\n\n# Create a vector full of NA's, the same length as the number of columns in sample.df\nna.vec <- rep(NA, ncol(sample.df))\n\n# Loop through the columns and fill na.vec\nfor(i in 1:ncol(sample.df)){\n     na.vec[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.vec\nna.vec\n\n\n[1] 0 1 2\n\nNow we can see that there are 0 NA’s in the first column, 1 NA in the second column, and 2 NA’s in the third column.\nBut if you have 30 columns, it’s a pain to map those numbers to the column names. So let’s do better. Instead of just printing the numbers of NA’s in a vector, we’ll put them in a data frame along with the names of the columns.\n\n\n# Create a data frame\nna.df <- data.frame(\n     Column = names(sample.df),\n     num.nas = NA\n)\n\n# Loop through the columns of sample.df and fill na.df\nfor(i in 1:ncol(sample.df)){\n     na.df$num.nas[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.df\nna.df\n\n\n  Column num.nas\n1   site       0\n2   temp       1\n3  depth       2\n\nSo much better!\nOnce you get used to it, this is a quick loop to write. But I got sick of re-creating this process every time, so I wrote a function called locate.nas. Feel free to use it:\n\n\n#Locate NA's: produces a data frame with column names and number of na's\nlocate.nas <- function(df){\n  na.df <- data.frame(\n    colname = names(df),\n    nas = NA\n  )\n  \n  for(i in 1:ncol(df)){\n    na.df$nas[i] <- sum(is.na(df[,i]))\n  }\n  return(na.df)\n}\n\n\n\nOutcome\nA quick look at the distribution of missing values (NA’s) in my data frame turned up an obvious pattern. I checked the sampling protocol and saw that certain variables had only been measured for lotic areas (moving water), while others had only been measured for lentic areas (still water). Since every observation point was in either a lotic or a lentic area, filtering out incomplete observations would have left me with no data at all.\nBy adding an indicator variable for lotic/lentic area, I could sort out my data and run PCA separately. Or I could remove the variables measured for only one area. Problem solved.\nResources\nlocate_nas function\nThere’s also a whole package that makes dealing with NA’s easier. I didn’t know about it when I originally wrote this post, but I’ve since discovered it, and you should check it out! It’s called naniar, and you can find it here\n\n\n\n",
    "preview": "posts/2018-07-06-where-are-my-nas/naniar.jpg",
    "last_modified": "2021-02-11T18:41:42-05:00",
    "input_file": {}
  }
]
