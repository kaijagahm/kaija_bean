[
  {
    "path": "posts/2021-10-14-publishing-scrollable-slides-in-rmarkdown/",
    "title": "A Semi-Automated Process for Converting Slides to a Blog Post in RMarkdown",
    "description": "Recently, I adapted slides from one of my talks into a blog post format, with each slide's narration transcribed below it. In my experience, people don't usually post slides this way, and the process of converting a presentation to a blog post wasn't trivial. Here's the process I followed--I hope it helps anyone else who would like to do this too.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\n\nContents\nMotivation\nWhat you’ll need\n1. A slide deck\n2. Slide-by-slide transcriptions or captions\n\nStep by step\n1. Create an RMarkdown document\n2. Convert slides to images\n3. Compile your transcription data\n5. Make the for loop\n6. Knit the document\n\nAdding a video of your talk\nOptions for video storage\nEmbed the video\n\n\nMotivation\nIn July 2021, I gave a talk at the inaugural SORTEE conference, describing my experience auditing colleague’s workflow to improve reproducibility.\nRecently, I posted my slides here after converting them to a blog post/text format. My goal was to present each slide, followed by a block of text, essentially transcribing what I said during the presentation.\nThe goal: To transform a slide deck to a scrollable blog post where each slide is followed by its caption or transcriptionWhy did I want to do this in the first place? In my experience looking for help and ideas on the internet, it’s often easier to skim a text-based blog post than to watch an entire video.\nIt’s become very common to post slides after giving a talk, and I think that’s fantastic. Here are some examples:\nElla Kaye | Maëlle Salmon | Alison Hill\nThe problem is, my slides are often pretty minimal. I try to communicate my points mainly through images and diagrams. If you weren’t there for the talk, I don’t think you’d get a lot out of looking at my slides by themselves. So when it came time to post my talk and its slides, I wanted some way to convert it into a blog post, without totally reinventing the wheel.\nMy slides are pretty heavy on diagrams and don’t have a lot of text. Without the context of my narration, you probably wouldn’t really know what’s going on.Here, I’ll outline the process that I used to achieve that format. I hope you find this helpful. If you’ve done a similar thing, feel free to comment with any tips and tricks or lessons learned!\nWhat you’ll need\nThis post refers to the process for converting slides to a scrollable RMarkdown document, like this post.\nYou will need two basic components:\n1. A slide deck\nWhile I dream of one day becoming proficient in xaringan, for now I make my presentations in Google Slides, which integrates pretty well into my institution’s Google-based storage system.\nSo, after creating my talk, I had a nice slide deck saved as a Google Slides presentation. Yours might be a Powerpoint, a pdf, or some other type of presentation. I won’t speak to the particularities of each type of slide deck, but the important thing is that you need to be able to convert your slides to a pdf document, one slide per page. If you can do that, this process will work.\nMy slides!2. Slide-by-slide transcriptions or captions\nTo display your slides as a scrollable page, with each slide followed by some text, you’ll of course need the text for each slide.\nFor this process, I’m assuming that you already know the text you’d like to associate with each slide, up front.\nWhy? Well, of course you could insert slides one by one into RMarkdown and then type out the text in the document after each one. But that’s laborious and slow, and it kind of defeats the point of automating the process.\nInstead, I’m assuming that you already have the text written. Examples: - You already had to write down and provide talk transcripts for the venue where you presented your talk, and you have them saved somewhere. - You want to take some time to click through each slide and write about it. - You’ve written presenter notes in Google Slides/Powerpoint/Keynote/whatever and want to present the notes along with the slides.\nStep by step\nOkay, armed with your slides and transcripts or text, let’s do this.\n1. Create an RMarkdown document\nIf you’re publishing your post in Distill, you might want to use distill::create_post().\nOtherwise, just create a regular old RMarkdown document.\nMake sure to take note of where your RMarkdown document lives. You’re going to need to store the materials (slides and captions) in a folder somewhere logical–either in your post folder for Distill, or maybe in a subfolder of your RStudio Project. (Consider pairing this with the here package–it helps get around the weirdness with working directories in RMarkdown.)\n2. Convert slides to images\nI got stuck when it came to extracting each slide from Google Slides as an image. Unfortunately, there’s no good way to download Google Slides as individual images. The thought of taking individual screenshots of slides made me want to abandon the whole project.\nInstead, I started by saving the slides to a PDF document. In Google Slides, you can choose File > Download > PDF Document (.pdf)\nSaving a Google Slides presentation as a PDFNext, I needed to convert the PDF to a series of images so I could insert them one by one into the RMarkdown document.\nI did this using smallpdf to convert the multi-page PDF to a series of JPG images. (Smallpdf has free features if you create an account or log in using e.g. Google, or there’s a free trial for some of their paid features. Converting a pdf to jpg images seems to be a free feature–I had no problem doing it using my Google account.)\nThe images will be saved by default with file names like 0001.jpg, in order. Put all of the images into a folder and save it as a subdirectory of your post/project folder, or wherever makes sense in your Project directory. Make sure they’re in their own folder (no other files), and all in the same place.\n3. Compile your transcription data\nNow, let’s get your captions or transcripts in a format that will be easy to use. Eventually, we’ll want a CSV or text file with each slide’s caption in its own row.\nYou can create this file however you want. I made a new document in Google Sheets and copied and pasted my slide transcripts into successive rows from the document in which I had written them.\nI also added an index column for good measure, to keep track of the slide numbers. I wanted to be able to cross-reference the slide image file names with the captions later on, to make sure I’d assigned the right caption to each slide.\nI exported the finished sheet as a csv, so it looked something like this:\n\n\n\nA quick note about line breaks and text formatting:\nBecause we’re passing in the caption data from an external spreadsheet, there are limitations to how much text formatting we can apply to them. It would be easier to be able to edit each caption in RMarkdown, with all the formatting possibilities that affords, but doing that makes automation impossible. So I kept my formatting pretty simple.\nIf you do want to add formatting, you can use html formatting tags to make your text bold, italic, etc. or to insert line breaks.\nFor some reason, \\n doesn’t work to create a line break in the slide captions. But the html tag <br> does, so I used <br> pretty liberally to break up long, multi-paragraph slide captions. ### 4. Format input data for the for loop\nWe’re going to be programmatically inserting each slide, with its caption, into the RMarkdown document. I used a for loop to do this.\nYou’re going to need two inputs for the for loop: a vector of slide image file paths and a vector of slide captions. I chose to combine the two into a single data frame, although you don’t strictly have to do this.\n\n\n# Read in the csv of slide captions\ndescriptionData <- read.csv(\"retroactivelyReproducible_slideCaptions.csv\")\n\n# Create a vector of slide image names by listing the image file paths\nfileNames <- list.files(path =\"slideImages\", \n                        full.names = T) # full file paths\n\n# Create a data frame combining the file paths and captions\ninput <- data.frame(slideDescription = descriptionData$description,\n                    slideImage = fileNames)\n\n\n\n5. Make the for loop\nNext, I used a for loop to combine the data into a dynamically-generated portion of the RMarkdown document. The code I used is adapted from this StackOverflow post.\n\n\n# Set up a for loop for as many iterations as there are slides\nfor(i in 1:nrow(input)){\n  # For convenience, make a temp object for the current slide\n  currentSlide <- input[i, ] \n  \n  # Create raw markdown code to insert each image, pasting in the file name\n  cat(paste0(\"![](\", currentSlide$slideImage, \")\"))\n  \n  # Add the slide description immediately under the image\n  cat(currentSlide$slideDescription)\n  \n  # Add space after the description to differentiate it from the next slide\n  cat(\"<br><br><br>\")\n}\n\n\n\nNote: For the code chunk containing this for loop, make sure to set the chunk option results = 'asis'. It’s important to do this because we’re dynamically generating a raw markdown string to insert the images, so we need the results to be evaluated as if they had been typed into RMarkdown by hand. If you don’t set results = 'asis', then the results will show up as RMarkdown chunk output and none of the formatting will come through.\nAlso, set echo = FALSE so that the code itself doesn’t show up.\nSo, the final code chunk should look something like this:\n\n\n\n6. Knit the document\nBe sure to test out this code a few times by knitting the document to make sure it looks good! Adjust the line spacing and other formatting as necessary. I knitted my markdown to html because I was putting it on a Distill site; I’m not sure how well this process would work for a knitted Word or PDF document.\nNote: Sometimes, the formatting for dynamically-generated markdown like this won’t show up correctly in the html preview of your knitted RMarkdown document. Make sure to open the page in a browser window to see how the final product will actually look.\nAdding a video of your talk\nI was excited that the above process worked for turning a slide deck into a scrollable blog post. But I also wanted to embed the video recording of my talk in the same post so people could access that medium if they preferred.\nA lot has been written about how to embed videos in RMarkdown outputs, but some of the approaches I found didn’t work for me. I don’t quite understand why code that worked for other people failed for me, but here’s what did work:\nOptions for video storage\nYou can either store your video locally and embed it similarly to how you’d embed a photo, like this. This works best if the file is pretty small, especially if your RMarkdown (like mine) is backed up to a GitHub repository.\nMy video was over 100MB, so GitHub wasn’t an easy option. I started looking into alternatives.\nThe easiest thing turned out to be to upload the video to YouTube and leave it as “unlisted” (publicly accessible, but not findable in a search–you can only see it if you have access to the direct video link).\nThis approach does require that you have a YouTube account, but you can sign up for free through Google. I think YouTube also has some restrictions for how long/how big your videos can be, and what formats are acceptable. Mine was 10 minutes and didn’t need to be high definition, and it was saved as .mp4, so this worked well for me.\nInstructions on how to upload a video to YouTube here.\nNext, find the video’s embed link (instructions). Copy that link to your clipboard; you’ll use it in the next code chunk.\nEmbed the video\nI embedded the video as an iframe in html. The code looked like this, and it can just go in the main body of your RMarkdown document (doesn’t have to be in a code chunk):\n<iframe width=\"560\" height=\"450\" src=\"https://www.youtube.com/embed/cJq2kp114Mg\" \ntitle=\"Your Title Here\" frameborder=\"0\" allow=\"accelerometer; \nautoplay; clipboard-write; encrypted-media; gyroscope; \npicture-in-picture\" allowfullscreen><\/iframe>\nSubstitute your own video link for the one above, of course.\nTweak the dimensions of your video until it looks right. 560 by 450 isn’t anything special, it’s just what ended up looking decent for my video.\nThere you have it! Again, here is the post as it appears in its final form.\nThe full code I used to create the post is on my GitHub.\nThanks for reading!\n\n\n\n",
    "preview": "posts/2021-10-14-publishing-scrollable-slides-in-rmarkdown/schematic.jpeg",
    "last_modified": "2021-10-20T16:46:10-07:00",
    "input_file": "publishing-scrollable-slides-in-rmarkdown.knit.md"
  },
  {
    "path": "posts/2021-05-12-greent-how-to/",
    "title": "greenT part 2: Behind the app",
    "description": "A more detailed explanation of some of the code that makes the greenT app work.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-05-13",
    "categories": [],
    "contents": "\n\nContents\nHow did I:\nChange the font across the entire app?\nGetting the font\nApplying the font\n\nMake the title and subtitle have two different formats?\nPlot the colored blocks?\nMaking the plot\nConditionally displaying letters\n\nChange the color of the ggplot letters based on the rectangle color?\nSave the rectangle plot as an image?\nMake the contribution form?\nSave the contributed data to Google Drive?\nSetup\nGet a token\nNon-interactive authentication\nNotes and caveats\n\nWork efficiently with all 36 color selectors?\nMaking some vectors\nExample: Creating the selectors\nArranging the letters horizontally\n\n\nGet involved\n\n\n\n\nFigure 1: The digits 0-9, according to my brain\n\n\n\nThe other day, I posted an app about grapheme-color synesthesia, called greenT. This is part 2 of a blog post about the app. Part 1 gave an introduction to the who/what/why of the app, and provided a little guided tour through each page and feature. You can read part 1 here.\nThis post will go into more detail about some of the code in the app. If you’re interested in Shiny and want to know how I made everything work, this is the post for you. I picked a few features that were the most interesting to me, and/or the ones that I had to do the most independent troubleshooting to figure out. If you have questions about the rest of the app, get in touch via email, Twitter, or GitHub and I’ll do my best to respond!\nOk, here goes.\nHow did I:\nHere, I’ll discuss some of the app’s parts in more detail, in the spirit of providing ever-more resources for Shiny developers. I’m going to focus on things I struggled with, because most likely, if I didn’t struggle, the answer is available on Google somewhere.\nOf course, you’re also welcome to look at my code, on GitHub. The main scripts are ‘app.R’ (the main app script) and ‘defs.R’ (which defines some supporting functions). Then ‘about.R’ and ‘contribute.R’ just have a lot of long text, to avoid cluttering up the main script with it.\nChange the font across the entire app?\nJonathan created a file called ‘tea-style.css’. It lives inside the folder ‘www/’ in the main app directory. The file path written using the here package is here::here(\"www\", \"tea-style.css\").\nGetting the font\nInside ‘tea-style.css’, I import the font I want to use, like this:\n@import url('https://fonts.Googleapis.com/css2?family=Baloo+2&display=swap');\nThat URL comes from Google Fonts, which is where I found Baloo 2, the font that I decided to use. I clicked on it to see the different styles (weights, italic, etc.) and selected the one that I liked by clicking “Select this style” on the right.\n\n\n\nFigure 2: Choosing a style from the Baloo 2 font\n\n\n\nClicking that + sign popped open a dialog on the right. I chose the ‘import’ option instead of <link>, and then copied the import code within the <style> tag.\n\n\n\nFigure 3: Copying code to import Baloo\n\n\n\nApplying the font\nThen, in the same ‘tea-style.css’ file, I added this code:\n* {\nfont-family: 'Baloo 2', sans-serif;\n}\nThe * tells css to assign that font to all text in the app, not just a particular class of element. The ‘sans-serif’ after the name of the font tells the app to default to a generic sans-serif font if Baloo 2 were to be unavailable. And don’t forget the semicolon at the end of the line!\nMake the title and subtitle have two different formats?\nGooood question. I don’t really know html and css. In this case I kind of Googled around until I found something that worked. And I completely despaired of being able to get the title to be large, so I asked Jonathan, and he showed me where to put the font size. Here’s the relevant code snippet, within the Shiny UI:\n\n\n# Title and subtitle, two different formats\ntitlePanel(div(HTML(\"<b style = 'font-size: 30px;'>greenT<\/b><em><small>\n                    exploring grapheme-color synesthesia<\/em><\/small>\")),\n           # windowTitle controls what shows up on the browser tab\n           windowTitle = \"greenT\")\n\n\n\nPlot the colored blocks?\nJust some ggplot magic!\nMaking the plot\nHere’s the ggplot code that plots the rectangles. If you want to fully understand the data structure that goes into the plot, you’ll have to take a look at the app and trace everything back: rectangleDF() is a reactive expression that depends on all the color inputs and the text that the user enters.\nI use geom_rect to make rectangles, defining their dimensions so that they plot right up against each other. I had to use scale_fill_identity so that the rectangles would be filled with the literal color values included in the input data frame, instead of ggplot’s default behavior, which is to map colors to values based on an external color palette.\ntheme_void() gave me a totally blank background–no axis labels, no ticks, no nothing. I wanted the plot to read as “an image of colored rectangles”, rather than as “a ggplot”.\n\n\n# Plot color blocks -------------------------------------------------------\nplotVals <- reactiveValues() # initialize a reactiveValues object to store the plot object\noutput$colorBlocks <- renderPlot({\n  p <-  rectangleDF() %>%\n    ggplot() +\n    geom_rect(aes(xmin = xmin, xmax = xmax,\n                  ymin = ymin, ymax = ymax, \n                  fill = hex))+ # fill w hex colors\n    scale_fill_identity()+ # take the literal hex values as colors, instead of mapping other colors to them.\n    theme_void() + # totally blank background\n    {if(input$showLetters)geom_text(aes(x = xmin + 0.5, \n                                        y = ymin + 0.3, \n                                        label = grapheme,\n                                        col = contrastColor),\n                                    size = 10,\n                                    family = \"Baloo 2\")}+\n    scale_color_identity()\n  plotVals$rectanglePlot <- p\n  print(p)\n})\n\n\n\nConditionally displaying letters\nThen, I included a conditional layer: only add the letters if the interactive toggle switch is TRUE; otherwise remove the layers. I often find myself wanting to make conditional layers, especially in Shiny, and I can never remember the syntax on the first try. I refer back to Stack Overflow posts like this one. Again with the community!\nNote that I was having trouble getting the ggplot to show up until I assigned it to a variable, p, and then included the print(p) statement at the end. Not sure why, but this worked.\nThere are two other interesting pieces of code here that work together with other parts of the app:\nChange the color of the ggplot letters based on the rectangle color?\nI took the same approach for the text color as for the rectangle background colors. I encoded the text color (either #FFFFFF/white or #OOOOOO/black) in the dynamically-created input data frame (a reactive expression) and then used scale_color_identity() to tell ggplot to use those literal hex values to color each letter, instead of mapping other colors onto them. You can see the scale_color_identity() code in the plot code snippet above.\nThe tricky part was dynamically assigning each letter to either white or black based on the chosen background color. I figured I shouldn’t need to reinvent the wheel here. Surely web designers have already figured out how to do this. Once again, I Googled, and I found this post, which gives a handy formula for calculating the best contrast color based on the rgb values of the background. The recommended threshold value in that post is 186; I fiddled around and ended up settling on 140 because, I don’t know, it just looked better to me.\nTo calculate these colors, I had to convert my color hex codes into individual r, g, and b values, which I did using the col2rgb() function.\nHere’s the full code I used for those color computations. Incidentally, you’ll notice that the reactive expression being calculated here is rectangleDF(), which becomes the input to the plot code I shared above.\n\n\n# Create df input for the ggplot ------------------------------------------\nrectangleDF <- reactive({\n  req(length(split()) > 0) # will only work when there is text entered in the box\n  data.frame(grapheme = split(),\n             ymin = 1,\n             ymax = 5) %>%\n    mutate(xmin = 1:nrow(.), xmax = 2:(nrow(.)+1)) %>%\n    left_join(colorsDF(), by = \"grapheme\") %>%\n    mutate(r = col2rgb(hex)[1,],\n           g = col2rgb(hex)[2,],\n           b = col2rgb(hex)[3,]) %>%\n    mutate(contrastColor = case_when((r*0.299 + g*0.587 + b*0.114) > \n                                       140 ~ \"#000000\",\n                                     TRUE ~ \"#FFFFFF\"))\n})\n\n\n\n(I thought about giving line numbers for the code, but I plan to keep editing this app, so the line numbers would keep changing. You can use ctrl+F in the app.R script, though)\nSave the rectangle plot as an image?\nThis part was surprisingly easy, actually! Shiny has a nice downloadHandler() functionality that has two parts: one where you define the content you want to download, and one where you define the (dynamically-created) file name. The syntax is a little weird, so I copied and pasted the code from the Shiny docs and then filled in my parts.\nI’ve reproduced the download code below. The filename function converts the user-entered text to lowercase and replaces all non-alphanumeric characters to underscores, so we get a nice file name consisting of underscore-separated words. Then I append a nicely formatted date.\nNext, I need to define the object to be downloaded–in this case, the rectangle plot. Take a look back at the ggplot code I posted above. You might have been confused by this line, at the top: plotVals <- reactiveValues(); and this line, at the bottom: plotVals$rectanglePlot <- p. What we’re doing there is defining a reactiveValues object that gets updated each time that the ggplot re-renders.\nNow, inside the content function in the downloadHandler, I ggsave() the currently-displayed plot (plotVals$rectanglePlot).\n\n\n# Download rectangles plot as image ---------------------------------------\noutput$downloadPlot <- downloadHandler(\n  filename = function(){\n    paste0(str_replace_all(tolower(input$displayText),\n                           \"[^a-z0-9]\", \"_\") %>% \n             str_replace_all(., \"_{2,}\", \"_\"), \n           \"_\",\n           dateTimeFormat(),\n           '.png')\n  },\n  \n  content = function(file){\n    req(plotVals$rectanglePlot)\n    ggsave(file, plot = plotVals$rectanglePlot, \n           device = 'png', width = length(split())/2, \n           height = 5/2)\n  })\n\n\n\nMake the contribution form?\nI closely followed Dean Attali’s tutorial, in most cases using his code directly. Of course, I wrote my own form questions.\nThere are three main differences between my approach and Dean’s.\nWhile Dean collected responses that would be added to an existing data sheet, each as a new line, I wanted to save a whole data frame per response. For that reason, my contributionData() reactive expression does a bunch of data reformatting. In the following code, I bind together the color data that the user has previously entered (co-opting the colorsForExport() reactive expression that I created in order to allow the user to download the color data as a csv) and the responses that the user gave to the demographic questions in the form.\nOf course, this relies on the user having previously entered their color data on the Explore page before filling out a form. I added a mandatory checkboxInput with text to this effect.\nI actually don’t love this approach. I think it would be less confusing for the user to have a “mirror” of the Explore tab’s color selectors show up on the Contribute page, so they could tweak their colors before saving the form. I don’t know if it’s possible to have a single input widget show up in multiple places in a Shiny app (so that changing it in one place will automatically also change it in the other places), and I suspect it’s not.\nSo in order to make this work, I would have to program a whole new set of color selectors, and then add some logic that updates each selector in response to a change in its mirror, and vice versa. I’m sure this is possible, but I don’t know how to do it off the top of my head, and I left it out for the sake of time.\nI added some dynamic question options. I wanted respondents to be able to self-describe their gender identity. Also, for the question at the end about different types of synesthesia, I wanted respondents to be able to include types that I hadn’t listed. I listed most of the major synesthesia types I could find based on my research, but you never know!\nThis turned out to be pretty simple, using the uiOutput/renderUI pair of functions. Here’s a snippet of code from the UI portion of the form:\n\n# (more form questions above)\nradioButtons(\"gender\", labelMandatory(\"What is your gender identity?\"),\n             choices = c(\"woman\", \"non-binary\", \n                         \"man\", \"prefer not to say\", \n                         \"prefer to self-describe (click to add text)\"), \n             selected = character(0), width = \"100%\"),\nuiOutput(\"genderSelfDescribe\"), # here's the uiOutput that dynamically generates a new question if the respondent wants to self-describe their gender.\nradioButtons(\"sex\", labelMandatory(\"Sex assigned at birth:\"),\n             choices = c(\"female\", \"male\", \"intersex\", \n                         \"prefer not to say\"), \n             selected = character(0), width = \"100%\"),\n# (more form questions below)\n\nSandwiched between the gender and sex questions, the line uiOutput(\"genderSelfDescribe\") allows a dynamically-generated question to appear if the respondent selects “prefer to self-describe” in the gender question.\nHere’s the code on the server side that makes this possible:\n\n\n# Contribute form -------------------------------------------------------\n# Dynamic options\n## self-describe gender\noutput$genderSelfDescribe <- renderUI({\n  req(input$gender)\n  if(!input$gender == \"prefer to self-describe (click to add text)\"){\n    return(NULL)\n  }else{\n    textInput(\"genderSelfDescribe\", \n              label = NULL, \"\")\n  }\n})\n\n\n\nAnd then of course, there’s similar code to provide a dynamic option for the free-text synesthesia types question.\nWhile Dean’s tutorial describes saving responses using persistent data storage on the server where the Shiny app is hosted, he also points out that this isn’t possible on shinyapps.io. So I decided to use one of the other options that he outlines in the follow-up to the original tutorial: storing the data on Google Drive uisng the Googlesheets4 package.\nSave the contributed data to Google Drive?\nIt took me a while (lots of trial and error, and lots of Google searches that I can’t reproduce) to figure out how to get the data saving to Google Drive to work.\nBasically, here’s the challenge. In order to read and write to Google Drive, you have to “authenticate”, i.e. “log in” to an account. That makes sense–of course it would be bad if I could make an app that could modify files in some random other person’s account with impunity.\nBut, typically when we authenticate Google Drive (or anything else) from an R script, the package does a little “dance” with your web browser. You run the code command to authenticate, and then a browser window pops open and prompts you to enter your password. You do that, and then the code will run back in your RStudio window.\nThat setup works fine for interactive programming, but it doesn’t work for a Shiny app. First of all, it would be super annoying for app users to have to deal with a separate browser popup just in order to be able to submit their forms. And more to the point, it wouldn’t help anyway–because they’d have to log into my Google account! So that’s not possible or desirable.\nThat means that in order to write data to Google Drive directly from my Shiny app, I’ll have to use “non-interactive authentication” via a secret “token”.\nHere’s how I did that.\nSetup\nFirst, I had to authenticate interactively one time, in order to set things up. To do this, I first created a hidden directory inside my main Shiny project and called it “.secrets”. To do this, I opened up the Terminal window inside RStudio. (You can also do this with the Terminal app, or the equivalent, on your computer–it doesn’t have to be through RStudio.)\n\n\n\nFigure 4: Making the .secrets directory\n\n\n\nFirst, I checked to make sure I was in the root project directory by typing ls and checking what other files were there. Alternatively, you could print the working directory with pwd.\nOnce I determined that I was indeed in the root directory, I typed mkdir .secrets. The . before ‘secrets’ will make the directory hidden.\nNext, and this is important! I hid ‘.secrets’ from being tracked by git. It’s a good idea to do this right at the beginning to make sure you don’t forget. You definitely do not want your private access token to end up on GitHub for all the world to see!\nIf you have git running through RStudio, an easy way to do this is to go up to the git panel, click the gear icon, and then click “Ignore”. This will open the .gitignore file. Now just add ‘.secrets’ as a new line in the .gitignore file to ignore the entire contents of the secrets directory. If you’re more comfortable using the terminal, you can of course open the .gitignore using nano .gitignore or similar.\n\n\n\nFigure 5: Opening the .gitignore file\n\n\n\nOkay, now we need to load the googlesheets4 and gargle packages. gargle manages token authentication, and googlesheets4 is what we’ll use to write the data to Google Sheets.\n\n\nlibrary(googlesheets4) # we'll use this for writing to google sheets\nlibrary(gargle) # for authentication/tokens\n\n\n\nNext, set some options(). You can do this part in the console–it only needs to be done once. You’re just telling gargle where to save the token that you’re about to generate.\n\n\noptions(gargle_oauth_email = \"kaija.gahm@aya.yale.edu\", # the email I wanted to use for the app\n        gargle_oauth_cache = \".secrets\") # \n\n\n\nGet a token\nIn order to actually generate a token, type gs4_auth() in the console. This should open up a window in your browser that will prompt you to sign in to your Google account. If you, like me, have multiple Google accounts (I have five right now), double-check that you’re signing into the same account that you specified in the options() call above. That’s the account whose Drive you’ll be writing to from your app.\nOnce you’ve authenticated, you should get a plain-looking screen that tells you the authentication is complete. You can close that window and go back to R now. (This is the auth “dance” I mentioned earlier).\nNow, double-check that your token got added to your ‘.secrets/’ directory. It should get put there automatically because of the options you set. If it didn’t, try re-running the options() code above and re-trying the authentication.\nI checked the contents of the ‘.secrets/’ directory by using the Terminal window in RStudio to navigate into ‘.secrets/’ and then list the files inside.\n\ncd .secrets\nls\n\nYou should see something like nfo674ndu6e8gh50p24q2nm8hk3a6e0k_kaija.gahm@aya.yale.edu (obviously that’s not my real token, but it has the same format–a string of numbers and letters, followed by an underscore and then your email address.)\nNon-interactive authentication\nOkay. Now you have a token. Phew! The next step is to set up non-interactive authentication. I did this by following this vignette. It’s really long. Don’t panic! The section you want is Project-level OAuth cache.\nYou’ll notice that the first part of this section gives instructions on how to do basically what we just did: create a “.secrets/” directory and use a one-time interactive authentication to store a token in it. Here, they use the googledrive package as an example. My example above used googlesheets4 instead, but it’s the same process.\n\n\n\nFigure 6: My alphabet\n\n\n\nNow, follow the rest of the steps in this tutorial. They include adding this code at the top of your app, which you’ll see at the top of my own app.R file.\n\n\noptions(\n  gargle_oauth_cache = \".secrets\",\n  gargle_oauth_email = TRUE\n)\n\n\n\nNotes and caveats\nNote that there are other approaches to non-interactive authentication that use a Google service account. I tried to get this set up and didn’t have any luck–or rather, I got it set up, but then the process of actually accessing the data in a Google service account is complicated and I gave up and followed this tutorial instead with my personal account. For a small, personal app like greenT, that’s fine. If you want to implement something bigger or more sensitive, you should look into that approach. Maybe you’ll succeed where I didn’t. A warning: the Google API website is one of the most confusing sites I’ve ever tried to navigate, and I’m no Luddite.\nOne more note on saving the form data to Google Sheets. Ideally, I would want each respondent’s data to be saved as its own sheet, and I’d want those sheets to be stored in a designated folder in my Google Drive. Unfortunately, and counter-intuitively, this turns out to be hard to do.\nThe googledrive package in R is designed for interacting with Google Drive file structures, while the googlesheets4 package is built to deal with individual Google Sheets files, but not so much with their organization within Google Drive. If you’re used to working with Google spreadsheets stored on Google Drive the normal way (i.e. through the web interface), it’s jarring to have these processes separated.\nI spent a while trying to find the functionality in googlesheets4 that would allow me to organize files. Finally, I came across this issue on the googlesheets4 GitHub page, where Jenny Bryan explains the separation of functionality between these two packages. She subsequently wrote a vignette on working with the two packages together. I followed the vignette and had a whole nice workflow set up where the data got written to a google sheet that was then moved to a ‘greenT/’ directory I had pre-created on Google Drive. But because both packages had to make calls to the googledrive and googlesheets4 API’s, the whole thing was too slow, and the user had to wait an inordinately long time for their form response to submit.\nI’m sure there’s a way to speed this up, but I needed to have it working on a short timeline, so I ended up sticking with individual google sheets for each response, stored in my main Google Drive. Not ideal, but I added a ‘greenT_’ prefix to each file name so at least I’ll be able to find them easily. (Note: the code snippet I’m reproducing here lives in ‘defs.R’ within my app.)\n\n\n# Function to save form data ----------------------------------------------\nsaveData <- function(data) {\n  fileName <- sprintf(\"%s_%s_%s\",\n                      \"greenT\",\n                      dateTimeFormat(),\n                      digest::digest(data))\n  \n  # Create an empty spreadsheet\n  ss <- gs4_create(name = fileName,\n                   sheets = \"data\")\n  \n  # Put the data.frame in the spreadsheet and provide the sheet_id so it can be found\n  sheet_write(data, ss, sheet = \"data\")\n}\n\n\n\nWork efficiently with all 36 color selectors?\nSince the app requires assigning a color to each letter (26) and digit (10), we have 36 color selectors to deal with, each with its own inputId. I called the letter selectors input$a, input$b, etc., and I called the number selectors input$one, input$two since there are annoying hoops to jump through if you want to make a number into a variable.\n36 is a lot of individual variables to type out. Frustratingly, it’s right on the cusp of what I’m willing to do manually–few enough that manual listing is possible, but enough that it’s a serious pain, especially if you have to list them out more than once.\nUltimately, then, I opted to deal with the color selectors programmatically. It was a little tricky to figure out, in part because there are three really three different lists of selectors.\nMaking some vectors\nThe inputId’s of the selectors. As I explained above, this is a combination of the letters of the alphabet (lowercase) with the spelled-out digits, “zero” through “nine”.\nThe display names of the selectors. For some arbitrary reason I decided I wanted them to display as uppercase letters and digits. Not sure why I did that. In retrospect, since everything shows up as lowercase in the ggplot, maybe I should have gone with lowercase. But regardless: the display names end up being c(A:Z, 0:9).\nThe characters that the selectors represent. This is important because I have to compile a data frame of all the selector input colors, to join to a data frame created from the text that the user entered. Since that text gets converted to lowercase, the selector inputs also have to be assigned lowercase letters in order to match up. So for this, we need yet another vector, c(a:z, 0-9). This is also how I want the characters to print out in the downloadable csv data. Lowercase just seems more standard and appropriate in that case.\nYeah, as I’m typing this I’m realizing that I really should have just combined steps 2 and 3 and gone with lowercase display labels. Oops. Note also: this ignores the possibility that someone might have different colors associated with uppercase and lowercase letters. That isn’t true for me, but it is true for at least one of my testers. Maybe I’ll add that functionality in the future!\nAnyway. I went ahead and defined these three vectors in my ‘defs.R’ script, like this:\n\n\ninputIds <- c(letters, c(\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \n                         \"six\", \"seven\", \"eight\", \"nine\"))\ndisplayNames <- c(LETTERS, 0:9)\ncharactersOut <- tolower(displayNames)\n\n\n\nThen, I could use those vectors to operate on all of my inputs at once, using functions like lapply and map2.\nFor example, let’s take a look at how I created the individual colourInput objects to display on the main page of the app.\nExample: Creating the selectors\nFirst, I defined a function, colorInit():\n\n\n# Function to create inputs -----------------------------------------------\ncolorInit <- function(x = .x, y = .y){\n  colourpicker::colourInput(x, y, value = randomColor(), showColour = \"background\")\n}\n\n\n\nThe .x and .y are default inputs because I know I’ll be using this function in a call to map2(). value = randomColor() initializes each selector with a random hex code when the app initially loads. And showColour = \"background\" shows the chosen color as the background of the selector, not as hex code text.\nNow, I can call this function in order to render the colorInputs in the UI. The column() code below had to be repeated 6 times (6 total columns) in the UI. There’s probably a way to streamline even that part of things, but I didn’t mind repeating myself 6 (rather than 36) times, and I didn’t want to write another function.\n\n\nfluidRow(\n  # ... (other columns)\n  column(width = 2,\n         purrr::map2(.x = horiz(inputIds)[7:12], \n                     .y = horiz(displayNames)[7:12], \n                     colorInit)\n  )\n  # ... (more columns)\n) # 6 columns in total\n\n\n\nSo, let’s take a look at this code. First, I’ve created a fluidRow() that will serve as the horizontal strip where all the color inputs are housed in the UI. Because I know I want to lay out the selectors in a 6x6 grid, I create 6 column() definitions, each with width 2 (a width of 12 means “take up the full page”, so dividing that by 6 gives a width of 2 for each column).\nThen, I use map2() to render the inputs. I define two arguments, .x and .y. (Ignore the horiz() function for now–I’ll talk about that in a second.) .x is going to be the inputId’s of the color selectors, so for that I can use inputIds, one of the three vectors I created in the previous step. .y is going to be the labels, or what actually gets displayed next to the selctors. So for that, I can use displayNames, the vector I created that includes capital letters and digits. Finally, colorInit is the .f argument (which I probably should have labeled as such–oops.) So, map2() passes .x and .y into that colorInit() function that I just defined, and for each element of the input vector, it creates a colourInput() object with inputId taken from the .x vector and label taken from the .y vector.\nArranging the letters horizontally\nCool, okay. So now let’s talk about the horiz() function, and also those indices [7:12]. The input vectors I created are, naturally, in alphabetical order, from A-Z and then 0-9. But I’m creating my inputs in 6 columns. If I populated the columns in order of that vector (for example, using elements [1:6] for the first column; [7:12] for the second column), I’d end up with the alphabet going down the columns instead of across rows.\nThat’s fine, I guess, but I find it more naturally to read left to right, top to bottom (instead of top to bottom, left to right). I want the top left selector to be A, and then the one directly to its right to be B, because I think that’s what the user will expect.\nSo, I created a function called horiz(). Here it is:\n\n\n# Function to reorganize the inputs horizontally --------------------------\nhoriz <- function(vec = inputIds, nrow = 6, ncol = 6){\n  mat <- matrix(vec, nrow = nrow, ncol = ncol, byrow = T)\n  horizVec <- c(mat)\n  return(horizVec)\n}\n\n\n\nhoriz() takes my (alphabetically-ordered) vector as input. It then generates a matrix from that vector, with byrow = T (i.e. laying out the vector in rows, not in columns–the way I want it!). Next, it uses c() to collapse that matrix back down into a vector. Because R works on columns by default, the matrix will be collapsed column-wise, not row-wise. So the vector returned will be c(\"a\", \"g\", \"m\", \"s\", \"y\", \"4\", \"b\", \"h\", \"n\", \"t\")… etc. etc.\nSo now, I can use the elements of that vector, 6 at a time, to populate and name the color selectors for each column.\nIn the case of the code snippet above that uses map2(), I’m making the second column, so I use elements [7:12] of each transformed vector. horiz(inputIds)[7:12] gives me c(\"b\", \"h\", \"n\", \"t\", \"z\", \"5\"), and horiz(displayNames)[7:12] gives me c(\"B\", \"H\", \"N\", \"T\", \"Z\", \"5\").\nYou might be wondering why I went to all this trouble. Why not just make the selectors row-wise, instead of column-wise?\nThere definitely might be a good way to do this (please share it if you know how!) but I couldn’t figure it out. It’s not trivial to align things using CSS, and I think that’s what I would have had to do in order to ensure equal spacing. I know R and I don’t know CSS, so I ended up going with the R-centric solution. Besides, I got to feel clever writing functions and such.\nGet involved\nThat’s it for specific notes on the code for this app! If you’re curious about the rest of the code, please check out the repository, which is publicly available here. If you have specific questions, you can email me, and I’ll try to get back to you (but no promises).\nI also welcome contributions, if you’re interested in making this app better! Go ahead and file an issue or submit a pull request on GitHub. I’m going to be busy over the coming months and I’m not sure how much time I’ll have to work on this, so I may not be actively responding to issues. But I welcome any and all contributions or ideas for the future.\nIf you haven’t already checked out my first blog post about this app, you can read it here. And the app itself lives here.\nThanks for reading.\n\n\n\n",
    "preview": "posts/2021-05-12-greent-how-to/numbers.png",
    "last_modified": "2021-05-13T06:55:34-07:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "posts/2021-05-09-greent/",
    "title": "greenT: Exploring grapheme-color synesthesia",
    "description": "An introduction to a Shiny app I built to show friends the inner workings of my brain--and some musings on my experience learning Shiny this year.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-05-09",
    "categories": [],
    "contents": "\n\nContents\nIntroduction\nA guided tour\nPage: ‘Explore’\nPage: ‘About’\nPage: ‘Contribute’\n\nFuture enhancements\nGet involved\nThoughts on the process of building the app\nBehind the app\n\n\n\n\nFigure 1: The digits 0-9, according to my brain\n\n\n\nThis article has been adapted from my entry into the 2021 RStudio Shiny contest, posted on RStudio Community.\n\n\n\nFigure 2: My alphabet\n\n\n\nIntroduction\nMy name is Kaija, and I have what’s known as grapheme-color synesthesia. It’s a (completely benign!) phenomenon where I experience letters and numbers as having colors associated with them. I don’t literally see colors floating in the air, but I’ve consistently associated a color with each letter and number for most of my life.\nHaving synesthesia doesn’t affect my life much. It occasionally comes in handy for remembering credit card PINs and phone numbers, and sometimes I draw associations between completely unrelated words because they happen to have similar colors. But it’s mostly just a fun little party trick.\nStill, when I tell my friends about my synesthesia, they want to know what color their name is. And while I can certainly type out “magenta, red, brown, pale yellow” (Mary) or “magenta, red, blue, blue, whitish, gray” (Maddie), that can be hard to picture. And it’s frustrating to have to consciously translate my mental impressions of color into words.\n\n\n\nFigure 3: Some example names: Mary and Maddie\n\n\n\nI wanted a way to quickly represent any word or words in the colors I see it in. I did a bit of googling to see if anyone had come up with a synesthesia simulator. The closest thing I found was Bernadette Sheridan’s website synesthesia.me, which did exactly what I wanted to do, but only for Bernadette’s own colors! Cool, but not useful to me or to any other synesthetes (like my brother) who might want to show me their own colors.\nSo, I built this app. I’m excited to add more features in the future, but for now, I’ll explain the three main parts.\nThe source code for the app is here.\nA guided tour\nPage: ‘Explore’\nAt the top of the app’s main page, you’ll see 36 color selectors (developed using the colourpicker package), one for each letter and digit. When you load the app, the colors are chosen randomly, but you can change any or all of the colors by clicking on each selector yourself. If you find it easier to start from white selectors, click “Set all to white” and then choose colors one at a time.\n\n\n\nFigure 4: The ‘Explore’ page\n\n\n\nIf you have synesthesia, you can set your own colors. If you don’t, feel free to use the app to choose whatever colors look good to you. Or you can click the “Kaija’s colors” button to see what color your name is in my head, similar to Bernadette’s project. Once you’re happy with the colors you’ve set, feel free to hide the selectors using the “Show/hide selectors” link at the top.\nNext, you can enter text you’d like to translate into colors. The text will plot as rectangles, with the letters optionally superimposed in either white or black, whichever provides the best contrast to the color you’ve chosen. Use the toggle switch to remove the letters. Note that for simplicity, all text gets converted to lowercase, and all non-alphanumeric characters show up as white.\n\n\n\nFigure 5: Entering text to display\n\n\n\nIf you want to show your friends what colors their names are, download the plot you’ve created using the “Download rectangles as .png” button. If you’re interested in analyzing your colors further, download the color values themselves, as both hex codes and RGB values, using the “Download colors as .csv” button.\n\n\n\nFigure 6: Here’s an example of the downloaded color data\n\n\n\nPage: ‘About’\nThe About page gives a bit of backstory for the app, similar to what I’ve written here. It also links to a few more resources you might be interested in if you want to learn more about synesthesia. And most importantly, I use this page to thank Jonathan Trattner, Dean Attali, and Bernadette Sheridan for their contributions to this app, in code or inspiration.\nPage: ‘Contribute’\nI’m always on the hunt for projects, and I’ve thought for a while that I would love to analyze data on my own colors and those of other people. For example, past research has found that there are some patterns in which letters synesthetes associate with which colors. I’m also interested in intra-individual variation. For example, while my K is a very strong magenta color and always has been (it has never varied for as long as I can remember, and it doesn’t change based on the letters around it), some of my letter-color associations are much weaker. E, which I’ve encoded in this app as gray, is really almost colorless or translucent. It tends to take on the colors of the letters around it. So I’d love to repeatedly collect my own color data and analyze it.\n\n\n\nFigure 7: My own name in my colors\n\n\n\nIf you want to contribute your data to my (independent, informal, unofficial!) research into this phenomenon, I encourage you to fill out the form on the Contribute page. Make sure to set your colors using the selectors on the Explore tab before filling out the rest of the form–when you click “Submit”, the currently-selected colors will be recorded along with the demographic info you’ve entered.\n\n\n\nFigure 8: The contribution form\n\n\n\nAlso: feel free to submit colors more than once, if yours are a bit variable like mine are! Just use the same name and (optionally) email address, so I can relate your responses to each other.\nFuture enhancements\nHere’s what I’d like to add to this app in the future:\nBookmarking, so you can save the colors you’ve entered and come back to them\nAbility to upload your previously-downloaded data (more robust version of bookmarking)\nA continuously-updating analysis page that shows statistics based on previously-submitted data (anonymized!)\nOpacity toggles or other more nuanced formatting tweaks for the colors\nA “colored text” mode that displays the letters themselves in color\nGet involved\nIf you’re interested in contributing to the app, submit an issue or PR on GitHub, or get in touch! I’d love to collaborate with you.\nThoughts on the process of building the app\nBuilding this app was a joyful process because it made me acutely aware of how far I’ve come with Shiny in less than a year.\nI had never built a Shiny app until July 28th, 2020, when I watched Garrett Grolemund’s Shiny tutorial videos and started playing around with a simple app. By chance, the Palmer penguins dataset was up for Tidy Tuesday that week, so I made a simple data explorer app for that dataset. I was surprised at how easy it was.\n\n\n\nI had started learning Shiny in order to build a much more complicated data explorer app for the Yale Grammatical Diversity Project. I really didn’t know what I was getting myself into–I had pitched the idea to my boss at the YGDP with the clear caveat that I had never programmed in Shiny before, ever. But I was confident that I could figure it out. I had yet to encounter a skill in R that I couldn’t crack if I put my mind and time to it.\n\n\n\n“Time” turned into around nine months of programming, debugging, and fiddling. It was a frustrating and at times lonely process, since I had no programmer colleagues to turn to for help and brainstorming. But I eventually produced an app I was proud of; you can read more about it here and see the finished app embedded at the bottom of that page or here.\nThe YGDP dashboard reinforced for me that I could do anything in Shiny that I set my mind to, and greenT was a chance to put that into practice. With many fewer moving parts, greenT was a bit of a softball to myself. That’s not to say that it was easy. Rather, when I did run up against a problem, I knew it could not possibly be as challenging as the problems I had already debugged for the YGDP app. I confidently laid out the app and linked up the components and had a working prototype in an afternoon. Then came many more afternoons of tidying and adding features, but that was the fun part.\n\n\n\nFigure 9: A particularly satisfying commit–I got the contribution form to write user data to Google drive\n\n\n\nI hope it doesn’t sound like I’m bragging. I had, at this point, almost a year of 10+ hours a week of Shiny under my belt. But I also want to point out that I didn’t really do this alone at all. I have been, as Steph Locke so eloquently put it, “community-taught”.\n\n\n\nPart of that is being able to jump into Shiny by watching beautifully thought-out videos like the Garrett Grolemund tutorial I referred to above. Part of that is benefiting from the packages that others before me have developed. For example, Dean Attali does great work in Shiny, and his code literally made this app possible: the color selectors on the main page come from his colourpicker package, and the contribution form is taken almost verbatim from his tutorial on mimicking a Google form with Shiny.\nI had another advantage in developing greenT: Jonathan Trattner collaborated with me during most of the app’s development. Even though his time was limited by classes and exams, it was invaluable to be able to ping him on Slack or hop on a video call to discuss something I was stuck on.\nBehind the app\nIf you’d like to read more about the code behind the app, check out Part 2 of this blog post, where I go into detail on the contribution form, non-interactive Google Sheets authentication, building a dynamic ggplot, programmatically generating color selectors, setting the font, and more.\n\n\n\n",
    "preview": "posts/2021-05-09-greent/numbers.png",
    "last_modified": "2021-05-13T07:14:17-07:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 750
  },
  {
    "path": "posts/2020-02-10-upping-your-pipe-game/",
    "title": "%$%: upping your pipe game",
    "description": "I love the magrittr/dplyr pipe: %>%. But it's meant to work with tidyverse functions, and it doesn't always work well with base R functions that take a single data frame column as input. Here, I use data about my friends' pets to explain how a different magrittr pipe, %$%, solves that problem.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nWhat do I do when %>% doesn’t work?\nContext\nI love the %>% pipe. Originally from magrittr, it’s now characteristic of most tidyverse code. Using %>% has revolutionized how I write code in R. But sometimes the basic pipe falls short.\ntable() is one of my favorite functions for exploring data in R: it creates a frequency table of values in a vector. I use table() to do sanity checks on my data, make sure that all factor levels are present, and generally get a sense of how my observations are distributed.\nA while back, though, I noticed that table() didn’t play nice with the %>% pipe.\nI’ve collected some data on my friends’ pets. Here it is (using pseudonyms, in case anyone has a secret pet they don’t want the world to know about…).\n\n\n\nFigure 1: This is one of the cats in the data frame below. She would like to hold your hand.\n\n\n\n\n\n# Load magrittr\nlibrary(magrittr)\nlibrary(dplyr)\n\n# Create data\npets <- data.frame(\n  friend = c(\"Mark\", \"Mark\", \"Kyle\", \"Kyle\", \"Miranda\", \"Kayla\", \n             \"Kayla\", \"Kayla\", \"Adriana\", \"Adriana\", \"Alex\", \"Randy\", \"Nancy\"), \n  pet = c(\"cat\", \"cat\", \"cat\", \"cat\", \"cat\", \"dog\", \"cat\", \"lizard\", \n          \"cat\", \"cat\", \"dog\", \"dog\", \"woodpecker\"), \n  main_pet_color = c(\"brown\", \"brown\", \"multi\", \"multi\", \"brown\", \n                     \"brown\", \"brown\", \"orange\", \"black\", \"white\", \n                     \"multi\", \"white\", \"multi\")) \n\n# Look at the data\npets\n\n\n    friend        pet main_pet_color\n1     Mark        cat          brown\n2     Mark        cat          brown\n3     Kyle        cat          multi\n4     Kyle        cat          multi\n5  Miranda        cat          brown\n6    Kayla        dog          brown\n7    Kayla        cat          brown\n8    Kayla     lizard         orange\n9  Adriana        cat          black\n10 Adriana        cat          white\n11    Alex        dog          multi\n12   Randy        dog          white\n13   Nancy woodpecker          multi\n\nUnsurprisingly, it looks like there are a lot of cats and dogs! There are also a lot of brown pets and a lot of multicolored ones. Let’s say I want to see a frequency table of the pet colors. I know that I can do this with table(), like so:\n\n\n# Make a frequency table of pet colors\ntable(pets$main_pet_color)\n\n\n\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nBut if I want to use tidy syntax, I might try to do it this way instead:\n\n\npets %>%\n  table(main_pet_color)\n\n\nError in table(., main_pet_color): object 'main_pet_color' not found\n\nWhat’s up with this? The syntax should work. pet is definitely a valid variable name in the data frame pets, and if I had used a different function, like arrange(), I would have had no problems:\n\n\n# Arrange the data frame by pet color\npets %>% arrange(main_pet_color) # works fine!\n\n\n    friend        pet main_pet_color\n1  Adriana        cat          black\n2     Mark        cat          brown\n3     Mark        cat          brown\n4  Miranda        cat          brown\n5    Kayla        dog          brown\n6    Kayla        cat          brown\n7     Kyle        cat          multi\n8     Kyle        cat          multi\n9     Alex        dog          multi\n10   Nancy woodpecker          multi\n11   Kayla     lizard         orange\n12 Adriana        cat          white\n13   Randy        dog          white\n\nSo why doesn’t this work with table()?? This problem has driven me crazy on several occasions. I always ended up reverting back to the table(pets$main_pet_color) syntax, but I was not happy about it.\nTurns out, there’s a simple fix.\nSolution\nIntroducing… a new pipe! %$% is called the “exposition pipe,” according to the magrittr package documentation, and it’s basically the tidy version of the with() function, which I wrote about previously.\nIf we simply swap out %>% for %$% in our failed code above, it works!\n\n\n# Make a frequency table of pet colors\npets %$% table(main_pet_color)\n\n\nmain_pet_color\n black  brown  multi orange  white \n     1      5      4      1      2 \n\nImportant note: Make sure you have magrittr loaded if you want to use this pipe. dplyr includes the basic %>%, but not the other magrittr pipes.\nWhy it works\nThe traditional pipe, %>%, works by passing a data frame or tibble into the next function. But that only works if the function you’re piping to is set up to take a data frame/tibble as an argument!\nFunctions in the tidyverse, like arrange(), are set up to take this kind of argument, so that piping works seamlessly. But many base R functions take vectors as inputs instead.\nThat’s the case with table(). When we write table(pets$main_pet_color), the argument pets$main_pet_color is a vector:\n\n\n# This returns a vector\npets$main_pet_color\n\n\n [1] \"brown\"  \"brown\"  \"multi\"  \"multi\"  \"brown\"  \"brown\"  \"brown\" \n [8] \"orange\" \"black\"  \"white\"  \"multi\"  \"white\"  \"multi\" \n\nWhen we try to pass pets into table() with the pipe, table() expects a vector but gets a data frame instead, and it throws an error.\nThe %$% pipe “exposes” the column names of the data frame to the function you’re piping to, allowing that function to make sense of the data frame that is passed to it.\nOutcome\nThe exposition pipe is great for integrating non-tidyverse functions into a tidy workflow. The outcome for me is that I can finally make frequency tables to my heart’s content, without “code switching” back from tidy to base R syntax. Of course, the downside is that you do have to install magrittr, which is sometimes an extra dependency that I don’t want to deal with. But it’s nice to have the option!\n\n\n\nFigure 2: Congrats, you made it to the end! Here are some more cats for you.\n\n\n\nResources\nmagrittr has a couple other pipes, too: %T% and %<>%. The package also has some nice aliases for basic arithmetic functions that allow them to be incorporated into a chain of pipes. To read more about these magrittr options, scroll to the bottom of the magrittr vignette.\nNote: The image at the top of this post was modified from the magrittr documentation.\n\n\n\n",
    "preview": "posts/2020-02-10-upping-your-pipe-game/magrittr.jpg",
    "last_modified": "2021-02-11T16:28:04-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/",
    "title": "Some lessons from rstudio::conf 2020",
    "description": "Some of my thoughts about R and the R community after attending the 2020 rstudio::conf in San Francisco.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2020-02-03",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nWhen I started in R a few years ago, I never thought I would have any place at a coding conference for computer people. But thanks to some help from my lab and my university, last week I flew out to sunny San Francisco for the 2020 rstudio::conf, and I had a blast. Here are some things I learned and some avenues I’m excited to explore in the future.\n1. The R community is awesome.\n\n\n\nFigure 1: This art is by RStudio artist-in-residence Allison Horst. She does amazing illustrations! You can find her work on her GitHub page.\n\n\n\nI was pretty nervous to attend this conference. I’ve never really considered myself a computer person, and I learned R pretty much accidentally through my work in biology. In my experience, people who do a lot of computer programming tend to use a lot of intimidating jargon, and I was scared I’d flounder.\nI was surprised at how genuinely welcome I felt. At meals and talks, I sat down next to random people and asked what they did or how they knew R. I met people with so many diverse interests! Among others:\n• A linguist now working for a nonprofit • A neuroscience PhD now working for Facebook • Several young professors teaching data science at their schools • A woman who flew from Brazil (!!) to attend the conference • Someone just getting to know R • Two young people who interned at RStudio, despite not being experts in R • An RStudio bigwig (swoon!) • A professor working on genomic data • A researcher at the Smithsonian • An aquatic ecologist who uses R for work • So many people! You get the gist. •\nAnd they were all happy to talk to me!\n2. So much happens on Twitter.\nI joined Twitter on a whim this fall, and it has been awesome. I learned about this conference through Twitter. I’ve found some internships through Twitter. And by following the #rstats hashtag and some key people in the R community, I’ve learned all sorts of tips and tricks about the kinds of things you can do with Twitter.\nApart from that, lots of people were live-tweeting the rstudio::conf, and I tried my hand at that, too! A highlight was when the one and only Hadley Wickham liked one of my tweets.\n3. I need to start making Shiny apps.\nAs I said in my tweet (the one that Hadley liked!), this conference convinced me that I really should start building apps with R Shiny as soon as possible.\nWhy haven’t I done this already?\nThe main reason is that the word “app” strikes fear into my heart. Surely I can’t develop an app??\nShiny apps are web apps, which is a little less intimidating, somehow. The basic gist of Shiny apps is that they are ways to explore your data interactively. That’s it. Some Shiny apps can get pretty complicated. For example, here’s a cool dashboard for visualizing tweets about the 2019 rstudio::conf. As you can see, it’s pretty fancy.\n\n\n\nBut Shiny apps can also be pretty simple, like this one, which shows a simple histogram of the duration of eruptions of the Old Faithful geyser:\n\n\n\nThis app just lets the user change the number of histogram bins, include a density curve, and show the individual observations on the x axis. Pretty simple, but still so much better than a static visualization! I really have no excuse not to make something like this.\nIn particular, I can’t wait to build an app for the Yale Grammatical Diversity Project (YGDP). I’m currently working with the YGDP to help organize their database of linguistic survey data. I’ve already created some reports in RMarkdown to help visualize the data (you can see an example here). But wouldn’t this be so much better if it were interactive??\n\n\n\n4. Bonus: fun with hex stickers\nI finally got to experience the hype about hexagon stickers for R packages. They are so pretty and fun, and they fit together so nicely! I picked up a whole bunch:\n\n\n\nAnd, funnily enough, I’ve been playing with hexagons as wall decorations for a while now, before I even knew about hex stickers…\n\n\n\n…so obviously, the possibilities now are truly endless. Hexagons on hexagons? A hex collage on the wall and one on my computer? Wow.\n\n\n\n",
    "preview": "posts/2020-02-03-some-lessons-from-rstudioconf-2020/rstudioconf2020.png",
    "last_modified": "2021-02-11T15:42:23-08:00",
    "input_file": {},
    "preview_width": 1796,
    "preview_height": 766
  },
  {
    "path": "posts/2019-11-22-if-ifelse-had-more-ifs-and-an-else/",
    "title": "If ifelse() had more if's (case_when(), part 2)",
    "description": "In which I learn how to make case_when() behave like a true if/else, including the else part. What happens when you only want to assign particular outcomes to a few cases, without touching the rest? Or if you want to bulk-assign all the unspecified cases to one outcome? The syntax is weird, but it works.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-11-22",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Source: Interesting Engineering (https://interestingengineering.com/video/make-your-very-own-arduino-based-color-candy-sorting-machine)\n\n\n\nThis post has been modified from its original form on woodpeckR.\nProblem\nLast month, I was super excited to discover the case_when() function in dplyr. But when I showed my blog post to a friend, he pointed out a problem: there seemed to be no way to specify a “background” case, like the “else” in ifelse().\n\n\n\nFigure 2: Michael isn’t satisfied with my description of case_when().\n\n\n\nIn the previous post, I gave an example with three outcomes based on test results. The implication was that there would be roughly equal numbers of people in each group. But what if the vast majority of people failed both tests, and we really just wanted to filter out the ones who didn’t?\nContext\nLet’s say I’m analyzing morphometric data for the penguins in the Palmer Penguins dataset. I conduct a principal components analysis.\n\n\n# Load the penguins data\nlibrary(palmerpenguins)\nlibrary(dplyr) # for pipe and case_when and mutate etc.\nlibrary(tibble) # for rownames_to_column\nlibrary(ggplot2) # for plotting\n\n# Conduct Principal Components Analysis (PCA)\npca <- prcomp(~bill_length_mm + bill_depth_mm + \n                flipper_length_mm + body_mass_g, data = penguins)\n\n# Label pca score rows to prepare for join with penguins.\ncomponents <- pca$x %>% \n  as.data.frame() %>%\n  rownames_to_column(\"id\")\n\n# Join pca scores to penguins\npenguins <- penguins %>%\n  rownames_to_column(\"id\") %>% \n  left_join(components, by = \"id\")\n\n\n\n\n\n# Take a peek at the data:\npenguins %>%\n  as.data.frame() %>%\n  head() # now the PC scores are joined onto the penguins data, so I can use it in plotting.\n\n\n  id species    island bill_length_mm bill_depth_mm flipper_length_mm\n1  1  Adelie Torgersen           39.1          18.7               181\n2  2  Adelie Torgersen           39.5          17.4               186\n3  3  Adelie Torgersen           40.3          18.0               195\n4  4  Adelie Torgersen             NA            NA                NA\n5  5  Adelie Torgersen           36.7          19.3               193\n6  6  Adelie Torgersen           39.3          20.6               190\n  body_mass_g    sex year       PC1       PC2         PC3        PC4\n1        3750   male 2007 -452.0232 13.336636 -1.14798019 -0.3534919\n2        3800 female 2007 -401.9500  9.152694  0.09037342 -1.0483310\n3        3250 female 2007 -951.7409 -8.261476  2.35184450  0.8417657\n4          NA   <NA> 2007        NA        NA          NA         NA\n5        3450 female 2007 -751.8127 -1.975922  4.81117040  2.1800839\n6        3650   male 2007 -551.8746  3.343783  1.11849344  2.7060578\n\nAfter running the PCA, I make a plot of the 2nd and 3rd principal components that looks like this:\n\n\n\nBefore continuing my analysis, I wanted to take a closer look at a few points that look like they might be outliers. Specifically, I’m interested in the Adelie point that has a PC2 value greater than 20, and the Chinstrap point that has a PC3 value less than -15. I’m also slightly worried about the two points that have a PC2 value below -15, but they aren’t quite as far out, so I want to classify them separately.\nTo figure out which penguins to look at in the data, I will have to pull out rows based on their scores on the PC2 and PC3 axes.\nSolution\nI decide to add a column called investigate to my data, set to either ‘investigate’, ‘maybe’ or ‘no’ depending on whether the observation in question needs to be checked.\nThis is a great use for my new friend case_when()! I’ll approach it like this:\n\n\npenguins <- penguins %>%\n  mutate(investigate = case_when(PC2 > 20 | PC3 < -15 ~ \"investigate\",\n                                 PC2 < -15 ~ \"maybe\",\n                                 TRUE ~ \"no\"))\n\n\n\nWhat’s up with that weird TRUE ~ \"no\" line at the end of the case_when() statement? Basically, the TRUE is the equivalent of an else. It translates, roughly, to “assign anything that’s left to ‘no.’”\nWhy ‘TRUE’, not ‘else’?\nI don’t love the choice of TRUE here–I think the syntax is pretty confusing, and it’s something I had to memorize long before I understood the logic behind it.\nBasically, case_when() works by checking each of your ‘cases’ (conditions, if-statements) in order. For each row of the data frame, it checks the first case, and applies the resulting assignment if the row meets that case. If the row does not meet the first case, the function moves on to the second case, then the third, and on and on until it finds a case that evaluates to TRUE for that row.\nSo, when you want to write an “else” condition, you write TRUE as a catchall. TRUE will always evaluate to TRUE, so all rows that are left over after failing the first however many conditions will all “pass” that last condition and will be assigned to your desired “else” value.\nBecause of this, order matters! If I had started off with the TRUE ~ “ok” statement and then specified the other conditions, my code wouldn’t have worked: everything would just get assigned to “no”.\nThe dark side of the TRUE condition\nYou might be wondering what would happen if we omitted the TRUE condition. In part 1 of my case_when() explanation\n\n\n\n",
    "preview": "posts/2019-11-22-if-ifelse-had-more-ifs-and-an-else/colorSorter.jpg",
    "last_modified": "2021-03-12T15:36:57-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-12-loading-packages-efficiently/",
    "title": "Loading packages efficiently",
    "description": "A trick I sometimes use to load a whole bunch of packages for every script in a project, while saving myself some typing. (Preview image from https://towardsdatascience.com/a-comprehensive-list-of-handy-r-packages-e85dad294b3d)",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-12",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from towards data science\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nEspecially in a project with many different scripts, it can be challenging to keep track of all the packages you need to load. It’s also easy to lose track of whether or not you’ve incorporated package loading into the script itself until you switch to a new computer or restart R and all of a sudden, your packages need to be re-loaded.\nContext\nWhen I was first starting out in R, I learned quickly to load packages all together at the top of a script, not along the way as I needed them. But it took a while, until I started using R Projects, before I decided to centralize package loading above the script level. I was sick of having to deal with loading the right packages at the right times, so I decided to just streamline the whole thing.\nSolution\nMake a separate R script, called “libraries.R” or “packages.R” or something. Keep it consistent. Mine is always called “libraries,” and I keep it in my project folder.\n\n\n\nThe script looks something like this (individual packages may vary, of course):\n\n\n\nThen, at the top of each analysis script, I can simply source the libraries script, and all the libraries I need load automatically.\n\n\n\nOutcome\nI can easily load libraries in the context of a single R Project, keep track of which ones are loaded, and not have to worry about making my scripts look messy with a whole chunk of library() commands at the top of each one. It’s also straightforward to pop open the “libraries” script whenever I want to add a new package or delete one.\n\n\n\n",
    "preview": "posts/2019-10-12-loading-packages-efficiently/pkgs.png",
    "last_modified": "2021-02-11T15:43:29-08:00",
    "input_file": {},
    "preview_width": 1097,
    "preview_height": 233
  },
  {
    "path": "posts/2019-10-11-if-ifelse-had-more-ifs/",
    "title": "If ifelse() had more if's",
    "description": "In which I discover dplyr's case_when() function, a vectorized version of ifelse().",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-10-11",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Source: Interesting Engineering (https://interestingengineering.com/video/make-your-very-own-arduino-based-color-candy-sorting-machine)\n\n\n\nThis post has been modified from its original form on woodpeckR.\nProblem\nThe ifelse() function only allows for one “if” statement, and therefore two cases. You could add nested “if” statements, but that’s just a pain, especially if the 3+ conditions you want to use are all equivalent, conceptually. Is there a way to specify multiple conditions at the same time?\nContext\nI was recently given some survey data to clean up. It looked something like this (but obviously much larger):\n\n\ndat <- data.frame(name = c(\"Kaija\", \"Ella\", \"Andis\"),\n                  test1 = c(FALSE, TRUE, TRUE),\n                  test2 = c(FALSE, FALSE, TRUE))\ndat\n\n\n   name test1 test2\n1 Kaija FALSE FALSE\n2  Ella  TRUE FALSE\n3 Andis  TRUE  TRUE\n\nI needed to classify people in this data set based on whether they had passed or failed certain tests.\nI wanted to separate the people into three groups:\nPeople who passed both tests: Group A\nPeople who passed one test: Group B\nPeople who passed neither test: Group C\nI thought about using a nested ifelse() statement, and I certainly could have done that. But that approach didn’t make sense to me. The tests are equivalent and not given in any order; I simply want to sort the people into three equal groups. Any nesting of “if” statements would seem to imply a hierarchy that doesn’t really exist in the data. Not to mention that I don’t usually like nesting functions–I find it confusing and hard to read.\nSolution\nOnce again, dplyr to the rescue! I’m becoming more and more of a tidyverse fan with each passing day.\nTurns out, dplyr has a function for exactly this purpose: case_when(). It’s also known as “a general vectorised if,” but I like to think of it as “if ifelse() had more if’s.”\nHere’s the syntax:\n\n\nlibrary(dplyr) # load dplyr\n\ndat <- dat %>% # using the dplyr/magrittr pipe\n  mutate(grp = case_when(test1 & test2 ~ \"A\", # both tests: group A\n                         xor(test1, test2) ~ \"B\", # one test: group B\n                         !test1 & !test2 ~ \"C\" # neither test: group C\n  ))\n\n# Show the result:\ndat\n\n\n   name test1 test2 grp\n1 Kaija FALSE FALSE   C\n2  Ella  TRUE FALSE   B\n3 Andis  TRUE  TRUE   A\n\nLet me translate the above into English:\nAfter loading the package, I reassign dat, my data frame, to a modified version of the old dat. Then (%>%), I use the dplyr::mutate function to add a new column called grp. The contents of the column will be defined by the case_when() function.\ncase_when(), in this example, took three conditions. The condition is on the left side of the ~, and the resulting group assignment (in my case, A, B, or C) is on the right.\nI used logical operators for my conditions. The newest one to me was the xor() function, which is an exclusive or: only one of the conditions in the parentheses can be TRUE, not both.\nOutcome\nEasily make conditional assignments within a data frame. This function is a little less succinct than ifelse(), so I’m probably not going to use it for applications with only two cases, where ifelse() would work fine. But for three or more cases, it can’t be beat. Notice that I could have added any number of conditions to my case_when() statement, with no other caveats.\nI love this function, and I think we should all be using it.\nNOTE: case_when() has some tricky behavior when it comes to cases you don’t explicitly assign to an outcome. That is to say, the else part of this vectorized ifelse function is a little confusing. I discuss the else more in the next installment of this post.\n\n\n\n",
    "preview": "posts/2019-10-11-if-ifelse-had-more-ifs/colorSorter.jpg",
    "last_modified": "2021-03-08T13:53:01-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-13-initializing-an-empty-list/",
    "title": "Initializing an empty list",
    "description": "When you're using a for loop to fill up a list, sometimes it's a good idea to initialize the list ahead of time, with the right number of elements but with no data. But actually doing this is a little harder than I had anticipated.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2019-09-13",
    "categories": [],
    "contents": "\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I initialize an empty list for use in a for-loop or function?\nContext\nSometimes I’m writing a for-loop (I know, I know, I shouldn’t use for-loops in R, but sometimes it’s just easier. I’m a little less comfortable with apply functions than I’d like to be) and I know I’ll need to store the output in a list. Once in a while, the new list will be similar in form to an existing one, but more often, I just need to start from scratch, knowing only the number of elements I want to include.\nThis isn’t a totally alien thing to need to do––it’s pretty familiar if you’re used to initializing empty vectors before for-loops. There’s a whole other debate to be had about whether or not it’s acceptable to start with a truly empty vector and append to it on every iteration of the loop or whether you should always know the length beforehand, but I’ll just focus on the latter case for now.\nAnyway, initializing a vector of a given length is easy enough; I usually do it like this:\n\n\ndesired_length <- 10 # or whatever length you want\nempty_vec <- rep(NA, desired_length)\n\n\n\nI couldn’t immediately figure out how to replicate this for a list, though. The solution turns out to be relatively simple, but it’s just different enough that I can never seem to remember the syntax. This post is more for my records than anything, then.\nSolution\nInitializing an empty list turns out to have an added benefit over my rep(NA) method for vectors; namely, the list ends up actually empty, not filled with NA’s. Confusingly, the function to use is vector, not list.\n\n\ndesired_length <- 10 # or whatever length you want\nempty_list <- vector(mode = \"list\", \n                     length = desired_length)\n\nstr(empty_list)\n\n\nList of 10\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n\nOutcome\nVoilà, an empty list. No restrictions on the data type or structure of the individual list elements. Specify the length easily. Useful for loops, primarily, but may have other applications I haven’t come across yet.\nI learned something interesting recently, during the R working group I take part in through the Yale School of Forestry. See how, above, I initialized a list using the vector() function? Why is that?\nMaybe this should have been obvious, but a list is actually a type of vector. The thing that I would typically call a vector (a series of elements, all of the same type), is more specifically an ‘atomic vector’. A list, on the other hand, is just a vector that can contain more than one type of data.\n\n\n\n",
    "preview": "posts/2019-09-13-initializing-an-empty-list/emptyList.jpg",
    "last_modified": "2021-02-23T15:38:22-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-automatically-show-output/",
    "title": "(Automatically show output)",
    "description": "A neat trick with parentheses to print the contents of an object you just created, without running another line of code.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nIt’s annoying to have to type the name of an object I just created in order to print its output.\nContext\nA certain lightsaber-wielding stats professor of mine liked to point out that R doesn’t go out of its way to be helpful. If you write a line of code that creates an object and then run that line of code, there’s no message to tell you that the object has been successfully created. R doesn’t say “Task complete! What’s next?” or otherwise give you any indication that anything has happened. To actually view the object you just created, you have to type its name or run some other command on it.\nOnce in a while, this lack of transparency can be frustrating. What if I want to save objects and also view them in real time as they are created? Say I’ve used the handy prop.table function to transform a frequency table into a proportion table. I’d like to be able to view prop, prop.1 and prop.2 without typing their names and adding extra lines of code.\nSolution\nThe same lightsaber-wielding stats professor who wished R would be a little more communicative taught me a trick to do just this: encase a command in parentheses to automatically print its output when it runs. Hence,\n\n\n# Load data from GitHub\nlibrary(dplyr)\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n(prop <- with(polygon, table(revetment, pool)) \n %>% prop.table())\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…returns the same thing as leaving out the parentheses and typing the name of the object, prop, on a new line:\n\n\nprop <- with(polygon, table(revetment, pool)) %>%\n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nAlso note that this is different (better) than just running the command without the assignment arrow, like this:\n\n\nwith(polygon, table(revetment, pool)) %>% \n  prop.table()\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\n…because the above doesn’t save the table you created, it just shows it to you once.\nOutcome\nCreate objects and view them at the same time, while saving some typing. This is also great for use in RMarkdown, because it will print the output below the code chunk without your having to add another line of code.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-23T15:42:03-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-22-proptable/",
    "title": "prop.table()",
    "description": "Making a frequency table with proportions instead of counts. Preview image from https://twitter.com/lyric_rep/status/1010594530435846144.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Image from https://twitter.com/lyric_rep/status/1010594530435846144\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I convert a frequency table into proportions?\nContext\nThis is a continuation of the data manipulation discussed in the with() post. I had just finished making a table\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\")\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nWhat if I want to see this table broken down by proportion of polygons, not counts?\nSolution\nThe prop.table() function will do this nicely.\n\n\nlibrary(dplyr)\n\nprop <- with(polygon, table(revetment, pool)) %>% \n  prop.table()\n\nprop\n\n\n         pool\nrevetment          4          8         13\n        0 0.13472486 0.37760911 0.19544592\n        1 0.10815939 0.10056926 0.08349146\n\nBy default, the proportions are calculated over the entire table. So each cell represents the proportion of all polygons that are in that pool with that value of revetment. The whole table sums to 1.\nIf you want proportions across rows or down columns, all you need to do is add the margin = argument.\nmargin = 1 sums across rows. Each row sums to 1. This would answer the question, “What proportion of the polygons [with, or without] revetment are located in each of the three pools?”\n\n\nprop.1 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 1)\n\nprop.1\n\n\n         pool\nrevetment         4         8        13\n        0 0.1903485 0.5335121 0.2761394\n        1 0.3701299 0.3441558 0.2857143\n\nmargin = 2 sums down columns. Each column sums to 1. This would answer the question, \"What proportion of the polygons in [pool] have revetment? (or, what proportion don’t have revetment?)\n\n\nprop.2 <- with(polygon, table(revetment, pool)) %>% \n  prop.table(margin = 2)\n\nprop.2\n\n\n         pool\nrevetment         4         8        13\n        0 0.5546875 0.7896825 0.7006803\n        1 0.4453125 0.2103175 0.2993197\n\nOutcome\nHandy function for creating proportion tables.\n\n\n\n",
    "preview": "posts/2018-07-22-proptable/propTable.jpg",
    "last_modified": "2021-02-23T15:44:47-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-20-with/",
    "title": "with()",
    "description": "A brief introduction to the with() function",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-20",
    "categories": [],
    "contents": "\nProblem\nMaking graphics with base R is annoying for many reasons, but a big one is having to type the name of the data frame over and over again to reference different columns.\nContext\nBack to our Mississippi River fish data. I’ve aggregated my sampling points into polygons, and now I want to explore some of their characteristics. To do that, I’d like to make some tables and plots, and because these are just quick, exploratory plots, I don’t feel like dealing with ggplot.\nLoad in the data (accessible on GitHub).\n\n\n# Load data from GitHub\npolygon <- read.csv(\"https://tinyurl.com/rta6hkbo\") \n\n# Look at what we're dealing with\ndim(polygon) # How big is the data set? \n\n\n[1] 527  21\n\nhead(polygon, 3) # Look at the first few rows\n\n\n     poly_id propsnag n_points habitat_code pool      Area Perimeter\n1 P04_CFL_13   0.8000        5          CFL    4 105288.80  2067.890\n2 P04_CFL_14   0.2000        5          CFL    4  42668.28  1770.465\n3 P04_CFL_15   0.4375       16          CFL    4 678390.21  5226.963\n  max_depth avg_depth tot_vol shoreline_density_index pct_aqveg\n1      1.30 0.3625869   33955                1.797759 19.133960\n2      0.74 0.3291391    5953                2.417852 41.252704\n3      2.68 0.6159651  356757                1.790213  9.256465\n  pct_terr pct_prm_wetf med_dist_to_land med_dist_to_forest\n1 93.87983     79.67522         34.13379           34.13379\n2 94.76871     42.44244         18.90166           32.64112\n3 86.17161     41.27248         21.40210           39.27576\n  med_current wingdam revetment tributary pct_shallow_area\n1        0.02       0         1         0        0.9278354\n2        0.02       0         0         0        1.0000000\n3        0.01       1         1         1        0.9194788\n\nFirst, I’d like to see how total volume tot_vol of the aquatic area scales with its Area.\nIn base R:\n\n\n# Formula notation\nplot(polygon$tot_vol ~ polygon$Area)\n# OR: \n\n# Comma notation\nplot(polygon$Area, polygon$tot_vol)\n\n\n\nEither way, we get this:\n\n\n\nOr a more informative plot, with both variables on a log scale:\n\n\nplot(log(polygon$tot_vol) ~ log(polygon$Area))\n\n\n\n\nThis isn’t too too clunky, but if the data frame name or column names are long, it can get a little annoying.\nSolution\nThe with() function allows you to specify the data frame your variables are coming from and then reference the variables with respect to the data frame, similar to the ggplot argument data =. Handy.\n\n\n# Plot using the with() function \nwith(polygon, plot(tot_vol ~ Area))\n\n\n\n\nYou can add any other arguments inside of the function, as normal, it’s just now wrapped in with().\n\n\n# Log-transform variables and make the points blue dots, because why not?\nwith(polygon, plot(log(tot_vol) ~ log(Area), # log-transform \n                   pch = 20, # dots instead of circles \n                   col = \"blue\", # make the dots blue \n                   main = \"Polygon volume by area, log-transformed\") # title \n     )\n\n\n\n\nIt’s worth noting that this works for other functions besides plot(), too. Here’s an example with table(): let’s look at how many sampling polygons include revetment, broken down by navigation pool (area of the river). The data set contains three navigation pools: 4, 8, and 13.\n\n\n# Two-way table by pool and revetment\nwith(polygon, table(revetment, pool))\n\n\n         pool\nrevetment   4   8  13\n        0  71 199 103\n        1  57  53  44\n\nOutcome\nQuick plots and data manipulation made even quicker!\nResources\nDiscussion of when to use with(): https://stackoverflow.com/questions/42283479/when-to-use-with-function-and-why-is-it-good\n\n\n\n",
    "preview": "posts/2018-07-20-with/noPreview.png",
    "last_modified": "2021-02-23T15:48:03-08:00",
    "input_file": {},
    "preview_width": 353,
    "preview_height": 50
  },
  {
    "path": "posts/2018-07-11-changing-individual-column-names/",
    "title": "Changing individual column names",
    "description": "How to rename individual columns in a data frame, based on the previous names and without using previous names.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-11",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow do I change the name of just one column in a data frame?\nContext\nThis is a simple one that keeps coming up. Sometimes, whoever put together my data decided to capitalize the first letter of some column names and not others. Sometimes I’ve merged several data frames together and I need to distinguish the columns from each other.\nSay my data frame is p8_0 and I’d like to change the column Area to area.\nIn the past, I’ve done this in one of two ways. Either I change all of the column names at once (if all of them need to be changed), or I use numerical column indexing. The latter makes a lot more sense if I have a lot of columns to deal with, but it means I have to know the number of the column whose name I have to change.\nTo find this out, I first have to look at all of the column names. Okay, no problem.\n\n\n\n\n\n# See column names and numerical indices\nnames(p8_0)\n\n\n  [1] \"FID\"                     \"Join_Count\"             \n  [3] \"TARGET_FID\"              \"Field1\"                 \n  [5] \"barcode\"                 \"stratum\"                \n  [7] \"lcode\"                   \"sdate\"                  \n  [9] \"utm_e\"                   \"utm_n\"                  \n [11] \"snag\"                    \"OBJECTID\"               \n [13] \"uniq_id\"                 \"aa_num\"                 \n [15] \"AQUA_CODE\"               \"AQUA_DESC\"              \n [17] \"pool\"                    \"Area\"                   \n [19] \"Perimeter\"               \"bath_pct\"               \n [21] \"max_depth\"               \"avg_depth\"              \n [23] \"sd_depth\"                \"tot_vol\"                \n [25] \"area_gt50\"               \"area_gt100\"             \n [27] \"area_gt200\"              \"area_gt300\"             \n [29] \"avg_fetch\"               \"shoreline_density_index\"\n [31] \"econ\"                    \"sill\"                   \n [33] \"min_rm\"                  \"max_rm\"                 \n [35] \"len_met\"                 \"len_prm_lotic\"          \n [37] \"pct_prm_lotic\"           \"num_lotic_outl\"         \n [39] \"len_prm_lentic\"          \"pct_prm_lentic\"         \n [41] \"num_lentic_outl\"         \"pct_aqveg\"              \n [43] \"pct_opwat\"               \"len_terr\"               \n [45] \"pct_terr\"                \"pct_aq\"                 \n [47] \"len_wetf\"                \"pct_prm_wetf\"           \n [49] \"pct_terr_shore_wetf\"     \"len_wd\"                 \n [51] \"wdl_p_m2\"                \"num_wd\"                 \n [53] \"scour_wd\"                \"psco_wd\"                \n [55] \"len_revln\"               \"rev_p_m2\"               \n [57] \"num_rev\"                 \"pct_terr_shore_rev\"     \n [59] \"pct_prm_rev\"             \"area_tpi1\"              \n [61] \"pct_tpi1\"                \"area_tpi2\"              \n [63] \"pct_tpi2\"                \"area_tpi3\"              \n [65] \"pct_tpi3\"                \"area_tpi4\"              \n [67] \"pct_tpi4\"                \"sinuosity\"              \n [69] \"year_phot\"               \"NEAR_TERR_FID\"          \n [71] \"NEAR_TERR_DIST\"          \"NEAR_TERR_CLASS_31\"     \n [73] \"NEAR_TERR_CLASS_15\"      \"NEAR_TERR_CLASS_7\"      \n [75] \"NEAR_TERR_CLASS_31_N\"    \"NEAR_TERR_CLASS_15_N\"   \n [77] \"NEAR_TERR_CLASS_7_N\"     \"NEAR_TERR_HEIGHT_N\"     \n [79] \"NEAR_FOREST_FID\"         \"NEAR_FOREST_DIST\"       \n [81] \"NEAR_FOREST_CLASS_31\"    \"NEAR_FOREST_CLASS_15\"   \n [83] \"NEAR_FOREST_CLASS_7\"     \"NEAR_FOREST_CLASS_31_N\" \n [85] \"NEAR_FOREST_CLASS_15_N\"  \"NEAR_FOREST_CLASS_7_N\"  \n [87] \"NEAR_FOREST_HEIGHT_N\"    \"year.p\"                 \n [89] \"depth.p\"                 \"current.p\"              \n [91] \"gear.p\"                  \"stageht.p\"              \n [93] \"substrt.p\"               \"wingdike.p\"             \n [95] \"riprap.p\"                \"trib.p\"                 \n [97] \"snagyn\"                  \"area_le50\"              \n [99] \"area_le100\"              \"area_le200\"             \n[101] \"area_le300\"              \"pct_area_le100\"         \n[103] \"pct_area_le50\"           \"pct_area_le200\"         \n[105] \"pct_area_le300\"          \"stratum_name\"           \n\nOkay, yes problem.\nIt’s not that hard to see that Area is the 18th column. But there are a bunch of columns that start with NEAR_TERR_ and NEAR_FOREST_ that would be easy to confuse. And what if I later modify my data cleaning script, insert new columns, and mess up the numerical indexing?\nSolution\nThe first solution I came up with is simple but pretty clunky. At least it solves the problem of numerical indices getting misaligned. And if you mistype the column name or try to change the name of a column that doesn’t exist, it doesn’t throw an error.\n\n\n# Change \"Area\" column name to \"area\"\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\n\n\n\nThis works well, but it gets annoying if you have more than one column name to change. Every column requires typing names(p8_0) twice, and that adds up to a lot of lines of code.\nTo no one’s surprise, dplyr has a more elegant solution, using the rename function.\n\n\n\n\n\n# Load dplyr \nlibrary(dplyr) \n\n# Rename variable (new name first) \np8_0 <- p8_0 %>% \n  rename(area = Area)\n\n\n\nA quick note on rename: somewhat counterintuitively, the new name comes before the old name. General example:\n\n\n# General syntax for rename \n#df %>% \n#  rename(newname = oldname)\n\n\n\nrename saves a whole bunch of keystrokes and also scales very well to multiple columns.\nLet’s say I wanted to change Area and Perimeter to area and perimeter, respectively, and I also wanted to change the rather clunky shoreline_density_index to sdi. And while we’re at it, snagyn, a factor variable that indicates whether a large piece of wood was present at the site (“yes” or “no”), might be clearer as snag_yn, and sinuosity could be shortened to sinu\nWithout dplyr:\n\n\n\n\n\n# Change each column name individually\nnames(p8_0)[names(p8_0) == \"Area\"] <- \"area\"\nnames(p8_0)[names(p8_0) == \"Perimeter\"] <- \"perimeter\"\nnames(p8_0)[names(p8_0) == \"shoreline_density_index\"] <- \"sdi\"\nnames(p8_0)[names(p8_0) == \"snagyn\"] <- \"snag_yn\"\nnames(p8_0)[names(p8_0) == \"sinuosity\"] <- \"sinu\"\n\n\n\nWith dplyr:\n\n\n\n\n\n# Change any column names you want to, all at once\np8_0 <- p8_0 %>% rename(area = Area, \n                perimeter = Perimeter,\n                sdi = shoreline_density_index, \n                snag_yn = snagyn,\n                sinu = sinuosity)\n\n\n\nSo pretty. As an added bonus, you’re saved from both quotation marks and the dreaded double equals sign (!!!).\nIn case anyone was counting, that’s 102 characters vs. 238 (spaces not included). 116 if you include loading dplyr, but you already had it loaded because you’re using it throughout your code, of course.\nOutcome\nNow I can rename only the columns I want, by name instead of numerical index, without fear of having to change everything if I insert or delete some columns later on.\nResources\nMore thoughts on changing individual variable names, including a couple other packages if you feel like trying them: https://stackoverflow.com/questions/7531868/how-to-rename-a-single-column-in-a-data-frame\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T16:20:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-08-the-notin-operator/",
    "title": "The %notin% operator",
    "description": "Why is negating `%in%` such a pain?",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-08",
    "categories": [],
    "contents": "\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nI keep forgetting how to select all elements of an object except a few, by name. I get the ! operator confused with the - operator, and I find both of them less than intuitive to use. How can I negate the %in% operator?\nContext\nI have a data frame called electrofishing that contains observations from a fish sampling survey. One column, stratum, gives the aquatic habitat type of the sampling site. I’d like to exclude observations sampled in the “Tailwater Zone” or “Impounded-Offshore” aquatic habitats.\n\n\nelectrofishing <- data.frame(stratum = c(\"Tailwater Zone\", \"Tailwater Zone\", \"Impounded\", \"Main Channel Border\", \"Side Channel\", \"Impounded-Offshore\", \"Side Channel\"), \n                             idx = 1:7)\n\nelectrofishing\n\n\n              stratum idx\n1      Tailwater Zone   1\n2      Tailwater Zone   2\n3           Impounded   3\n4 Main Channel Border   4\n5        Side Channel   5\n6  Impounded-Offshore   6\n7        Side Channel   7\n\nMy instinct would be to do this:\n\nelectrofishing <- electrofishing[electrofishing$stratum !%in% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\nError: <text>:1:57: unexpected '!'\n1: electrofishing <- electrofishing[electrofishing$stratum !\n                                                            ^\n\nBut that doesn’t work. You can’t negate the %in% operator directly. Instead, you have to wrap the %in% statement in parentheses and negate the entire statement, returning the opposite of the original boolean vector:\n\n\nelectrofishing <- electrofishing[!(electrofishing$stratum %in% \n                                     c(\"Tailwater Zone\", \"Impounded-Offshore\")),]\n\n\n\nI’m not saying this doesn’t make sense, but I can never remember it. My English-speaking brain would much rather say “rows whose stratum is not included in c(”Tailwater Zone“,”Impounded-Offshore“)” than “not rows whose stratum is included in c(”Tailwater Zone“,”Impounded-Offshore“)”.\nSolution\nLuckily, it’s pretty easy to negate %in% and create a %notin% operator. I credit this answer to user “catastrophic-failure” on this Stack Overflow question.\n\n\n`%notin%` <- Negate(`%in%`)\n\n\n\nI didn’t even know that the Negate function existed. The more you know.\nOutcome\nI know there are lots of ways to negate selections in R. dplyr has select() and filter() functions that are easier to use with -c(). Or I could just learn to throw a ! in front of my %in% statements. But %notin% seems a little more intuitive.\nNow it’s straightforward to select these rows from my data frame.\n\n\nelectrofishing <- electrofishing[electrofishing$stratum %notin% \n                                   c(\"Tailwater Zone\", \"Impounded-Offshore\"),]\n\n\n\nResources\nhttps://stackoverflow.com/questions/38351820/negation-of-in-in-r\nThis one does a good job of explaining why !%in% doesn’t work.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-11T15:41:56-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-07-06-where-are-my-nas/",
    "title": "Where are my NA's?",
    "description": "I wrote a function to summarize how many `NA`'s are in each column of a data frame. Preview image by Allison Horst, https://github.com/allisonhorst.",
    "author": [
      {
        "name": "Kaija Gahm",
        "url": {}
      }
    ],
    "date": "2018-07-06",
    "categories": [],
    "contents": "\n\n\n\nFigure 1: Artwork by Allison Horst, https://github.com/allisonhorst/stats-illustrations.\n\n\n\nThis post has been slightly modified from its original form on woodpeckR.\nProblem\nHow can I (quickly and intuitively) figure out how many NA’s are in my dataset and which columns they’re in?\nContext\nWhen I tried to run PCA (Principal Components Analysis) on some USGS fish sampling data, I noticed that I had a bunch of missing values. PCA needs complete observations, so this was a problem.\nOne option would have been to remove any observations with missing values from my data set:\n\n\n# Select only \"complete\" rows from the data frame `df`  \n# noNAs <- df[complete.cases(df),]\n\n\n\nThe problem was, I had over 30 variables and who knows how many missing values. The data frame had only ~2000 observations. By using only complete cases, I might lose a lot of observations and reduce my sample size by a huge amount.\nIn fact, I pretty often find myself in this situation. It would be really nice to have a quick way to see where those NA values are located so I can get a better sense of my dataset and figure out how to move forward.\nSolution\nWrite a loop that tells us how many NA’s are in each column.\nFirst, let’s create a sample data frame and call it sample.df:\n\n\n# Create the data frame\nsample.df <- data.frame(\n     site = 1:4, \n     temp = c(10, 15, 13, NA), \n     depth = c(1.1, NA, 2.0, NA)\n)\n\n# Show the data frame\nsample.df\n\n\n  site temp depth\n1    1   10   1.1\n2    2   15    NA\n3    3   13   2.0\n4    4   NA    NA\n\nLoop through the columns and print out the number of NA’s:\n\n\n# Create a vector full of NA's, the same length as the number of columns in sample.df\nna.vec <- rep(NA, ncol(sample.df))\n\n# Loop through the columns and fill na.vec\nfor(i in 1:ncol(sample.df)){\n     na.vec[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.vec\nna.vec\n\n\n[1] 0 1 2\n\nNow we can see that there are 0 NA’s in the first column, 1 NA in the second column, and 2 NA’s in the third column.\nBut if you have 30 columns, it’s a pain to map those numbers to the column names. So let’s do better. Instead of just printing the numbers of NA’s in a vector, we’ll put them in a data frame along with the names of the columns.\n\n\n# Create a data frame\nna.df <- data.frame(\n     Column = names(sample.df),\n     num.nas = NA\n)\n\n# Loop through the columns of sample.df and fill na.df\nfor(i in 1:ncol(sample.df)){\n     na.df$num.nas[i] <- sum(is.na(sample.df[,i]))\n}\n\n# Take a look at na.df\nna.df\n\n\n  Column num.nas\n1   site       0\n2   temp       1\n3  depth       2\n\nSo much better!\nOnce you get used to it, this is a quick loop to write. But I got sick of re-creating this process every time, so I wrote a function called locate.nas. Feel free to use it:\n\n\n#Locate NA's: produces a data frame with column names and number of na's\nlocate.nas <- function(df){\n  na.df <- data.frame(\n    colname = names(df),\n    nas = NA\n  )\n  \n  for(i in 1:ncol(df)){\n    na.df$nas[i] <- sum(is.na(df[,i]))\n  }\n  return(na.df)\n}\n\n\n\nOutcome\nA quick look at the distribution of missing values (NA’s) in my data frame turned up an obvious pattern. I checked the sampling protocol and saw that certain variables had only been measured for lotic areas (moving water), while others had only been measured for lentic areas (still water). Since every observation point was in either a lotic or a lentic area, filtering out incomplete observations would have left me with no data at all.\nBy adding an indicator variable for lotic/lentic area, I could sort out my data and run PCA separately. Or I could remove the variables measured for only one area. Problem solved.\nResources\nlocate_nas function\nThere’s also a whole package that makes dealing with NA’s easier. I didn’t know about it when I originally wrote this post, but I’ve since discovered it, and you should check it out! It’s called naniar, and you can find it here\n\n\n\n",
    "preview": "posts/2018-07-06-where-are-my-nas/naniar.jpg",
    "last_modified": "2021-02-11T15:41:42-08:00",
    "input_file": {}
  }
]
